{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Phase 2: Data Preprocessing\n",
    "\n",
    "## Overview\n",
    "This notebook implements a comprehensive data preprocessing pipeline for the CDC Diabetes Health Indicators dataset (BRFSS 2015). Key processes include profile deduplication with frequency-based sample weights, logical consistency validation, target variable binarization, and weight normalization.\n",
    "\n",
    "**Core Objectives:**\n",
    "- **Profile Aggregation**: Merge identical clinical feature combinations to reduce redundancy\n",
    "- **Quality Control**: Remove logically inconsistent records\n",
    "- **Target Re-alignment**: Convert trinary diabetes status (0/1/2) to binary classification target (0/1)\n",
    "- **Weight Preservation**: Maintain statistical representativeness through normalized sample weights (mean=1.0)\n",
    "- **Integrity Assurance**: Ensure zero duplicates, zero missing values, and valid target encoding\n",
    "\n",
    "**Pipeline Stages:**\n",
    "1. Environment Configuration\n",
    "2. Path Configuration\n",
    "3. Utility Functions\n",
    "4. Data Ingestion\n",
    "5. Memory Optimization\n",
    "6. Profile Aggregation\n",
    "7. Aggregation Verification\n",
    "8. Logical Consistency Cleaning\n",
    "9. Target Re-alignment (Binarization)\n",
    "10. Feature Cleanup\n",
    "11. Ultimate Deduplication (Critical)\n",
    "12. Weight Normalization\n",
    "13. Post-Normalization Verification\n",
    "14. Export Final Dataset\n",
    "15. Preprocessing Pipeline Summary\n",
    "\n",
    "## Output Dataset\n",
    "**File**: `data/processed/data_preprocessing/CDC_Diabetes_Cleaned.csv`\n",
    "\n",
    "**Purpose**: Serves as the foundation dataset for downstream feature engineering (Phase 3). Each record represents a unique clinical profile with statistically representative weight, eliminating redundant survey responses while preserving population-level prevalence ratios.\n",
    "\n",
    "**Schema**:\n",
    "- **21 Original Features**: HighBP, HighChol, CholCheck, BMI, Smoker, Stroke, HeartDiseaseorAttack, PhysActivity, Fruits, Veggies, HvyAlcoholConsump, AnyHealthcare, NoDocbcCost, GenHlth, MentHlth, PhysHlth, DiffWalk, Sex, Age, Education, Income\n",
    "- **Diabetes_binary** (Target): Binary classification label (0=Non-diabetic, 1=Pre-diabetic or Diabetic)\n",
    "- **Sample_Weight**: Normalized frequency weight (mean=1.0) for weighted model training"
   ],
   "id": "bf87dd71"
  },
  {
   "cell_type": "markdown",
   "id": "0e1895bd",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "2819bf38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:06.008844500Z",
     "start_time": "2026-02-04T20:13:05.958713100Z"
    }
   },
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display configuration\n",
    "pd.set_option('display.max_columns', 25)\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.13.11 | packaged by Anaconda, Inc. | (main, Dec 10 2025, 21:21:58) [MSC v.1929 64 bit (AMD64)]\n",
      "Pandas version: 3.0.0\n",
      "NumPy version: 2.3.5\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "7ae375b1",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "0ffa30b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:06.077703800Z",
     "start_time": "2026-02-04T20:13:06.016842800Z"
    }
   },
   "source": [
    "# Define project directory structure\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "DATA_PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "PREPROCESSING_OUTPUT_DIR = DATA_PROCESSED_DIR / \"data_preprocessing\"\n",
    "\n",
    "# Source and destination file paths\n",
    "INPUT_FILE = DATA_RAW_DIR / \"CDC Diabetes Dataset.csv\"\n",
    "OUTPUT_FILE = PREPROCESSING_OUTPUT_DIR / \"CDC_Diabetes_Cleaned.csv\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "PREPROCESSING_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[CONFIG] Output file: {OUTPUT_FILE}\")\n",
    "\n",
    "print(f\"[CONFIG] Project root: {PROJECT_ROOT}\")\n",
    "print(f\"[CONFIG] Preprocessing output directory: {PREPROCESSING_OUTPUT_DIR}\")\n",
    "print(f\"[CONFIG] Input file: {INPUT_FILE}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIG] Output file: D:\\ProgramSoftware\\PyCharm\\WorkPlace\\DiaMetric-CDC\\data\\processed\\data_preprocessing\\CDC_Diabetes_Cleaned.csv\n",
      "[CONFIG] Project root: D:\\ProgramSoftware\\PyCharm\\WorkPlace\\DiaMetric-CDC\n",
      "[CONFIG] Preprocessing output directory: D:\\ProgramSoftware\\PyCharm\\WorkPlace\\DiaMetric-CDC\\data\\processed\\data_preprocessing\n",
      "[CONFIG] Input file: D:\\ProgramSoftware\\PyCharm\\WorkPlace\\DiaMetric-CDC\\data\\raw\\CDC Diabetes Dataset.csv\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "7c09774d",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "5338bc82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:06.119596Z",
     "start_time": "2026-02-04T20:13:06.082706800Z"
    }
   },
   "source": [
    "def load_dataset(filepath: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load CSV dataset with error handling.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : Path\n",
    "        Path to the CSV file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Loaded dataframe.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    FileNotFoundError\n",
    "        If file does not exist.\n",
    "    ValueError\n",
    "        If file is empty.\n",
    "    \"\"\"\n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found at: {filepath}\")\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    if df.empty:\n",
    "        raise ValueError(f\"Dataset is empty: {filepath}\")\n",
    "    \n",
    "    print(f\"[SUCCESS] Loaded {len(df):,} records with {df.shape[1]} columns.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def optimize_memory(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downcast numeric columns to reduce memory footprint.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with numeric columns.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Memory-optimized dataframe.\n",
    "    \"\"\"\n",
    "    initial_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    \n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    \n",
    "    final_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    reduction_pct = (1 - final_mem / initial_mem) * 100\n",
    "    \n",
    "    print(f\"[MEMORY] {initial_mem:.2f} MB -> {final_mem:.2f} MB ({reduction_pct:.1f}% reduction)\")\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "2cbe5de3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:06.195635500Z",
     "start_time": "2026-02-04T20:13:06.155595800Z"
    }
   },
   "source": [
    "def aggregate_profiles(df: pd.DataFrame) -> Tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Aggregate identical clinical profiles and compute sample weights.\n",
    "    \n",
    "    Groups records by feature values, treating duplicate rows as repeated\n",
    "    observations of the same clinical archetype. Profile frequency stored\n",
    "    as Sample_Weight.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Raw dataframe with potential duplicate profiles.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[pd.DataFrame, dict]\n",
    "        - Aggregated dataframe with unique profiles and Sample_Weight column.\n",
    "        - Summary statistics dictionary.\n",
    "    \"\"\"\n",
    "    original_count = len(df)\n",
    "    feature_columns = df.columns.tolist()\n",
    "    \n",
    "    # Group by all columns and compute frequency\n",
    "    df_aggregated = (\n",
    "        df.groupby(feature_columns, as_index=False)\n",
    "        .size()\n",
    "        .rename(columns={'size': 'Sample_Weight'})\n",
    "    )\n",
    "    \n",
    "    unique_count = len(df_aggregated)\n",
    "    reduction_pct = (1 - unique_count / original_count) * 100\n",
    "    \n",
    "    summary = {\n",
    "        'original_count': original_count,\n",
    "        'unique_profiles': unique_count,\n",
    "        'duplicates_removed': original_count - unique_count,\n",
    "        'reduction_percentage': reduction_pct,\n",
    "        'weight_sum': df_aggregated['Sample_Weight'].sum()\n",
    "    }\n",
    "    \n",
    "    return df_aggregated, summary\n",
    "\n",
    "\n",
    "def verify_data_integrity(\n",
    "    df: pd.DataFrame,\n",
    "    original_count: int,\n",
    "    expected_columns: int = 23\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Validate data integrity constraints after aggregation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Aggregated dataframe to verify.\n",
    "    original_count : int\n",
    "        Original row count before aggregation.\n",
    "    expected_columns : int, default 23\n",
    "        Expected number of columns (22 features + Sample_Weight).\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    AssertionError\n",
    "        If any integrity constraint is violated.\n",
    "    \"\"\"\n",
    "    # Verify column count (22 original + Sample_Weight)\n",
    "    assert df.shape[1] == expected_columns, (\n",
    "        f\"Column count mismatch: expected {expected_columns}, got {df.shape[1]}\"\n",
    "    )\n",
    "    \n",
    "    # Verify weight summation equals original count\n",
    "    weight_sum = df['Sample_Weight'].sum()\n",
    "    assert weight_sum == original_count, (\n",
    "        f\"Weight sum mismatch: expected {original_count}, got {weight_sum}\"\n",
    "    )\n",
    "    \n",
    "    # Verify no null values in weight column\n",
    "    assert df['Sample_Weight'].notna().all(), (\n",
    "        \"Sample_Weight contains null values\"\n",
    "    )\n",
    "    \n",
    "    # Verify all weights are positive integers\n",
    "    assert (df['Sample_Weight'] > 0).all(), (\n",
    "        \"Sample_Weight contains non-positive values\"\n",
    "    )\n",
    "    \n",
    "    print(\"[VALIDATED] All data integrity assertions passed.\")"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "6937fdda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:06.288045100Z",
     "start_time": "2026-02-04T20:13:06.217638400Z"
    }
   },
   "source": [
    "def save_processed_data(df: pd.DataFrame, filepath: Path) -> None:\n",
    "    \"\"\"\n",
    "    Export processed dataframe to CSV.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe to export.\n",
    "    filepath : Path\n",
    "        Destination file path.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    df.to_csv(filepath, index=False)\n",
    "    \n",
    "    file_size_mb = filepath.stat().st_size / 1024**2\n",
    "    print(f\"[SUCCESS] Saved to: {filepath}\")\n",
    "    print(f\"[INFO] File size: {file_size_mb:.2f} MB\")"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "dfced143",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "id": "0f5f3f0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:06.805157400Z",
     "start_time": "2026-02-04T20:13:06.292047200Z"
    }
   },
   "source": [
    "# Load raw dataset\n",
    "df_raw = load_dataset(INPUT_FILE)\n",
    "\n",
    "# Display initial structure\n",
    "print(f\"\\n[INFO] Shape: {df_raw.shape}\")\n",
    "print(f\"[INFO] Columns: {df_raw.columns.tolist()}\")\n",
    "df_raw.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Loaded 253,680 records with 22 columns.\n",
      "\n",
      "[INFO] Shape: (253680, 22)\n",
      "[INFO] Columns: ['Diabetes_012', 'HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Diabetes_012  HighBP  HighChol  CholCheck     BMI  Smoker  Stroke  HeartDiseaseorAttack  PhysActivity  Fruits  Veggies  HvyAlcoholConsump  AnyHealthcare  NoDocbcCost  GenHlth  MentHlth  PhysHlth  \\\n",
       "0        0.0000  1.0000    1.0000     1.0000 40.0000  1.0000  0.0000                0.0000        0.0000  0.0000   1.0000             0.0000         1.0000       0.0000   5.0000   18.0000   15.0000   \n",
       "1        0.0000  0.0000    0.0000     0.0000 25.0000  1.0000  0.0000                0.0000        1.0000  0.0000   0.0000             0.0000         0.0000       1.0000   3.0000    0.0000    0.0000   \n",
       "2        0.0000  1.0000    1.0000     1.0000 28.0000  0.0000  0.0000                0.0000        0.0000  1.0000   0.0000             0.0000         1.0000       1.0000   5.0000   30.0000   30.0000   \n",
       "3        0.0000  1.0000    0.0000     1.0000 27.0000  0.0000  0.0000                0.0000        1.0000  1.0000   1.0000             0.0000         1.0000       0.0000   2.0000    0.0000    0.0000   \n",
       "4        0.0000  1.0000    1.0000     1.0000 24.0000  0.0000  0.0000                0.0000        1.0000  1.0000   1.0000             0.0000         1.0000       0.0000   2.0000    3.0000    0.0000   \n",
       "\n",
       "   DiffWalk    Sex     Age  Education  Income  \n",
       "0    1.0000 0.0000  9.0000     4.0000  3.0000  \n",
       "1    0.0000 0.0000  7.0000     6.0000  1.0000  \n",
       "2    1.0000 0.0000  9.0000     4.0000  8.0000  \n",
       "3    0.0000 0.0000 11.0000     3.0000  6.0000  \n",
       "4    0.0000 0.0000 11.0000     5.0000  4.0000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes_012</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>HvyAlcoholConsump</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>40.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>3.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>28.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>8.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>27.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>6.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>4.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "9dcbee16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:07.097940Z",
     "start_time": "2026-02-04T20:13:06.916205600Z"
    }
   },
   "source": [
    "# Verify expected column count\n",
    "EXPECTED_RAW_COLUMNS = 22\n",
    "assert df_raw.shape[1] == EXPECTED_RAW_COLUMNS, (\n",
    "    f\"Expected {EXPECTED_RAW_COLUMNS} columns, got {df_raw.shape[1]}\"\n",
    ")\n",
    "print(f\"[VALIDATED] Column count verified: {df_raw.shape[1]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALIDATED] Column count verified: 22\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "91bdf33d",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "id": "613d082e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:07.529519800Z",
     "start_time": "2026-02-04T20:13:07.238130100Z"
    }
   },
   "source": [
    "# Downcast numeric types to reduce memory footprint\n",
    "df_optimized = optimize_memory(df_raw.copy())\n",
    "\n",
    "# Display optimized dtypes\n",
    "print(\"\\n[INFO] Optimized data types:\")\n",
    "print(df_optimized.dtypes.value_counts())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MEMORY] 42.58 MB -> 21.29 MB (50.0% reduction)\n",
      "\n",
      "[INFO] Optimized data types:\n",
      "float32    22\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "id": "3dc64202",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Profile Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "id": "9f6cc5f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:08.066654300Z",
     "start_time": "2026-02-04T20:13:07.589447800Z"
    }
   },
   "source": [
    "# Execute profile convergence strategy\n",
    "df_aggregated, agg_summary = aggregate_profiles(df_optimized)\n",
    "\n",
    "# Display aggregation summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROFILE AGGREGATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original record count:    {agg_summary['original_count']:>12,}\")\n",
    "print(f\"Unique clinical profiles: {agg_summary['unique_profiles']:>12,}\")\n",
    "print(f\"Duplicate records merged: {agg_summary['duplicates_removed']:>12,}\")\n",
    "print(f\"Data reduction:           {agg_summary['reduction_percentage']:>11.2f}%\")\n",
    "print(f\"Weight sum verification:  {agg_summary['weight_sum']:>12,}\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROFILE AGGREGATION SUMMARY\n",
      "============================================================\n",
      "Original record count:         253,680\n",
      "Unique clinical profiles:      229,781\n",
      "Duplicate records merged:       23,899\n",
      "Data reduction:                  9.42%\n",
      "Weight sum verification:       253,680\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "c1406360",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:08.275972800Z",
     "start_time": "2026-02-04T20:13:08.240269700Z"
    }
   },
   "source": [
    "# Examine weight distribution\n",
    "print(\"[INFO] Sample_Weight distribution:\")\n",
    "print(df_aggregated['Sample_Weight'].describe())\n",
    "\n",
    "print(f\"\\n[INFO] Profiles with weight > 1: {(df_aggregated['Sample_Weight'] > 1).sum():,}\")\n",
    "print(f\"[INFO] Maximum weight: {df_aggregated['Sample_Weight'].max():,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Sample_Weight distribution:\n",
      "count   229781.0000\n",
      "mean         1.1040\n",
      "std          0.8490\n",
      "min          1.0000\n",
      "25%          1.0000\n",
      "50%          1.0000\n",
      "75%          1.0000\n",
      "max         59.0000\n",
      "Name: Sample_Weight, dtype: float64\n",
      "\n",
      "[INFO] Profiles with weight > 1: 11,187\n",
      "[INFO] Maximum weight: 59\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "id": "e99d9857",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Aggregation Verification"
   ]
  },
  {
   "cell_type": "code",
   "id": "a2ddd48e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:08.294661900Z",
     "start_time": "2026-02-04T20:13:08.279970500Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 7 VERIFICATION: Profile Aggregation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Execute integrity assertions\n",
    "verify_data_integrity(\n",
    "    df=df_aggregated,\n",
    "    original_count=agg_summary['original_count'],\n",
    "    expected_columns=23  # 22 features + Sample_Weight\n",
    ")\n",
    "\n",
    "# Verify weight statistics\n",
    "assert df_aggregated['Sample_Weight'].min() > 0, \"Invalid: negative or zero weights detected\"\n",
    "assert df_aggregated['Sample_Weight'].sum() == agg_summary['original_count'], \"Weight sum mismatch\"\n",
    "\n",
    "print(f\"\\n[VALIDATED] Shape: {df_aggregated.shape}\")\n",
    "print(f\"[VALIDATED] Weight range: [{df_aggregated['Sample_Weight'].min()}, {df_aggregated['Sample_Weight'].max()}]\")\n",
    "print(f\"[VALIDATED] Weight mean: {df_aggregated['Sample_Weight'].mean():.2f}\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 7 VERIFICATION: Profile Aggregation\n",
      "============================================================\n",
      "[VALIDATED] All data integrity assertions passed.\n",
      "\n",
      "[VALIDATED] Shape: (229781, 23)\n",
      "[VALIDATED] Weight range: [1, 59]\n",
      "[VALIDATED] Weight mean: 1.10\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "a7e8e9f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:08.348189700Z",
     "start_time": "2026-02-04T20:13:08.305662800Z"
    }
   },
   "source": [
    "# Display aggregated dataframe structure\n",
    "print(f\"\\n[INFO] Aggregated dataset preview:\")\n",
    "df_aggregated.head(10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Aggregated dataset preview:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Diabetes_012  HighBP  HighChol  CholCheck     BMI  Smoker  Stroke  HeartDiseaseorAttack  PhysActivity  Fruits  Veggies  HvyAlcoholConsump  AnyHealthcare  NoDocbcCost  GenHlth  MentHlth  PhysHlth  \\\n",
       "0        0.0000  0.0000    0.0000     0.0000 14.0000  1.0000  0.0000                0.0000        1.0000  1.0000   1.0000             0.0000         1.0000       0.0000   3.0000    4.0000    4.0000   \n",
       "1        0.0000  0.0000    0.0000     0.0000 15.0000  0.0000  0.0000                0.0000        0.0000  1.0000   0.0000             0.0000         0.0000       0.0000   1.0000    0.0000    0.0000   \n",
       "2        0.0000  0.0000    0.0000     0.0000 15.0000  1.0000  0.0000                0.0000        0.0000  0.0000   0.0000             0.0000         1.0000       0.0000   5.0000   20.0000   28.0000   \n",
       "3        0.0000  0.0000    0.0000     0.0000 15.0000  1.0000  0.0000                1.0000        0.0000  1.0000   1.0000             0.0000         0.0000       1.0000   3.0000    0.0000   29.0000   \n",
       "4        0.0000  0.0000    0.0000     0.0000 16.0000  0.0000  0.0000                0.0000        0.0000  0.0000   0.0000             0.0000         1.0000       0.0000   2.0000    0.0000    0.0000   \n",
       "5        0.0000  0.0000    0.0000     0.0000 16.0000  0.0000  0.0000                0.0000        0.0000  1.0000   1.0000             0.0000         1.0000       0.0000   2.0000    7.0000    5.0000   \n",
       "6        0.0000  0.0000    0.0000     0.0000 16.0000  0.0000  0.0000                0.0000        1.0000  0.0000   0.0000             0.0000         1.0000       1.0000   2.0000    0.0000    0.0000   \n",
       "7        0.0000  0.0000    0.0000     0.0000 16.0000  0.0000  0.0000                0.0000        1.0000  0.0000   1.0000             0.0000         1.0000       0.0000   2.0000    0.0000    4.0000   \n",
       "8        0.0000  0.0000    0.0000     0.0000 16.0000  0.0000  0.0000                0.0000        1.0000  1.0000   1.0000             0.0000         0.0000       0.0000   2.0000    0.0000    0.0000   \n",
       "9        0.0000  0.0000    0.0000     0.0000 16.0000  0.0000  0.0000                1.0000        1.0000  0.0000   1.0000             0.0000         1.0000       1.0000   4.0000    0.0000    3.0000   \n",
       "\n",
       "   DiffWalk    Sex     Age  Education  Income  Sample_Weight  \n",
       "0    0.0000 1.0000 11.0000     6.0000  8.0000              1  \n",
       "1    0.0000 0.0000  1.0000     5.0000  7.0000              1  \n",
       "2    1.0000 0.0000 10.0000     6.0000  4.0000              1  \n",
       "3    0.0000 0.0000  7.0000     5.0000  2.0000              1  \n",
       "4    0.0000 0.0000 11.0000     5.0000  5.0000              1  \n",
       "5    0.0000 0.0000  1.0000     5.0000  5.0000              1  \n",
       "6    0.0000 0.0000  6.0000     5.0000  6.0000              1  \n",
       "7    0.0000 0.0000  3.0000     6.0000  8.0000              1  \n",
       "8    0.0000 0.0000  9.0000     5.0000  2.0000              1  \n",
       "9    0.0000 0.0000  6.0000     6.0000  2.0000              1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes_012</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>HvyAlcoholConsump</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "      <th>Sample_Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>14.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>28.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "id": "c355e85c",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Logical Consistency Cleaning\n",
    "\n",
    "Identifies and removes records with logical inconsistencies between self-reported health indicators. Profiles reporting 30 days of poor physical health (PhysHlth=30) while claiming excellent general health (GenHlth=1) represent contradictory self-assessments that compromise model reliability."
   ]
  },
  {
   "cell_type": "code",
   "id": "f0ddb500",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:08.495049Z",
     "start_time": "2026-02-04T20:13:08.397941300Z"
    }
   },
   "source": [
    "def remove_logical_inconsistencies(df: pd.DataFrame) -> Tuple[pd.DataFrame, int]:\n",
    "    \"\"\"\n",
    "    Remove records with contradictory health indicator combinations.\n",
    "    \n",
    "    Logical inconsistency: PhysHlth=30 with GenHlth=1.\n",
    "    - PhysHlth=30: Poor physical health for all 30 days\n",
    "    - GenHlth=1: Excellent general health\n",
    "    \n",
    "    Mutually exclusive conditions indicating response errors or\n",
    "    survey question misinterpretation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with PhysHlth and GenHlth columns.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[pd.DataFrame, int]\n",
    "        Cleaned dataframe and count of removed records.\n",
    "    \"\"\"\n",
    "    initial_count = len(df)\n",
    "    \n",
    "    # Define inconsistency mask: maximum physical distress + excellent health\n",
    "    inconsistency_mask = (df['PhysHlth'] == 30) & (df['GenHlth'] == 1)\n",
    "    \n",
    "    # Filter out inconsistent records\n",
    "    df_cleaned = df[~inconsistency_mask].copy()\n",
    "    \n",
    "    removed_count = initial_count - len(df_cleaned)\n",
    "    \n",
    "    return df_cleaned, removed_count"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "de5d13a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:08.823881900Z",
     "start_time": "2026-02-04T20:13:08.651209200Z"
    }
   },
   "source": [
    "# Execute logical consistency cleaning\n",
    "df_consistent, removed_count = remove_logical_inconsistencies(df_aggregated)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 8 VERIFICATION: Logical Consistency Cleaning\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Records before cleaning:  {len(df_aggregated):>12,}\")\n",
    "print(f\"Inconsistent records:     {removed_count:>12,}\")\n",
    "print(f\"Records after cleaning:   {len(df_consistent):>12,}\")\n",
    "\n",
    "# Calculate weight statistics\n",
    "weight_sum_before = df_aggregated['Sample_Weight'].sum()\n",
    "weight_sum_after = df_consistent['Sample_Weight'].sum()\n",
    "weight_removed = weight_sum_before - weight_sum_after\n",
    "\n",
    "print(f\"\\nWeight statistics:\")\n",
    "print(f\"  Weight sum before:      {weight_sum_before:>12,.0f}\")\n",
    "print(f\"  Weight sum after:       {weight_sum_after:>12,.0f}\")\n",
    "print(f\"  Weight removed:         {weight_removed:>12,.0f}\")\n",
    "\n",
    "# Verify no contradictory records remain\n",
    "inconsistent_check = ((df_consistent['PhysHlth'] == 30) & (df_consistent['GenHlth'] == 1)).sum()\n",
    "assert inconsistent_check == 0, f\"Inconsistency removal failed: {inconsistent_check} contradictory records remain\"\n",
    "\n",
    "print(f\"\\n[VALIDATED] No contradictory records remaining: {inconsistent_check}\")\n",
    "print(f\"[VALIDATED] Data integrity maintained\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 8 VERIFICATION: Logical Consistency Cleaning\n",
      "============================================================\n",
      "Records before cleaning:       229,781\n",
      "Inconsistent records:              416\n",
      "Records after cleaning:        229,365\n",
      "\n",
      "Weight statistics:\n",
      "  Weight sum before:           253,680\n",
      "  Weight sum after:            253,264\n",
      "  Weight removed:                  416\n",
      "\n",
      "[VALIDATED] No contradictory records remaining: 0\n",
      "[VALIDATED] Data integrity maintained\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "a99d0d75",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Target Re-alignment (Binarization)\n",
    "\n",
    "The original trinary target variable (`Diabetes_012`) is transformed into a binary classification target. Pre-diabetic cases (value 1) are consolidated with diabetic cases (value 2) to create a unified positive class, enabling binary classification modeling."
   ]
  },
  {
   "cell_type": "code",
   "id": "3b3fc1fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:08.916198500Z",
     "start_time": "2026-02-04T20:13:08.900825600Z"
    }
   },
   "source": [
    "def binarize_target(df: pd.DataFrame, source_col: str = 'Diabetes_012') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert trinary diabetes target to binary classification target.\n",
    "    \n",
    "    Mapping:\n",
    "    - 0 (No diabetes) -> 0 (Negative class)\n",
    "    - 1 (Pre-diabetes) -> 1 (Positive class)\n",
    "    - 2 (Diabetes) -> 1 (Positive class)\n",
    "    \n",
    "    Original column retained for validation and removed in subsequent cleanup stage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with trinary target column.\n",
    "    source_col : str, default 'Diabetes_012'\n",
    "        Name of the original target column.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe with new binary target column 'Diabetes_binary'.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If source column contains NaN values.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Check for NaN values\n",
    "    if df[source_col].isna().any():\n",
    "        nan_count = df[source_col].isna().sum()\n",
    "        raise ValueError(\n",
    "            f\"Found {nan_count} NaN values in {source_col}. \"\n",
    "            \"Handle missing values before binarization.\"\n",
    "        )\n",
    "    \n",
    "    # Binary mapping: 0 stays 0, any positive value becomes 1\n",
    "    df['Diabetes_binary'] = df[source_col].apply(lambda x: 0 if x == 0 else 1)\n",
    "    \n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "ba68c22d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:09.189442100Z",
     "start_time": "2026-02-04T20:13:09.035428300Z"
    }
   },
   "source": [
    "# Execute target binarization on cleaned dataset\n",
    "df_binarized = binarize_target(df_consistent)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 9 VERIFICATION: Target Re-alignment (Binarization)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nOriginal target distribution (Diabetes_012):\")\n",
    "original_dist = df_binarized['Diabetes_012'].value_counts().sort_index()\n",
    "for val, count in original_dist.items():\n",
    "    label = {0: 'No diabetes', 1: 'Pre-diabetes', 2: 'Diabetes'}[int(val)]\n",
    "    print(f\"  {int(val)} ({label}): {count:,}\")\n",
    "\n",
    "print(\"\\nBinarized target distribution (Diabetes_binary):\")\n",
    "binary_dist = df_binarized['Diabetes_binary'].value_counts().sort_index()\n",
    "for val, count in binary_dist.items():\n",
    "    label = {0: 'Negative', 1: 'Positive'}[val]\n",
    "    print(f\"  {val} ({label}): {count:,}\")\n",
    "\n",
    "# Verify mapping correctness\n",
    "expected_negative = original_dist.get(0, 0)\n",
    "expected_positive = original_dist.get(1, 0) + original_dist.get(2, 0)\n",
    "actual_negative = binary_dist.get(0, 0)\n",
    "actual_positive = binary_dist.get(1, 0)\n",
    "\n",
    "assert expected_negative == actual_negative, \"Negative class count mismatch\"\n",
    "assert expected_positive == actual_positive, \"Positive class count mismatch\"\n",
    "assert set(df_binarized['Diabetes_binary'].unique()) == {0, 1}, \"Invalid values in binary target\"\n",
    "\n",
    "print(f\"\\n[VALIDATED] Mapping correctness: Negative={actual_negative:,}, Positive={actual_positive:,}\")\n",
    "print(f\"[VALIDATED] Binary target contains only values: {sorted(df_binarized['Diabetes_binary'].unique())}\")\n",
    "print(f\"[VALIDATED] Class balance ratio: {actual_positive/actual_negative:.4f}\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 9 VERIFICATION: Target Re-alignment (Binarization)\n",
      "============================================================\n",
      "\n",
      "Original target distribution (Diabetes_012):\n",
      "  0 (No diabetes): 189,697\n",
      "  1 (Pre-diabetes): 4,620\n",
      "  2 (Diabetes): 35,048\n",
      "\n",
      "Binarized target distribution (Diabetes_binary):\n",
      "  0 (Negative): 189,697\n",
      "  1 (Positive): 39,668\n",
      "\n",
      "[VALIDATED] Mapping correctness: Negative=189,697, Positive=39,668\n",
      "[VALIDATED] Binary target contains only values: [np.int64(0), np.int64(1)]\n",
      "[VALIDATED] Class balance ratio: 0.2091\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "id": "d006a593",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Feature Cleanup\n",
    "\n",
    "Removes original trinary target column to prevent target leakage. Ensures downstream modeling pipeline uses only the binary target variable."
   ]
  },
  {
   "cell_type": "code",
   "id": "1ecff04b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:09.476878600Z",
     "start_time": "2026-02-04T20:13:09.447068300Z"
    }
   },
   "source": [
    "# Drop original trinary target to prevent leakage\n",
    "df_cleaned = df_binarized.drop(columns=['Diabetes_012'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 10 VERIFICATION: Feature Cleanup\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "assert 'Diabetes_012' not in df_cleaned.columns, \"Target leakage: Diabetes_012 column still exists\"\n",
    "assert 'Diabetes_binary' in df_cleaned.columns, \"Binary target column missing\"\n",
    "\n",
    "print(f\"[VALIDATED] Diabetes_012 removed (prevent target leakage)\")\n",
    "print(f\"[VALIDATED] Diabetes_binary preserved (binary target)\")\n",
    "print(f\"[VALIDATED] Current shape: {df_cleaned.shape}\")\n",
    "print(f\"[VALIDATED] Current columns: {df_cleaned.shape[1]}\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 10 VERIFICATION: Feature Cleanup\n",
      "============================================================\n",
      "[VALIDATED] Diabetes_012 removed (prevent target leakage)\n",
      "[VALIDATED] Diabetes_binary preserved (binary target)\n",
      "[VALIDATED] Current shape: (229365, 23)\n",
      "[VALIDATED] Current columns: 23\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "id": "34d874ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:09.590115400Z",
     "start_time": "2026-02-04T20:13:09.491876700Z"
    }
   },
   "source": [
    "def remove_constant_features(df: pd.DataFrame, exclude: list = ['Sample_Weight']) -> Tuple[pd.DataFrame, list]:\n",
    "    \"\"\"\n",
    "    Identify and remove features that have the same value across all samples.\n",
    "    \"\"\"\n",
    "    constant_cols = [col for col in df.columns \n",
    "                     if df[col].nunique() <= 1 and col not in exclude]\n",
    "    \n",
    "    if constant_cols:\n",
    "        df = df.drop(columns=constant_cols)\n",
    "        print(f\"[CLEANUP] Removed constant features: {constant_cols}\")\n",
    "    else:\n",
    "        print(\"[CLEANUP] No constant features detected.\")\n",
    "    \n",
    "    return df, constant_cols\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ADDITIONAL AUDIT: Constant Feature Check\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Execute function\n",
    "df_cleaned, removed_const = remove_constant_features(df_cleaned)\n",
    "\n",
    "# Validate returned list\n",
    "assert isinstance(removed_const, list), \"Returned value type error\"\n",
    "print(f\"[VALIDATED] Audit process recorded.\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ADDITIONAL AUDIT: Constant Feature Check\n",
      "============================================================\n",
      "[CLEANUP] No constant features detected.\n",
      "[VALIDATED] Audit process recorded.\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "id": "19164132",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Ultimate Deduplication (Critical)\n",
    "\n",
    "After feature cleanup (removing `Diabetes_012`), rows that originally had different `Diabetes_012` values but the same features and `Diabetes_binary` value now become duplicates. This step performs the **ultimate aggregation** by grouping all identical feature profiles and summing their weights."
   ]
  },
  {
   "cell_type": "code",
   "id": "ddd62685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:10.095429500Z",
     "start_time": "2026-02-04T20:13:09.595112800Z"
    }
   },
   "source": [
    "# Perform ultimate deduplication: merge all identical profiles\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 11: ULTIMATE DEDUPLICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "records_before = len(df_cleaned)\n",
    "feature_cols = [col for col in df_cleaned.columns if col != 'Sample_Weight']\n",
    "\n",
    "# Check for potential duplicates\n",
    "dup_mask = df_cleaned[feature_cols].duplicated(keep=False)\n",
    "print(f\"Records before deduplication: {records_before:,}\")\n",
    "print(f\"Potentially duplicate profiles: {dup_mask.sum()}\")\n",
    "\n",
    "# Aggregate: group by all features, sum weights\n",
    "df_deduplicated = (\n",
    "    df_cleaned.groupby(feature_cols, as_index=False)\n",
    "    .agg({'Sample_Weight': 'sum'})\n",
    ")\n",
    "\n",
    "records_after = len(df_deduplicated)\n",
    "merged_count = records_before - records_after\n",
    "\n",
    "print(f\"Records after deduplication:  {records_after:,}\")\n",
    "print(f\"Duplicate records merged:     {merged_count}\")\n",
    "\n",
    "# Verify no duplicates remain\n",
    "final_dup_check = df_deduplicated[feature_cols].duplicated().sum()\n",
    "assert final_dup_check == 0, f\"Deduplication failed: {final_dup_check} duplicates remain\"\n",
    "\n",
    "# Verify weight integrity\n",
    "weight_sum_before = df_cleaned['Sample_Weight'].sum()\n",
    "weight_sum_after = df_deduplicated['Sample_Weight'].sum()\n",
    "assert np.isclose(weight_sum_before, weight_sum_after, rtol=1e-9), \"Weight sum changed during deduplication\"\n",
    "\n",
    "print(f\"\\n[VALIDATED] No duplicate profiles remain: {final_dup_check}\")\n",
    "print(f\"[VALIDATED] Weight sum preserved: {weight_sum_after:,.2f}\")\n",
    "print(f\"[VALIDATED] Dataset ready for normalization\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 11: ULTIMATE DEDUPLICATION\n",
      "============================================================\n",
      "Records before deduplication: 229,365\n",
      "Potentially duplicate profiles: 138\n",
      "Records after deduplication:  229,296\n",
      "Duplicate records merged:     69\n",
      "\n",
      "[VALIDATED] No duplicate profiles remain: 0\n",
      "[VALIDATED] Weight sum preserved: 253,264.00\n",
      "[VALIDATED] Dataset ready for normalization\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "id": "459fac6d",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Weight Normalization\n",
    "\n",
    "Sample weights are normalized by dividing each weight by the mean, ensuring a distribution mean of 1.0. This facilitates stable numerical computations in downstream modeling while preserving relative prevalence ratios."
   ]
  },
  {
   "cell_type": "code",
   "id": "d63204c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:10.160645700Z",
     "start_time": "2026-02-04T20:13:10.145964600Z"
    }
   },
   "source": [
    "def normalize_weights(df: pd.DataFrame, weight_col: str = 'Sample_Weight') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize sample weights by dividing by mean.\n",
    "    \n",
    "    Ensures weight distribution has mean 1.0, facilitating stable numerical\n",
    "    computations while preserving relative prevalence ratios.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with raw sample weights.\n",
    "    weight_col : str, default 'Sample_Weight'\n",
    "        Name of the weight column.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe with normalized weights.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    mean_weight = df[weight_col].mean()\n",
    "    df[weight_col] = df[weight_col] / mean_weight\n",
    "    \n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "id": "d5c83470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:10.230502700Z",
     "start_time": "2026-02-04T20:13:10.163646400Z"
    }
   },
   "source": [
    "# Execute weight normalization on deduplicated dataset\n",
    "pre_norm_mean = df_deduplicated['Sample_Weight'].mean()\n",
    "df_normalized = normalize_weights(df_deduplicated)\n",
    "post_norm_mean = df_normalized['Sample_Weight'].mean()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 12 VERIFICATION: Weight Normalization\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Pre-normalization mean:   {pre_norm_mean:.6f}\")\n",
    "print(f\"Post-normalization mean:  {post_norm_mean:.6f}\")\n",
    "\n",
    "# Validate mean equals 1.0 (within floating-point tolerance)\n",
    "assert np.isclose(post_norm_mean, 1.0, atol=1e-10), (\n",
    "    f\"Weight normalization failed: mean = {post_norm_mean}, expected 1.0\"\n",
    ")\n",
    "\n",
    "weight_std = df_normalized['Sample_Weight'].std()\n",
    "weight_min = df_normalized['Sample_Weight'].min()\n",
    "weight_max = df_normalized['Sample_Weight'].max()\n",
    "\n",
    "print(f\"\\n[VALIDATED] Weight mean: {post_norm_mean:.6f} (target: 1.0)\")\n",
    "print(f\"[VALIDATED] Weight std: {weight_std:.4f}\")\n",
    "print(f\"[VALIDATED] Weight range: [{weight_min:.4f}, {weight_max:.4f}]\")\n",
    "print(f\"\\nWeight distribution:\")\n",
    "print(df_normalized['Sample_Weight'].describe())\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 12 VERIFICATION: Weight Normalization\n",
      "============================================================\n",
      "Pre-normalization mean:   1.104529\n",
      "Post-normalization mean:  1.000000\n",
      "\n",
      "[VALIDATED] Weight mean: 1.000000 (target: 1.0)\n",
      "[VALIDATED] Weight std: 0.7697\n",
      "[VALIDATED] Weight range: [0.9054, 53.4165]\n",
      "\n",
      "Weight distribution:\n",
      "count   229296.0000\n",
      "mean         1.0000\n",
      "std          0.7697\n",
      "min          0.9054\n",
      "25%          0.9054\n",
      "50%          0.9054\n",
      "75%          0.9054\n",
      "max         53.4165\n",
      "Name: Sample_Weight, dtype: float64\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "id": "7a60f7e8",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Post-Normalization Verification\n",
    "\n",
    "Verifies absence of duplicate rows after normalization. Although normalization operates via scalar division and should not generate duplicates, this verification ensures data integrity."
   ]
  },
  {
   "cell_type": "code",
   "id": "ed7ba97f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:10.496170900Z",
     "start_time": "2026-02-04T20:13:10.232502Z"
    }
   },
   "source": [
    "# Final duplicate check after normalization\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 13 VERIFICATION: Post-Normalization Duplicate Check\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_final = df_normalized.copy()\n",
    "\n",
    "# Check for complete duplicates (all columns including Sample_Weight)\n",
    "full_dup = df_final.duplicated().sum()\n",
    "print(f\"Complete duplicate rows: {full_dup}\")\n",
    "\n",
    "# Check for feature duplicates (excluding Sample_Weight)\n",
    "feature_cols = [col for col in df_final.columns if col != 'Sample_Weight']\n",
    "feature_dup = df_final[feature_cols].duplicated().sum()\n",
    "print(f\"Feature duplicate rows (excluding Sample_Weight): {feature_dup}\")\n",
    "\n",
    "assert full_dup == 0, f\"ERROR: {full_dup} complete duplicates found after normalization!\"\n",
    "assert feature_dup == 0, f\"ERROR: {feature_dup} feature duplicates found after normalization!\"\n",
    "\n",
    "print(f\"\\n[VALIDATED] No duplicate rows detected\")\n",
    "print(f\"[VALIDATED] Final dataset shape: {df_final.shape}\")\n",
    "print(f\"[VALIDATED] Final record count: {len(df_final):,}\")\n",
    "print(f\"[VALIDATED] Data pipeline complete - ready for export\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 13 VERIFICATION: Post-Normalization Duplicate Check\n",
      "============================================================\n",
      "Complete duplicate rows: 0\n",
      "Feature duplicate rows (excluding Sample_Weight): 0\n",
      "\n",
      "[VALIDATED] No duplicate rows detected\n",
      "[VALIDATED] Final dataset shape: (229296, 23)\n",
      "[VALIDATED] Final record count: 229,296\n",
      "[VALIDATED] Data pipeline complete - ready for export\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "id": "62a150e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:10.523005800Z",
     "start_time": "2026-02-04T20:13:10.499171200Z"
    }
   },
   "source": [
    "def final_quality_audit(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Perform final data integrity checks before export.\n",
    "    \"\"\"\n",
    "    # 1. Check for missing values\n",
    "    assert df.isna().sum().sum() == 0, \"Unexpected missing value found!\"\n",
    "    \n",
    "    # 2. Check if the binary target is complete\n",
    "    assert set(df['Diabetes_binary'].unique()) == {0, 1}, \"Target variable does not conform to binary logic\"\n",
    "    \n",
    "    # 3. Check if the sample weights are normalized (double check)\n",
    "    assert np.isclose(df['Sample_Weight'].mean(), 1.0, atol=1e-5), \"Sample weights normalization deviates from threshold\"\n",
    "    \n",
    "    print(\"Data flow integrity: 100% passed.\")\n",
    "\n",
    "final_quality_audit(df_final)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data flow integrity: 100% passed.\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "id": "d65b4582",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Export Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "7ae4692d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:13.588894300Z",
     "start_time": "2026-02-04T20:13:10.537006400Z"
    }
   },
   "source": [
    "# Save final cleaned and processed dataset\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EXPORT\")\n",
    "print(\"=\"*60)\n",
    "save_processed_data(df_final, OUTPUT_FILE)\n",
    "print(f\"[SUCCESS] Dataset ready for feature engineering\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL EXPORT\n",
      "============================================================\n",
      "[SUCCESS] Saved to: D:\\ProgramSoftware\\PyCharm\\WorkPlace\\DiaMetric-CDC\\data\\processed\\data_preprocessing\\CDC_Diabetes_Cleaned.csv\n",
      "[INFO] File size: 23.54 MB\n",
      "[SUCCESS] Dataset ready for feature engineering\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "id": "be1e75ee",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. Preprocessing Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "id": "3d019936",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T20:13:13.672058100Z",
     "start_time": "2026-02-04T20:13:13.627537400Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Comprehensive preprocessing summary\n",
    "print(\"\\n\" + \"#\"*60)\n",
    "print(\"DATA PREPROCESSING PIPELINE SUMMARY\")\n",
    "print(\"#\"*60)\n",
    "\n",
    "# Verify all required variables exist (use globals() for notebook scope)\n",
    "required_vars = {\n",
    "    'agg_summary': 'Section 6-7 (Profile Aggregation)',\n",
    "    'removed_count': 'Section 8 (Logical Consistency)',\n",
    "    'records_before': 'Section 11 (Ultimate Deduplication)',\n",
    "    'merged_count': 'Section 11 (Ultimate Deduplication)',\n",
    "    'records_after': 'Section 11 (Ultimate Deduplication)',\n",
    "    'pre_norm_mean': 'Section 12 (Weight Normalization)',\n",
    "    'post_norm_mean': 'Section 12 (Weight Normalization)',\n",
    "    'full_dup': 'Section 13 (Post-Norm Verification)',\n",
    "    'feature_dup': 'Section 13 (Post-Norm Verification)',\n",
    "    'df_final': 'Section 13 (Post-Norm Verification)'\n",
    "}\n",
    "\n",
    "missing_vars = {var: stage for var, stage in required_vars.items() if var not in globals()}\n",
    "if missing_vars:\n",
    "    print(\"\\nWARNING: Cannot generate complete summary\")\n",
    "    print(\"The following variables are missing:\\n\")\n",
    "    for var, stage in missing_vars.items():\n",
    "        print(f\"  - {var} (from {stage})\")\n",
    "    print(\"\\nExecute all cells sequentially from the beginning.\")\n",
    "    print(\"#\"*60)\n",
    "else:\n",
    "    # Recalculate class distribution from final dataset for accuracy\n",
    "    actual_negative = (df_final['Diabetes_binary'] == 0).sum()\n",
    "    actual_positive = (df_final['Diabetes_binary'] == 1).sum()\n",
    "    \n",
    "    print(\"\\n[INPUT]\")\n",
    "    print(f\"  Source file: {INPUT_FILE.name}\")\n",
    "    print(f\"  Raw records: {agg_summary['original_count']:,}\")\n",
    "\n",
    "    print(\"\\n[PROCESSING STAGES]\")\n",
    "    print(f\"  Stage 1: Environment Configuration\")\n",
    "    print(f\"    - Python, Pandas, NumPy version check\")\n",
    "    print(f\"    - Display options configured\")\n",
    "    print(f\"    [VALIDATED] Runtime environment initialized\")\n",
    "    \n",
    "    print(f\"\\n  Stage 2: Path Configuration\")\n",
    "    print(f\"    - Project root: {PROJECT_ROOT.name}\")\n",
    "    print(f\"    - Input: {INPUT_FILE.name}\")\n",
    "    print(f\"    - Output: {OUTPUT_FILE.name}\")\n",
    "    print(f\"    [VALIDATED] File paths configured\")\n",
    "    \n",
    "    print(f\"\\n  Stage 3: Utility Functions\")\n",
    "    print(f\"    - load_dataset(), optimize_memory(), aggregate_profiles()\")\n",
    "    print(f\"    - verify_data_integrity(), save_processed_data()\")\n",
    "    print(f\"    [VALIDATED] Helper functions defined\")\n",
    "    \n",
    "    print(f\"\\n  Stage 4: Data Ingestion\")\n",
    "    print(f\"    - Loaded: {INPUT_FILE.name}\")\n",
    "    print(f\"    - Expected columns: 22\")\n",
    "    print(f\"    [VALIDATED] Dataset loaded successfully\")\n",
    "    \n",
    "    print(f\"\\n  Stage 5: Memory Optimization\")\n",
    "    print(f\"    - Downcast numeric types to int8/int16\")\n",
    "    print(f\"    - Memory footprint reduced\")\n",
    "    print(f\"    [VALIDATED] Data types optimized\")\n",
    "    \n",
    "    print(f\"\\n  Stage 6-7: Profile Aggregation & Verification\")\n",
    "    print(f\"    - Unique profiles: {agg_summary['unique_profiles']:,}\")\n",
    "    print(f\"    - Duplicates merged: {agg_summary['duplicates_removed']:,}\")\n",
    "    print(f\"    - Data reduction: {agg_summary['reduction_percentage']:.2f}%\")\n",
    "    print(f\"    - Added: Sample_Weight column\")\n",
    "    print(f\"    [VALIDATED] Weight sum = {agg_summary['weight_sum']:,}\")\n",
    "\n",
    "    print(f\"\\n  Stage 8: Logical Consistency Cleaning\")\n",
    "    print(f\"    - Inconsistent records removed: {removed_count:,}\")\n",
    "    print(f\"    - Criteria: PhysHlth=30 & GenHlth=1 (contradictory)\")\n",
    "    if 'weight_removed' in globals():\n",
    "        print(f\"    - Weight removed: {weight_removed:,.0f}\")\n",
    "    print(f\"    [VALIDATED] No contradictory records remain\")\n",
    "\n",
    "    print(f\"\\n  Stage 9: Target Re-alignment (Binarization)\")\n",
    "    print(f\"    - Original: Diabetes_012 (0=No, 1=Pre, 2=Yes)\")\n",
    "    print(f\"    - Binarized: Diabetes_binary (0=Negative, 1=Positive)\")\n",
    "    print(f\"    [VALIDATED] Mapping correctness verified\")\n",
    "\n",
    "    print(f\"\\n  Stage 10: Feature Cleanup\")\n",
    "    print(f\"    - Dropped: Diabetes_012 (prevent target leakage)\")\n",
    "    print(f\"    - Retained: Diabetes_binary (binary target)\")\n",
    "    print(f\"    [VALIDATED] No data leakage\")\n",
    "\n",
    "    print(f\"\\n  Stage 11: Ultimate Deduplication (Critical)\")\n",
    "    print(f\"    - Records before: {records_before:,}\")\n",
    "    print(f\"    - Duplicates merged: {merged_count}\")\n",
    "    print(f\"    - Records after: {records_after:,}\")\n",
    "    print(f\"    [VALIDATED] No duplicate profiles remain\")\n",
    "\n",
    "    print(f\"\\n  Stage 12: Weight Normalization\")\n",
    "    print(f\"    - Pre-normalization mean: {pre_norm_mean:.4f}\")\n",
    "    print(f\"    - Post-normalization mean: {post_norm_mean:.6f}\")\n",
    "    print(f\"    [VALIDATED] Mean = 1.0, numerical stability ensured\")\n",
    "\n",
    "    print(f\"\\n  Stage 13: Post-Normalization Verification\")\n",
    "    print(f\"    - Complete duplicates: {full_dup}\")\n",
    "    print(f\"    - Feature duplicates: {feature_dup}\")\n",
    "    print(f\"    [VALIDATED] Zero duplicates after normalization\")\n",
    "    \n",
    "    print(f\"\\n  Stage 14: Export Final Dataset\")\n",
    "    print(f\"    - Destination: {OUTPUT_FILE.name}\")\n",
    "    print(f\"    - File saved successfully\")\n",
    "    print(f\"    [VALIDATED] Export completed\")\n",
    "\n",
    "    print(\"\\n[OUTPUT]\")\n",
    "    print(f\"  File: {OUTPUT_FILE.name}\")\n",
    "    print(f\"  Final shape: {df_final.shape}\")\n",
    "    print(f\"  Composition: 21 features + Diabetes_binary (target) + Sample_Weight\")\n",
    "    print(f\"  Records: {len(df_final):,}\")\n",
    "    print(f\"  Class distribution:\")\n",
    "    print(f\"    - Negative class: {actual_negative:,}\")\n",
    "    print(f\"    - Positive class: {actual_positive:,}\")\n",
    "    print(f\"    - Balance ratio: {actual_positive/actual_negative:.4f}\")\n",
    "\n",
    "    print(\"\\n[STATUS]\")\n",
    "    print(f\"  [VALIDATED] All validation checks passed\")\n",
    "    print(f\"  [VALIDATED] Dataset ready for feature engineering (Phase 3)\")\n",
    "    print(\"\\n\" + \"#\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "DATA PREPROCESSING PIPELINE SUMMARY\n",
      "############################################################\n",
      "\n",
      "[INPUT]\n",
      "  Source file: CDC Diabetes Dataset.csv\n",
      "  Raw records: 253,680\n",
      "\n",
      "[PROCESSING STAGES]\n",
      "  Stage 1: Environment Configuration\n",
      "    - Python, Pandas, NumPy version check\n",
      "    - Display options configured\n",
      "    [VALIDATED] Runtime environment initialized\n",
      "\n",
      "  Stage 2: Path Configuration\n",
      "    - Project root: DiaMetric-CDC\n",
      "    - Input: CDC Diabetes Dataset.csv\n",
      "    - Output: CDC_Diabetes_Cleaned.csv\n",
      "    [VALIDATED] File paths configured\n",
      "\n",
      "  Stage 3: Utility Functions\n",
      "    - load_dataset(), optimize_memory(), aggregate_profiles()\n",
      "    - verify_data_integrity(), save_processed_data()\n",
      "    [VALIDATED] Helper functions defined\n",
      "\n",
      "  Stage 4: Data Ingestion\n",
      "    - Loaded: CDC Diabetes Dataset.csv\n",
      "    - Expected columns: 22\n",
      "    [VALIDATED] Dataset loaded successfully\n",
      "\n",
      "  Stage 5: Memory Optimization\n",
      "    - Downcast numeric types to int8/int16\n",
      "    - Memory footprint reduced\n",
      "    [VALIDATED] Data types optimized\n",
      "\n",
      "  Stage 6-7: Profile Aggregation & Verification\n",
      "    - Unique profiles: 229,781\n",
      "    - Duplicates merged: 23,899\n",
      "    - Data reduction: 9.42%\n",
      "    - Added: Sample_Weight column\n",
      "    [VALIDATED] Weight sum = 253,680\n",
      "\n",
      "  Stage 8: Logical Consistency Cleaning\n",
      "    - Inconsistent records removed: 416\n",
      "    - Criteria: PhysHlth=30 & GenHlth=1 (contradictory)\n",
      "    - Weight removed: 416\n",
      "    [VALIDATED] No contradictory records remain\n",
      "\n",
      "  Stage 9: Target Re-alignment (Binarization)\n",
      "    - Original: Diabetes_012 (0=No, 1=Pre, 2=Yes)\n",
      "    - Binarized: Diabetes_binary (0=Negative, 1=Positive)\n",
      "    [VALIDATED] Mapping correctness verified\n",
      "\n",
      "  Stage 10: Feature Cleanup\n",
      "    - Dropped: Diabetes_012 (prevent target leakage)\n",
      "    - Retained: Diabetes_binary (binary target)\n",
      "    [VALIDATED] No data leakage\n",
      "\n",
      "  Stage 11: Ultimate Deduplication (Critical)\n",
      "    - Records before: 229,365\n",
      "    - Duplicates merged: 69\n",
      "    - Records after: 229,296\n",
      "    [VALIDATED] No duplicate profiles remain\n",
      "\n",
      "  Stage 12: Weight Normalization\n",
      "    - Pre-normalization mean: 1.1045\n",
      "    - Post-normalization mean: 1.000000\n",
      "    [VALIDATED] Mean = 1.0, numerical stability ensured\n",
      "\n",
      "  Stage 13: Post-Normalization Verification\n",
      "    - Complete duplicates: 0\n",
      "    - Feature duplicates: 0\n",
      "    [VALIDATED] Zero duplicates after normalization\n",
      "\n",
      "  Stage 14: Export Final Dataset\n",
      "    - Destination: CDC_Diabetes_Cleaned.csv\n",
      "    - File saved successfully\n",
      "    [VALIDATED] Export completed\n",
      "\n",
      "[OUTPUT]\n",
      "  File: CDC_Diabetes_Cleaned.csv\n",
      "  Final shape: (229296, 23)\n",
      "  Composition: 21 features + Diabetes_binary (target) + Sample_Weight\n",
      "  Records: 229,296\n",
      "  Class distribution:\n",
      "    - Negative class: 189,697\n",
      "    - Positive class: 39,599\n",
      "    - Balance ratio: 0.2087\n",
      "\n",
      "[STATUS]\n",
      "  [VALIDATED] All validation checks passed\n",
      "  [VALIDATED] Dataset ready for feature engineering (Phase 3)\n",
      "\n",
      "############################################################\n"
     ]
    }
   ],
   "execution_count": 51
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiaMetric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
