{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf87dd71",
   "metadata": {},
   "source": [
    "# Phase 2: Data Preprocessing\n",
    "\n",
    "## Profile Aggregation, Quality Control, and Target Re-alignment\n",
    "\n",
    "This notebook implements a comprehensive preprocessing pipeline for the CDC Diabetes Health Indicators dataset. The strategy aggregates identical clinical profiles while preserving statistical representativeness through frequency-based sample weights. Additional stages ensure logical consistency, weight normalization, and binary target construction for downstream classification modeling.\n",
    "\n",
    "**Pipeline Stages:**\n",
    "1. Environment Configuration\n",
    "2. Path Configuration\n",
    "3. Utility Functions\n",
    "4. Data Ingestion\n",
    "5. Memory Optimization\n",
    "6. Profile Aggregation\n",
    "7. Aggregation Verification\n",
    "8. Logical Consistency Cleaning\n",
    "9. Weight Normalization\n",
    "10. Target Re-alignment (Binarization)\n",
    "11. Feature Cleanup\n",
    "12. Export Final Dataset\n",
    "13. Preprocessing Pipeline Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1895bd",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "2819bf38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:25.100236100Z",
     "start_time": "2026-01-31T14:57:25.061982Z"
    }
   },
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display configuration\n",
    "pd.set_option('display.max_columns', 25)\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.13.11 | packaged by Anaconda, Inc. | (main, Dec 10 2025, 21:21:58) [MSC v.1929 64 bit (AMD64)]\n",
      "Pandas version: 3.0.0\n",
      "NumPy version: 2.3.5\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "id": "7ae375b1",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "0ffa30b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:25.178100100Z",
     "start_time": "2026-01-31T14:57:25.111234700Z"
    }
   },
   "source": [
    "# Define project directory structure\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "DATA_PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "# Source and destination file paths\n",
    "INPUT_FILE = DATA_RAW_DIR / \"CDC Diabetes Dataset.csv\"\n",
    "OUTPUT_FILE = DATA_PROCESSED_DIR / \"CDC_Diabetes_Cleaned.csv\"\n",
    "\n",
    "print(f\"[CONFIG] Project root: {PROJECT_ROOT}\")\n",
    "print(f\"[CONFIG] Input file: {INPUT_FILE}\")\n",
    "print(f\"[CONFIG] Output file: {OUTPUT_FILE}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIG] Project root: D:\\ProgramSoftware\\PyCharm\\WorkPlace\\DiaMetric-CDC\n",
      "[CONFIG] Input file: D:\\ProgramSoftware\\PyCharm\\WorkPlace\\DiaMetric-CDC\\data\\raw\\CDC Diabetes Dataset.csv\n",
      "[CONFIG] Output file: D:\\ProgramSoftware\\PyCharm\\WorkPlace\\DiaMetric-CDC\\data\\processed\\CDC_Diabetes_Cleaned.csv\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "7c09774d",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "5338bc82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:25.238102500Z",
     "start_time": "2026-01-31T14:57:25.201104600Z"
    }
   },
   "source": [
    "def load_dataset(filepath: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load CSV dataset with defensive error handling.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : Path\n",
    "        Absolute or relative path to the CSV file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Loaded dataframe.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    FileNotFoundError\n",
    "        If the specified file does not exist.\n",
    "    ValueError\n",
    "        If the file is empty or malformed.\n",
    "    \"\"\"\n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found at: {filepath}\")\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    if df.empty:\n",
    "        raise ValueError(f\"Dataset is empty: {filepath}\")\n",
    "    \n",
    "    print(f\"[SUCCESS] Loaded {len(df):,} records with {df.shape[1]} columns.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def optimize_memory(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downcast numeric columns to reduce memory footprint.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with numeric columns.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Memory-optimized dataframe.\n",
    "    \"\"\"\n",
    "    initial_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    \n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    \n",
    "    final_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    reduction_pct = (1 - final_mem / initial_mem) * 100\n",
    "    \n",
    "    print(f\"[MEMORY] {initial_mem:.2f} MB -> {final_mem:.2f} MB ({reduction_pct:.1f}% reduction)\")\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "2cbe5de3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:25.305739100Z",
     "start_time": "2026-01-31T14:57:25.273100400Z"
    }
   },
   "source": [
    "def aggregate_profiles(df: pd.DataFrame) -> Tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Aggregate identical clinical profiles and compute sample weights.\n",
    "    \n",
    "    This function groups all records by their feature values, treating\n",
    "    duplicate rows as repeated observations of the same clinical archetype.\n",
    "    The frequency of each profile is stored as Sample_Weight.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Raw dataframe with potential duplicate profiles.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[pd.DataFrame, dict]\n",
    "        - Aggregated dataframe with unique profiles and Sample_Weight column.\n",
    "        - Summary statistics dictionary.\n",
    "    \"\"\"\n",
    "    original_count = len(df)\n",
    "    feature_columns = df.columns.tolist()\n",
    "    \n",
    "    # Group by all columns and compute frequency\n",
    "    df_aggregated = (\n",
    "        df.groupby(feature_columns, as_index=False)\n",
    "        .size()\n",
    "        .rename(columns={'size': 'Sample_Weight'})\n",
    "    )\n",
    "    \n",
    "    unique_count = len(df_aggregated)\n",
    "    reduction_pct = (1 - unique_count / original_count) * 100\n",
    "    \n",
    "    summary = {\n",
    "        'original_count': original_count,\n",
    "        'unique_profiles': unique_count,\n",
    "        'duplicates_removed': original_count - unique_count,\n",
    "        'reduction_percentage': reduction_pct,\n",
    "        'weight_sum': df_aggregated['Sample_Weight'].sum()\n",
    "    }\n",
    "    \n",
    "    return df_aggregated, summary\n",
    "\n",
    "\n",
    "def verify_data_integrity(\n",
    "    df: pd.DataFrame,\n",
    "    original_count: int,\n",
    "    expected_columns: int = 23\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Assert data integrity constraints after aggregation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Aggregated dataframe to verify.\n",
    "    original_count : int\n",
    "        Original row count before aggregation.\n",
    "    expected_columns : int, optional\n",
    "        Expected number of columns (22 features + 1 weight), by default 23.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    AssertionError\n",
    "        If any integrity constraint is violated.\n",
    "    \"\"\"\n",
    "    # Verify column count (22 original + Sample_Weight)\n",
    "    assert df.shape[1] == expected_columns, (\n",
    "        f\"Column count mismatch: expected {expected_columns}, got {df.shape[1]}\"\n",
    "    )\n",
    "    \n",
    "    # Verify weight summation equals original count\n",
    "    weight_sum = df['Sample_Weight'].sum()\n",
    "    assert weight_sum == original_count, (\n",
    "        f\"Weight sum mismatch: expected {original_count}, got {weight_sum}\"\n",
    "    )\n",
    "    \n",
    "    # Verify no null values in weight column\n",
    "    assert df['Sample_Weight'].notna().all(), (\n",
    "        \"Sample_Weight contains null values\"\n",
    "    )\n",
    "    \n",
    "    # Verify all weights are positive integers\n",
    "    assert (df['Sample_Weight'] > 0).all(), (\n",
    "        \"Sample_Weight contains non-positive values\"\n",
    "    )\n",
    "    \n",
    "    print(\"[PASS] All data integrity assertions passed.\")"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "6937fdda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:25.343392600Z",
     "start_time": "2026-01-31T14:57:25.308738600Z"
    }
   },
   "source": [
    "def save_processed_data(df: pd.DataFrame, filepath: Path) -> None:\n",
    "    \"\"\"\n",
    "    Export processed dataframe to CSV with directory creation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe to export.\n",
    "    filepath : Path\n",
    "        Destination file path.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    df.to_csv(filepath, index=False)\n",
    "    \n",
    "    file_size_mb = filepath.stat().st_size / 1024**2\n",
    "    print(f\"[SUCCESS] Saved to: {filepath}\")\n",
    "    print(f\"[INFO] File size: {file_size_mb:.2f} MB\")"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "id": "dfced143",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "id": "0f5f3f0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:25.856160700Z",
     "start_time": "2026-01-31T14:57:25.360393200Z"
    }
   },
   "source": [
    "# Load raw dataset\n",
    "df_raw = load_dataset(INPUT_FILE)\n",
    "\n",
    "# Display initial structure\n",
    "print(f\"\\n[INFO] Shape: {df_raw.shape}\")\n",
    "print(f\"[INFO] Columns: {df_raw.columns.tolist()}\")\n",
    "df_raw.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Loaded 253,680 records with 22 columns.\n",
      "\n",
      "[INFO] Shape: (253680, 22)\n",
      "[INFO] Columns: ['Diabetes_012', 'HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Diabetes_012  HighBP  HighChol  CholCheck     BMI  Smoker  Stroke  HeartDiseaseorAttack  PhysActivity  Fruits  Veggies  HvyAlcoholConsump  AnyHealthcare  NoDocbcCost  GenHlth  MentHlth  PhysHlth  \\\n",
       "0        0.0000  1.0000    1.0000     1.0000 40.0000  1.0000  0.0000                0.0000        0.0000  0.0000   1.0000             0.0000         1.0000       0.0000   5.0000   18.0000   15.0000   \n",
       "1        0.0000  0.0000    0.0000     0.0000 25.0000  1.0000  0.0000                0.0000        1.0000  0.0000   0.0000             0.0000         0.0000       1.0000   3.0000    0.0000    0.0000   \n",
       "2        0.0000  1.0000    1.0000     1.0000 28.0000  0.0000  0.0000                0.0000        0.0000  1.0000   0.0000             0.0000         1.0000       1.0000   5.0000   30.0000   30.0000   \n",
       "3        0.0000  1.0000    0.0000     1.0000 27.0000  0.0000  0.0000                0.0000        1.0000  1.0000   1.0000             0.0000         1.0000       0.0000   2.0000    0.0000    0.0000   \n",
       "4        0.0000  1.0000    1.0000     1.0000 24.0000  0.0000  0.0000                0.0000        1.0000  1.0000   1.0000             0.0000         1.0000       0.0000   2.0000    3.0000    0.0000   \n",
       "\n",
       "   DiffWalk    Sex     Age  Education  Income  \n",
       "0    1.0000 0.0000  9.0000     4.0000  3.0000  \n",
       "1    0.0000 0.0000  7.0000     6.0000  1.0000  \n",
       "2    1.0000 0.0000  9.0000     4.0000  8.0000  \n",
       "3    0.0000 0.0000 11.0000     3.0000  6.0000  \n",
       "4    0.0000 0.0000 11.0000     5.0000  4.0000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes_012</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>HvyAlcoholConsump</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>40.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>3.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>28.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>8.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>27.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>6.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>4.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "9dcbee16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:26.117650800Z",
     "start_time": "2026-01-31T14:57:26.038954700Z"
    }
   },
   "source": [
    "# Verify expected column count\n",
    "EXPECTED_RAW_COLUMNS = 22\n",
    "assert df_raw.shape[1] == EXPECTED_RAW_COLUMNS, (\n",
    "    f\"Expected {EXPECTED_RAW_COLUMNS} columns, got {df_raw.shape[1]}\"\n",
    ")\n",
    "print(f\"[PASS] Column count verified: {df_raw.shape[1]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PASS] Column count verified: 22\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "id": "91bdf33d",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "id": "613d082e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:26.585742100Z",
     "start_time": "2026-01-31T14:57:26.253200600Z"
    }
   },
   "source": [
    "# Downcast numeric types to reduce memory footprint\n",
    "df_optimized = optimize_memory(df_raw.copy())\n",
    "\n",
    "# Display optimized dtypes\n",
    "print(\"\\n[INFO] Optimized data types:\")\n",
    "print(df_optimized.dtypes.value_counts())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MEMORY] 42.58 MB -> 21.29 MB (50.0% reduction)\n",
      "\n",
      "[INFO] Optimized data types:\n",
      "float32    22\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "id": "3dc64202",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Profile Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "id": "9f6cc5f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:27.004626900Z",
     "start_time": "2026-01-31T14:57:26.663958100Z"
    }
   },
   "source": [
    "# Execute profile convergence strategy\n",
    "df_aggregated, agg_summary = aggregate_profiles(df_optimized)\n",
    "\n",
    "# Display aggregation summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROFILE AGGREGATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original record count:    {agg_summary['original_count']:>12,}\")\n",
    "print(f\"Unique clinical profiles: {agg_summary['unique_profiles']:>12,}\")\n",
    "print(f\"Duplicate records merged: {agg_summary['duplicates_removed']:>12,}\")\n",
    "print(f\"Data reduction:           {agg_summary['reduction_percentage']:>11.2f}%\")\n",
    "print(f\"Weight sum verification:  {agg_summary['weight_sum']:>12,}\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROFILE AGGREGATION SUMMARY\n",
      "============================================================\n",
      "Original record count:         253,680\n",
      "Unique clinical profiles:      229,781\n",
      "Duplicate records merged:       23,899\n",
      "Data reduction:                  9.42%\n",
      "Weight sum verification:       253,680\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "id": "c1406360",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:27.102757300Z",
     "start_time": "2026-01-31T14:57:27.072021Z"
    }
   },
   "source": [
    "# Examine weight distribution\n",
    "print(\"[INFO] Sample_Weight distribution:\")\n",
    "print(df_aggregated['Sample_Weight'].describe())\n",
    "\n",
    "print(f\"\\n[INFO] Profiles with weight > 1: {(df_aggregated['Sample_Weight'] > 1).sum():,}\")\n",
    "print(f\"[INFO] Maximum weight: {df_aggregated['Sample_Weight'].max():,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Sample_Weight distribution:\n",
      "count   229781.0000\n",
      "mean         1.1040\n",
      "std          0.8490\n",
      "min          1.0000\n",
      "25%          1.0000\n",
      "50%          1.0000\n",
      "75%          1.0000\n",
      "max         59.0000\n",
      "Name: Sample_Weight, dtype: float64\n",
      "\n",
      "[INFO] Profiles with weight > 1: 11,187\n",
      "[INFO] Maximum weight: 59\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "id": "e99d9857",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Aggregation Verification"
   ]
  },
  {
   "cell_type": "code",
   "id": "a2ddd48e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:27.172001700Z",
     "start_time": "2026-01-31T14:57:27.145242700Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 1 VERIFICATION: Profile Aggregation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Execute integrity assertions\n",
    "verify_data_integrity(\n",
    "    df=df_aggregated,\n",
    "    original_count=agg_summary['original_count'],\n",
    "    expected_columns=23  # 22 features + Sample_Weight\n",
    ")\n",
    "\n",
    "# Verify weight statistics\n",
    "assert df_aggregated['Sample_Weight'].min() > 0, \"Invalid: negative or zero weights detected\"\n",
    "assert df_aggregated['Sample_Weight'].sum() == agg_summary['original_count'], \"Weight sum mismatch\"\n",
    "\n",
    "print(f\"\\n[VALIDATED] Shape: {df_aggregated.shape}\")\n",
    "print(f\"[VALIDATED] Weight range: [{df_aggregated['Sample_Weight'].min()}, {df_aggregated['Sample_Weight'].max()}]\")\n",
    "print(f\"[VALIDATED] Weight mean: {df_aggregated['Sample_Weight'].mean():.2f}\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 1 VERIFICATION: Profile Aggregation\n",
      "============================================================\n",
      "[PASS] All data integrity assertions passed.\n",
      "\n",
      "[VALIDATED] Shape: (229781, 23)\n",
      "[VALIDATED] Weight range: [1, 59]\n",
      "[VALIDATED] Weight mean: 1.10\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "id": "a7e8e9f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:27.203923600Z",
     "start_time": "2026-01-31T14:57:27.178070500Z"
    }
   },
   "source": [
    "# Display aggregated dataframe structure\n",
    "print(f\"\\n[INFO] Aggregated dataset preview:\")\n",
    "df_aggregated.head(10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Aggregated dataset preview:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Diabetes_012  HighBP  HighChol  CholCheck     BMI  Smoker  Stroke  HeartDiseaseorAttack  PhysActivity  Fruits  Veggies  HvyAlcoholConsump  AnyHealthcare  NoDocbcCost  GenHlth  MentHlth  PhysHlth  \\\n",
       "0        0.0000  0.0000    0.0000     0.0000 14.0000  1.0000  0.0000                0.0000        1.0000  1.0000   1.0000             0.0000         1.0000       0.0000   3.0000    4.0000    4.0000   \n",
       "1        0.0000  0.0000    0.0000     0.0000 15.0000  0.0000  0.0000                0.0000        0.0000  1.0000   0.0000             0.0000         0.0000       0.0000   1.0000    0.0000    0.0000   \n",
       "2        0.0000  0.0000    0.0000     0.0000 15.0000  1.0000  0.0000                0.0000        0.0000  0.0000   0.0000             0.0000         1.0000       0.0000   5.0000   20.0000   28.0000   \n",
       "3        0.0000  0.0000    0.0000     0.0000 15.0000  1.0000  0.0000                1.0000        0.0000  1.0000   1.0000             0.0000         0.0000       1.0000   3.0000    0.0000   29.0000   \n",
       "4        0.0000  0.0000    0.0000     0.0000 16.0000  0.0000  0.0000                0.0000        0.0000  0.0000   0.0000             0.0000         1.0000       0.0000   2.0000    0.0000    0.0000   \n",
       "5        0.0000  0.0000    0.0000     0.0000 16.0000  0.0000  0.0000                0.0000        0.0000  1.0000   1.0000             0.0000         1.0000       0.0000   2.0000    7.0000    5.0000   \n",
       "6        0.0000  0.0000    0.0000     0.0000 16.0000  0.0000  0.0000                0.0000        1.0000  0.0000   0.0000             0.0000         1.0000       1.0000   2.0000    0.0000    0.0000   \n",
       "7        0.0000  0.0000    0.0000     0.0000 16.0000  0.0000  0.0000                0.0000        1.0000  0.0000   1.0000             0.0000         1.0000       0.0000   2.0000    0.0000    4.0000   \n",
       "8        0.0000  0.0000    0.0000     0.0000 16.0000  0.0000  0.0000                0.0000        1.0000  1.0000   1.0000             0.0000         0.0000       0.0000   2.0000    0.0000    0.0000   \n",
       "9        0.0000  0.0000    0.0000     0.0000 16.0000  0.0000  0.0000                1.0000        1.0000  0.0000   1.0000             0.0000         1.0000       1.0000   4.0000    0.0000    3.0000   \n",
       "\n",
       "   DiffWalk    Sex     Age  Education  Income  Sample_Weight  \n",
       "0    0.0000 1.0000 11.0000     6.0000  8.0000              1  \n",
       "1    0.0000 0.0000  1.0000     5.0000  7.0000              1  \n",
       "2    1.0000 0.0000 10.0000     6.0000  4.0000              1  \n",
       "3    0.0000 0.0000  7.0000     5.0000  2.0000              1  \n",
       "4    0.0000 0.0000 11.0000     5.0000  5.0000              1  \n",
       "5    0.0000 0.0000  1.0000     5.0000  5.0000              1  \n",
       "6    0.0000 0.0000  6.0000     5.0000  6.0000              1  \n",
       "7    0.0000 0.0000  3.0000     6.0000  8.0000              1  \n",
       "8    0.0000 0.0000  9.0000     5.0000  2.0000              1  \n",
       "9    0.0000 0.0000  6.0000     6.0000  2.0000              1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes_012</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>HvyAlcoholConsump</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "      <th>Sample_Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>14.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>28.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "id": "c355e85c",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Logical Consistency Cleaning\n",
    "\n",
    "Records exhibiting logical inconsistencies between self-reported health indicators are identified and removed. Specifically, profiles reporting 30 days of poor physical health (`PhysHlth=30`) while simultaneously claiming excellent general health (`GenHlth=1`) represent contradictory self-assessments that may compromise model reliability."
   ]
  },
  {
   "cell_type": "code",
   "id": "f0ddb500",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:27.431144100Z",
     "start_time": "2026-01-31T14:57:27.324737900Z"
    }
   },
   "source": [
    "def remove_logical_inconsistencies(df: pd.DataFrame) -> Tuple[pd.DataFrame, int]:\n",
    "    \"\"\"\n",
    "    Remove records with contradictory health indicator combinations.\n",
    "    \n",
    "    Logical inconsistency definition:\n",
    "    - PhysHlth = 30 (poor physical health for all 30 days)\n",
    "    - GenHlth = 1 (self-reported Excellent general health)\n",
    "    \n",
    "    These mutually exclusive conditions indicate response errors or\n",
    "    misinterpretation of survey questions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with PhysHlth and GenHlth columns.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[pd.DataFrame, int]\n",
    "        - Cleaned dataframe with inconsistent records removed.\n",
    "        - Count of removed records.\n",
    "    \"\"\"\n",
    "    initial_count = len(df)\n",
    "    \n",
    "    # Define inconsistency mask: maximum physical distress + excellent health\n",
    "    inconsistency_mask = (df['PhysHlth'] == 30) & (df['GenHlth'] == 1)\n",
    "    \n",
    "    # Filter out inconsistent records\n",
    "    df_cleaned = df[~inconsistency_mask].copy()\n",
    "    \n",
    "    removed_count = initial_count - len(df_cleaned)\n",
    "    \n",
    "    return df_cleaned, removed_count"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "id": "de5d13a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:27.930915700Z",
     "start_time": "2026-01-31T14:57:27.712762900Z"
    }
   },
   "source": [
    "# Execute logical consistency cleaning\n",
    "df_consistent, removed_count = remove_logical_inconsistencies(df_aggregated)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 2 VERIFICATION: Logical Consistency Cleaning\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Records before cleaning:  {len(df_aggregated):>12,}\")\n",
    "print(f\"Inconsistent records:     {removed_count:>12,}\")\n",
    "print(f\"Records after cleaning:   {len(df_consistent):>12,}\")\n",
    "\n",
    "# Verify no contradictory records remain\n",
    "inconsistent_check = ((df_consistent['PhysHlth'] == 30) & (df_consistent['GenHlth'] == 1)).sum()\n",
    "assert inconsistent_check == 0, f\"Inconsistency removal failed: {inconsistent_check} contradictory records remain\"\n",
    "\n",
    "print(f\"\\n[VALIDATED] No contradictory records remaining: {inconsistent_check}\")\n",
    "print(f\"[VALIDATED] Data integrity maintained\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 2 VERIFICATION: Logical Consistency Cleaning\n",
      "============================================================\n",
      "Records before cleaning:       229,781\n",
      "Inconsistent records:              416\n",
      "Records after cleaning:        229,365\n",
      "\n",
      "[VALIDATED] No contradictory records remaining: 0\n",
      "[VALIDATED] Data integrity maintained\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "id": "a99d0d75",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Weight Normalization\n",
    "\n",
    "After logical consistency cleaning, the sample weights are normalized to ensure numerical stability and interpretability. Each weight is divided by the mean weight, resulting in a distribution centered at 1.0. This transformation preserves relative prevalence ratios while standardizing the magnitude scale."
   ]
  },
  {
   "cell_type": "code",
   "id": "3b3fc1fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:28.042061500Z",
     "start_time": "2026-01-31T14:57:28.023878Z"
    }
   },
   "source": [
    "def normalize_weights(df: pd.DataFrame, weight_col: str = 'Sample_Weight') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize sample weights by dividing each weight by the mean.\n",
    "    \n",
    "    This ensures the weight distribution has a mean of 1.0, facilitating\n",
    "    stable numerical computations in downstream modeling while preserving\n",
    "    the relative prevalence ratios between clinical profiles.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with raw sample weights.\n",
    "    weight_col : str, optional\n",
    "        Name of the weight column, by default 'Sample_Weight'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe with normalized weights.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    mean_weight = df[weight_col].mean()\n",
    "    df[weight_col] = df[weight_col] / mean_weight\n",
    "    \n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "id": "63e3e14f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:28.212272700Z",
     "start_time": "2026-01-31T14:57:28.161027300Z"
    }
   },
   "source": [
    "# Execute weight normalization on cleaned dataset\n",
    "pre_norm_mean = df_consistent['Sample_Weight'].mean()\n",
    "df_normalized = normalize_weights(df_consistent)\n",
    "post_norm_mean = df_normalized['Sample_Weight'].mean()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 3 VERIFICATION: Weight Normalization\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Pre-normalization mean:   {pre_norm_mean:.6f}\")\n",
    "print(f\"Post-normalization mean:  {post_norm_mean:.6f}\")\n",
    "\n",
    "# Validate mean equals 1.0 (within floating-point tolerance)\n",
    "assert np.isclose(post_norm_mean, 1.0, atol=1e-10), (\n",
    "    f\"Weight normalization failed: mean = {post_norm_mean}, expected 1.0\"\n",
    ")\n",
    "\n",
    "# Verify weight distribution properties\n",
    "weight_std = df_normalized['Sample_Weight'].std()\n",
    "weight_min = df_normalized['Sample_Weight'].min()\n",
    "weight_max = df_normalized['Sample_Weight'].max()\n",
    "\n",
    "print(f\"\\n[VALIDATED] Weight mean: {post_norm_mean:.6f} (target: 1.0)\")\n",
    "print(f\"[VALIDATED] Weight std: {weight_std:.4f}\")\n",
    "print(f\"[VALIDATED] Weight range: [{weight_min:.4f}, {weight_max:.4f}]\")\n",
    "\n",
    "print(f\"\\nWeight distribution:\")\n",
    "print(df_normalized['Sample_Weight'].describe())\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 3 VERIFICATION: Weight Normalization\n",
      "============================================================\n",
      "Pre-normalization mean:   1.104196\n",
      "Post-normalization mean:  1.000000\n",
      "\n",
      "[VALIDATED] Weight mean: 1.000000 (target: 1.0)\n",
      "[VALIDATED] Weight std: 0.7696\n",
      "[VALIDATED] Weight range: [0.9056, 53.4325]\n",
      "\n",
      "Weight distribution:\n",
      "count   229365.0000\n",
      "mean         1.0000\n",
      "std          0.7696\n",
      "min          0.9056\n",
      "25%          0.9056\n",
      "50%          0.9056\n",
      "75%          0.9056\n",
      "max         53.4325\n",
      "Name: Sample_Weight, dtype: float64\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "id": "d006a593",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Target Re-alignment (Binarization)\n",
    "\n",
    "The original trinary target variable (`Diabetes_012`) is transformed into a binary classification target. Pre-diabetic cases (value 1) are consolidated with diabetic cases (value 2) to create a unified positive class, enabling binary classification modeling."
   ]
  },
  {
   "cell_type": "code",
   "id": "1ecff04b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:28.272652400Z",
     "start_time": "2026-01-31T14:57:28.261287600Z"
    }
   },
   "source": [
    "def binarize_target(df: pd.DataFrame, source_col: str = 'Diabetes_012') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert trinary diabetes target to binary classification target.\n",
    "    \n",
    "    Mapping logic:\n",
    "    - 0 (No diabetes) -> 0 (Negative class)\n",
    "    - 1 (Pre-diabetes) -> 1 (Positive class)\n",
    "    - 2 (Diabetes) -> 1 (Positive class)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with trinary target column.\n",
    "    source_col : str, optional\n",
    "        Name of the original target column, by default 'Diabetes_012'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe with new binary target column 'Diabetes_binary'.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Binary mapping: 0 stays 0, any positive value becomes 1\n",
    "    df['Diabetes_binary'] = df[source_col].apply(lambda x: 0 if x == 0 else 1)\n",
    "    \n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "id": "3c62bae8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:28.396475400Z",
     "start_time": "2026-01-31T14:57:28.275648200Z"
    }
   },
   "source": [
    "# Execute target binarization on normalized dataset\n",
    "df_binarized = binarize_target(df_normalized)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 4 VERIFICATION: Target Re-alignment (Binarization)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nOriginal target distribution (Diabetes_012):\")\n",
    "original_dist = df_binarized['Diabetes_012'].value_counts().sort_index()\n",
    "for val, count in original_dist.items():\n",
    "    label = {0: 'No diabetes', 1: 'Pre-diabetes', 2: 'Diabetes'}[int(val)]\n",
    "    print(f\"  {int(val)} ({label}): {count:,}\")\n",
    "\n",
    "print(\"\\nBinarized target distribution (Diabetes_binary):\")\n",
    "binary_dist = df_binarized['Diabetes_binary'].value_counts().sort_index()\n",
    "for val, count in binary_dist.items():\n",
    "    label = {0: 'Negative', 1: 'Positive'}[val]\n",
    "    print(f\"  {val} ({label}): {count:,}\")\n",
    "\n",
    "# Verify mapping correctness\n",
    "expected_negative = original_dist.get(0, 0)\n",
    "expected_positive = original_dist.get(1, 0) + original_dist.get(2, 0)\n",
    "actual_negative = binary_dist.get(0, 0)\n",
    "actual_positive = binary_dist.get(1, 0)\n",
    "\n",
    "assert expected_negative == actual_negative, \"Negative class count mismatch\"\n",
    "assert expected_positive == actual_positive, \"Positive class count mismatch\"\n",
    "\n",
    "# Verify only binary values exist\n",
    "assert set(df_binarized['Diabetes_binary'].unique()) == {0, 1}, \"Invalid values in binary target\"\n",
    "\n",
    "print(f\"\\n[VALIDATED] Mapping correctness: Negative={actual_negative:,}, Positive={actual_positive:,}\")\n",
    "print(f\"[VALIDATED] Binary target contains only values: {sorted(df_binarized['Diabetes_binary'].unique())}\")\n",
    "print(f\"[VALIDATED] Class balance ratio: {actual_positive/actual_negative:.4f}\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 4 VERIFICATION: Target Re-alignment (Binarization)\n",
      "============================================================\n",
      "\n",
      "Original target distribution (Diabetes_012):\n",
      "  0 (No diabetes): 189,697\n",
      "  1 (Pre-diabetes): 4,620\n",
      "  2 (Diabetes): 35,048\n",
      "\n",
      "Binarized target distribution (Diabetes_binary):\n",
      "  0 (Negative): 189,697\n",
      "  1 (Positive): 39,668\n",
      "\n",
      "[VALIDATED] Mapping correctness: Negative=189,697, Positive=39,668\n",
      "[VALIDATED] Binary target contains only values: [np.int64(0), np.int64(1)]\n",
      "[VALIDATED] Class balance ratio: 0.2091\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "id": "19164132",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Feature Cleanup\n",
    "\n",
    "The original trinary target column is removed to prevent target leakage and ensure the downstream modeling pipeline uses only the binary target variable."
   ]
  },
  {
   "cell_type": "code",
   "id": "ddd62685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:28.431925600Z",
     "start_time": "2026-01-31T14:57:28.400475600Z"
    }
   },
   "source": [
    "# Drop original trinary target to prevent leakage\n",
    "df_final = df_binarized.drop(columns=['Diabetes_012'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 5 VERIFICATION: Feature Cleanup\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify column removalprint(f\"\\n[INFO] Final columns: {df_final.columns.tolist()}\")\n",
    "\n",
    "assert 'Diabetes_012' not in df_final.columns, \"Target leakage: Diabetes_012 column still exists\"\n",
    "print(f\"[VALIDATED] Final shape: {df_final.shape}\")\n",
    "\n",
    "assert 'Diabetes_binary' in df_final.columns, \"Binary target column missing\"\n",
    "print(f\"[VALIDATED] Final column count: {df_final.shape[1]}\")\n",
    "\n",
    "print(f\"[VALIDATED] Binary target preserved: Diabetes_binary\")\n",
    "print(f\"[VALIDATED] Original trinary target removed: Diabetes_012\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 5 VERIFICATION: Feature Cleanup\n",
      "============================================================\n",
      "============================================================\n",
      "[VALIDATED] Final shape: (229365, 23)\n",
      "[VALIDATED] Final column count: 23\n",
      "[VALIDATED] Binary target preserved: Diabetes_binary\n",
      "[VALIDATED] Original trinary target removed: Diabetes_012\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "id": "dd9195be",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Export Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "7ae4692d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:31.841271100Z",
     "start_time": "2026-01-31T14:57:28.435927400Z"
    }
   },
   "source": [
    "# Save final cleaned and processed dataset\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EXPORT\")\n",
    "print(\"=\"*60)\n",
    "save_processed_data(df_final, OUTPUT_FILE)\n",
    "print(f\"[SUCCESS] Dataset ready for feature engineering\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL EXPORT\n",
      "============================================================\n",
      "[SUCCESS] Saved to: D:\\ProgramSoftware\\PyCharm\\WorkPlace\\DiaMetric-CDC\\data\\processed\\CDC_Diabetes_Cleaned.csv\n",
      "[INFO] File size: 23.55 MB\n",
      "[SUCCESS] Dataset ready for feature engineering\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "id": "be1e75ee",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Preprocessing Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "id": "3d019936",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:57:31.928558500Z",
     "start_time": "2026-01-31T14:57:31.879932200Z"
    }
   },
   "source": [
    "# Comprehensive preprocessing summary\n",
    "print(\"\\n\" + \"#\"*60)\n",
    "print(\"DATA PREPROCESSING PIPELINE SUMMARY\")\n",
    "print(\"#\"*60)\n",
    "\n",
    "print(\"\\n[INPUT]\")\n",
    "print(f\"  Source file: {INPUT_FILE.name}\")\n",
    "print(f\"  Raw records: {agg_summary['original_count']:,}\")\n",
    "\n",
    "print(\"\\n[PROCESSING STAGES]\")\n",
    "print(f\"  Stage 1: Profile Aggregation & Weight Calculation\")\n",
    "print(f\"    - Unique profiles: {agg_summary['unique_profiles']:,}\")\n",
    "print(f\"    - Duplicates merged: {agg_summary['duplicates_removed']:,}\")\n",
    "print(f\"    - Data reduction: {agg_summary['reduction_percentage']:.2f}%\")\n",
    "print(f\"    - Added: Sample_Weight column\")\n",
    "print(f\"     Verified: Weight sum = {agg_summary['weight_sum']:,}\")\n",
    "\n",
    "print(f\"\\n  Stage 2: Logical Consistency Cleaning\")\n",
    "print(f\"    - Inconsistent records removed: {removed_count:,}\")\n",
    "print(f\"    - Criteria: PhysHlth=30 & GenHlth=1 (contradictory)\")\n",
    "print(f\"     Verified: No contradictory records remain\")\n",
    "\n",
    "print(f\"\\n  Stage 3: Weight Normalization\")\n",
    "print(f\"    - Pre-normalization mean: {pre_norm_mean:.4f}\")\n",
    "print(f\"    - Post-normalization mean: {post_norm_mean:.6f}\")\n",
    "print(f\"     Verified: Mean = 1.0 (numerical stability ensured)\")\n",
    "\n",
    "print(f\"\\n  Stage 4: Target Re-alignment (Binarization)\")\n",
    "print(f\"    - Original: Diabetes_012 (0=No, 1=Pre, 2=Yes)\")\n",
    "print(f\"    - Binarized: Diabetes_binary (0=Negative, 1=Positive)\")\n",
    "print(f\"    - Negative class: {actual_negative:,}\")\n",
    "print(f\"    - Positive class: {actual_positive:,}\")\n",
    "print(f\"     Verified: Mapping correctness validated\")\n",
    "\n",
    "print(f\"\\n  Stage 5: Feature Cleanup\")\n",
    "print(f\"    - Dropped: Diabetes_012 (prevent target leakage)\")\n",
    "print(f\"    - Retained: Diabetes_binary (binary target)\")\n",
    "print(f\"     Verified: No data leakage\")\n",
    "\n",
    "print(\"\\n[OUTPUT]\")\n",
    "print(f\"  File: {OUTPUT_FILE.name}\")\n",
    "print(f\"  Final shape: {df_final.shape}\")\n",
    "print(f\"  Features: {df_final.shape[1]-2} (+ Diabetes_binary + Sample_Weight)\")\n",
    "print(f\"  Records: {len(df_final):,}\")\n",
    "\n",
    "print(\"\\n[STATUS]\")\n",
    "print(f\"   All validation checks passed\")\n",
    "print(f\"   Dataset ready for feature engineering (Phase 3)\")\n",
    "print(\"\\n\" + \"#\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "DATA PREPROCESSING PIPELINE SUMMARY\n",
      "############################################################\n",
      "\n",
      "[INPUT]\n",
      "  Source file: CDC Diabetes Dataset.csv\n",
      "  Raw records: 253,680\n",
      "\n",
      "[PROCESSING STAGES]\n",
      "  Stage 1: Profile Aggregation & Weight Calculation\n",
      "    - Unique profiles: 229,781\n",
      "    - Duplicates merged: 23,899\n",
      "    - Data reduction: 9.42%\n",
      "    - Added: Sample_Weight column\n",
      "     Verified: Weight sum = 253,680\n",
      "\n",
      "  Stage 2: Logical Consistency Cleaning\n",
      "    - Inconsistent records removed: 416\n",
      "    - Criteria: PhysHlth=30 & GenHlth=1 (contradictory)\n",
      "     Verified: No contradictory records remain\n",
      "\n",
      "  Stage 3: Weight Normalization\n",
      "    - Pre-normalization mean: 1.1042\n",
      "    - Post-normalization mean: 1.000000\n",
      "     Verified: Mean = 1.0 (numerical stability ensured)\n",
      "\n",
      "  Stage 4: Target Re-alignment (Binarization)\n",
      "    - Original: Diabetes_012 (0=No, 1=Pre, 2=Yes)\n",
      "    - Binarized: Diabetes_binary (0=Negative, 1=Positive)\n",
      "    - Negative class: 189,697\n",
      "    - Positive class: 39,668\n",
      "     Verified: Mapping correctness validated\n",
      "\n",
      "  Stage 5: Feature Cleanup\n",
      "    - Dropped: Diabetes_012 (prevent target leakage)\n",
      "    - Retained: Diabetes_binary (binary target)\n",
      "     Verified: No data leakage\n",
      "\n",
      "[OUTPUT]\n",
      "  File: CDC_Diabetes_Cleaned.csv\n",
      "  Final shape: (229365, 23)\n",
      "  Features: 21 (+ Diabetes_binary + Sample_Weight)\n",
      "  Records: 229,365\n",
      "\n",
      "[STATUS]\n",
      "   All validation checks passed\n",
      "   Dataset ready for feature engineering (Phase 3)\n",
      "\n",
      "############################################################\n"
     ]
    }
   ],
   "execution_count": 59
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiaMetric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
