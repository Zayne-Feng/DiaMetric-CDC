{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf87dd71",
   "metadata": {},
   "source": [
    "# Phase 2: Data Preprocessing\n",
    "\n",
    "## Profile Aggregation, Quality Control, and Target Re-alignment\n",
    "\n",
    "This notebook implements a comprehensive preprocessing pipeline for the CDC Diabetes Health Indicators dataset. The strategy aggregates identical clinical profiles while preserving statistical representativeness through frequency-based sample weights. Additional stages ensure logical consistency, weight normalization, and binary target construction for downstream classification modeling.\n",
    "\n",
    "**Pipeline Stages:**\n",
    "1. Environment Configuration\n",
    "2. Path Configuration\n",
    "3. Utility Functions\n",
    "4. Data Ingestion\n",
    "5. Memory Optimization\n",
    "6. Profile Aggregation\n",
    "7. Aggregation Verification\n",
    "8. Logical Consistency Cleaning\n",
    "9. Target Re-alignment (Binarization)\n",
    "10. Feature Cleanup\n",
    "11. Ultimate Deduplication (Critical)\n",
    "12. Weight Normalization\n",
    "13. Post-Normalization Verification\n",
    "14. Export Final Dataset\n",
    "15. Preprocessing Pipeline Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1895bd",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "2819bf38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:36.087837800Z",
     "start_time": "2026-01-31T19:05:36.053862300Z"
    }
   },
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display configuration\n",
    "pd.set_option('display.max_columns', 25)\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.13.11 | packaged by Anaconda, Inc. | (main, Dec 10 2025, 21:21:58) [MSC v.1929 64 bit (AMD64)]\n",
      "Pandas version: 3.0.0\n",
      "NumPy version: 2.3.5\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "7ae375b1",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "0ffa30b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:36.121767700Z",
     "start_time": "2026-01-31T19:05:36.092838200Z"
    }
   },
   "source": [
    "# Define project directory structure\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "DATA_PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "# Source and destination file paths\n",
    "INPUT_FILE = DATA_RAW_DIR / \"CDC Diabetes Dataset.csv\"\n",
    "OUTPUT_FILE = DATA_PROCESSED_DIR / \"CDC_Diabetes_Cleaned.csv\"\n",
    "\n",
    "print(f\"[CONFIG] Project root: {PROJECT_ROOT}\")\n",
    "print(f\"[CONFIG] Input file: {INPUT_FILE}\")\n",
    "print(f\"[CONFIG] Output file: {OUTPUT_FILE}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIG] Project root: D:\\ProgramSoftware\\PyCharm\\WorkPlace\\DiaMetric-CDC\n",
      "[CONFIG] Input file: D:\\ProgramSoftware\\PyCharm\\WorkPlace\\DiaMetric-CDC\\data\\raw\\CDC Diabetes Dataset.csv\n",
      "[CONFIG] Output file: D:\\ProgramSoftware\\PyCharm\\WorkPlace\\DiaMetric-CDC\\data\\processed\\CDC_Diabetes_Cleaned.csv\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "7c09774d",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "5338bc82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:36.140243400Z",
     "start_time": "2026-01-31T19:05:36.125759200Z"
    }
   },
   "source": [
    "def load_dataset(filepath: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load CSV dataset with defensive error handling.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : Path\n",
    "        Absolute or relative path to the CSV file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Loaded dataframe.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    FileNotFoundError\n",
    "        If the specified file does not exist.\n",
    "    ValueError\n",
    "        If the file is empty or malformed.\n",
    "    \"\"\"\n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found at: {filepath}\")\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    if df.empty:\n",
    "        raise ValueError(f\"Dataset is empty: {filepath}\")\n",
    "    \n",
    "    print(f\"[SUCCESS] Loaded {len(df):,} records with {df.shape[1]} columns.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def optimize_memory(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downcast numeric columns to reduce memory footprint.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with numeric columns.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Memory-optimized dataframe.\n",
    "    \"\"\"\n",
    "    initial_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    \n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    \n",
    "    final_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    reduction_pct = (1 - final_mem / initial_mem) * 100\n",
    "    \n",
    "    print(f\"[MEMORY] {initial_mem:.2f} MB -> {final_mem:.2f} MB ({reduction_pct:.1f}% reduction)\")\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "2cbe5de3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:36.156555900Z",
     "start_time": "2026-01-31T19:05:36.143242800Z"
    }
   },
   "source": [
    "def aggregate_profiles(df: pd.DataFrame) -> Tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Aggregate identical clinical profiles and compute sample weights.\n",
    "    \n",
    "    This function groups all records by their feature values, treating\n",
    "    duplicate rows as repeated observations of the same clinical archetype.\n",
    "    The frequency of each profile is stored as Sample_Weight.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Raw dataframe with potential duplicate profiles.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[pd.DataFrame, dict]\n",
    "        - Aggregated dataframe with unique profiles and Sample_Weight column.\n",
    "        - Summary statistics dictionary.\n",
    "    \"\"\"\n",
    "    original_count = len(df)\n",
    "    feature_columns = df.columns.tolist()\n",
    "    \n",
    "    # Group by all columns and compute frequency\n",
    "    df_aggregated = (\n",
    "        df.groupby(feature_columns, as_index=False)\n",
    "        .size()\n",
    "        .rename(columns={'size': 'Sample_Weight'})\n",
    "    )\n",
    "    \n",
    "    unique_count = len(df_aggregated)\n",
    "    reduction_pct = (1 - unique_count / original_count) * 100\n",
    "    \n",
    "    summary = {\n",
    "        'original_count': original_count,\n",
    "        'unique_profiles': unique_count,\n",
    "        'duplicates_removed': original_count - unique_count,\n",
    "        'reduction_percentage': reduction_pct,\n",
    "        'weight_sum': df_aggregated['Sample_Weight'].sum()\n",
    "    }\n",
    "    \n",
    "    return df_aggregated, summary\n",
    "\n",
    "\n",
    "def verify_data_integrity(\n",
    "    df: pd.DataFrame,\n",
    "    original_count: int,\n",
    "    expected_columns: int = 23\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Assert data integrity constraints after aggregation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Aggregated dataframe to verify.\n",
    "    original_count : int\n",
    "        Original row count before aggregation.\n",
    "    expected_columns : int, optional\n",
    "        Expected number of columns (22 features + 1 weight), by default 23.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    AssertionError\n",
    "        If any integrity constraint is violated.\n",
    "    \"\"\"\n",
    "    # Verify column count (22 original + Sample_Weight)\n",
    "    assert df.shape[1] == expected_columns, (\n",
    "        f\"Column count mismatch: expected {expected_columns}, got {df.shape[1]}\"\n",
    "    )\n",
    "    \n",
    "    # Verify weight summation equals original count\n",
    "    weight_sum = df['Sample_Weight'].sum()\n",
    "    assert weight_sum == original_count, (\n",
    "        f\"Weight sum mismatch: expected {original_count}, got {weight_sum}\"\n",
    "    )\n",
    "    \n",
    "    # Verify no null values in weight column\n",
    "    assert df['Sample_Weight'].notna().all(), (\n",
    "        \"Sample_Weight contains null values\"\n",
    "    )\n",
    "    \n",
    "    # Verify all weights are positive integers\n",
    "    assert (df['Sample_Weight'] > 0).all(), (\n",
    "        \"Sample_Weight contains non-positive values\"\n",
    "    )\n",
    "    \n",
    "    print(\"[PASS] All data integrity assertions passed.\")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "6937fdda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:36.169824400Z",
     "start_time": "2026-01-31T19:05:36.158559700Z"
    }
   },
   "source": [
    "def save_processed_data(df: pd.DataFrame, filepath: Path) -> None:\n",
    "    \"\"\"\n",
    "    Export processed dataframe to CSV with directory creation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe to export.\n",
    "    filepath : Path\n",
    "        Destination file path.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    df.to_csv(filepath, index=False)\n",
    "    \n",
    "    file_size_mb = filepath.stat().st_size / 1024**2\n",
    "    print(f\"[SUCCESS] Saved to: {filepath}\")\n",
    "    print(f\"[INFO] File size: {file_size_mb:.2f} MB\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "dfced143",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "id": "0f5f3f0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:36.572073500Z",
     "start_time": "2026-01-31T19:05:36.171823900Z"
    }
   },
   "source": [
    "# Load raw dataset\n",
    "df_raw = load_dataset(INPUT_FILE)\n",
    "\n",
    "# Display initial structure\n",
    "print(f\"\\n[INFO] Shape: {df_raw.shape}\")\n",
    "print(f\"[INFO] Columns: {df_raw.columns.tolist()}\")\n",
    "df_raw.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Loaded 253,680 records with 22 columns.\n",
      "\n",
      "[INFO] Shape: (253680, 22)\n",
      "[INFO] Columns: ['Diabetes_012', 'HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Diabetes_012  HighBP  HighChol  CholCheck     BMI  Smoker  Stroke  HeartDiseaseorAttack  PhysActivity  Fruits  Veggies  HvyAlcoholConsump  AnyHealthcare  NoDocbcCost  GenHlth  MentHlth  PhysHlth  \\\n",
       "0        0.0000  1.0000    1.0000     1.0000 40.0000  1.0000  0.0000                0.0000        0.0000  0.0000   1.0000             0.0000         1.0000       0.0000   5.0000   18.0000   15.0000   \n",
       "1        0.0000  0.0000    0.0000     0.0000 25.0000  1.0000  0.0000                0.0000        1.0000  0.0000   0.0000             0.0000         0.0000       1.0000   3.0000    0.0000    0.0000   \n",
       "2        0.0000  1.0000    1.0000     1.0000 28.0000  0.0000  0.0000                0.0000        0.0000  1.0000   0.0000             0.0000         1.0000       1.0000   5.0000   30.0000   30.0000   \n",
       "3        0.0000  1.0000    0.0000     1.0000 27.0000  0.0000  0.0000                0.0000        1.0000  1.0000   1.0000             0.0000         1.0000       0.0000   2.0000    0.0000    0.0000   \n",
       "4        0.0000  1.0000    1.0000     1.0000 24.0000  0.0000  0.0000                0.0000        1.0000  1.0000   1.0000             0.0000         1.0000       0.0000   2.0000    3.0000    0.0000   \n",
       "\n",
       "   DiffWalk    Sex     Age  Education  Income  \n",
       "0    1.0000 0.0000  9.0000     4.0000  3.0000  \n",
       "1    0.0000 0.0000  7.0000     6.0000  1.0000  \n",
       "2    1.0000 0.0000  9.0000     4.0000  8.0000  \n",
       "3    0.0000 0.0000 11.0000     3.0000  6.0000  \n",
       "4    0.0000 0.0000 11.0000     5.0000  4.0000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes_012</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>HvyAlcoholConsump</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>40.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>3.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>28.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>8.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>27.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>6.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>4.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "9dcbee16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:36.885368Z",
     "start_time": "2026-01-31T19:05:36.619887800Z"
    }
   },
   "source": [
    "# Verify expected column count\n",
    "EXPECTED_RAW_COLUMNS = 22\n",
    "assert df_raw.shape[1] == EXPECTED_RAW_COLUMNS, (\n",
    "    f\"Expected {EXPECTED_RAW_COLUMNS} columns, got {df_raw.shape[1]}\"\n",
    ")\n",
    "print(f\"[PASS] Column count verified: {df_raw.shape[1]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PASS] Column count verified: 22\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "91bdf33d",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "id": "613d082e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:37.373370400Z",
     "start_time": "2026-01-31T19:05:37.097296800Z"
    }
   },
   "source": [
    "# Downcast numeric types to reduce memory footprint\n",
    "df_optimized = optimize_memory(df_raw.copy())\n",
    "\n",
    "# Display optimized dtypes\n",
    "print(\"\\n[INFO] Optimized data types:\")\n",
    "print(df_optimized.dtypes.value_counts())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MEMORY] 42.58 MB -> 21.29 MB (50.0% reduction)\n",
      "\n",
      "[INFO] Optimized data types:\n",
      "float32    22\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "3dc64202",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Profile Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "id": "9f6cc5f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:37.859889500Z",
     "start_time": "2026-01-31T19:05:37.464638900Z"
    }
   },
   "source": [
    "# Execute profile convergence strategy\n",
    "df_aggregated, agg_summary = aggregate_profiles(df_optimized)\n",
    "\n",
    "# Display aggregation summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROFILE AGGREGATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original record count:    {agg_summary['original_count']:>12,}\")\n",
    "print(f\"Unique clinical profiles: {agg_summary['unique_profiles']:>12,}\")\n",
    "print(f\"Duplicate records merged: {agg_summary['duplicates_removed']:>12,}\")\n",
    "print(f\"Data reduction:           {agg_summary['reduction_percentage']:>11.2f}%\")\n",
    "print(f\"Weight sum verification:  {agg_summary['weight_sum']:>12,}\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROFILE AGGREGATION SUMMARY\n",
      "============================================================\n",
      "Original record count:         253,680\n",
      "Unique clinical profiles:      229,781\n",
      "Duplicate records merged:       23,899\n",
      "Data reduction:                  9.42%\n",
      "Weight sum verification:       253,680\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "c1406360",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:37.966082400Z",
     "start_time": "2026-01-31T19:05:37.928830Z"
    }
   },
   "source": [
    "# Examine weight distribution\n",
    "print(\"[INFO] Sample_Weight distribution:\")\n",
    "print(df_aggregated['Sample_Weight'].describe())\n",
    "\n",
    "print(f\"\\n[INFO] Profiles with weight > 1: {(df_aggregated['Sample_Weight'] > 1).sum():,}\")\n",
    "print(f\"[INFO] Maximum weight: {df_aggregated['Sample_Weight'].max():,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Sample_Weight distribution:\n",
      "count   229781.0000\n",
      "mean         1.1040\n",
      "std          0.8490\n",
      "min          1.0000\n",
      "25%          1.0000\n",
      "50%          1.0000\n",
      "75%          1.0000\n",
      "max         59.0000\n",
      "Name: Sample_Weight, dtype: float64\n",
      "\n",
      "[INFO] Profiles with weight > 1: 11,187\n",
      "[INFO] Maximum weight: 59\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "e99d9857",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Aggregation Verification"
   ]
  },
  {
   "cell_type": "code",
   "id": "a2ddd48e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:38.042223900Z",
     "start_time": "2026-01-31T19:05:38.009108900Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 7 VERIFICATION: Profile Aggregation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Execute integrity assertions\n",
    "verify_data_integrity(\n",
    "    df=df_aggregated,\n",
    "    original_count=agg_summary['original_count'],\n",
    "    expected_columns=23  # 22 features + Sample_Weight\n",
    ")\n",
    "\n",
    "# Verify weight statistics\n",
    "assert df_aggregated['Sample_Weight'].min() > 0, \"Invalid: negative or zero weights detected\"\n",
    "assert df_aggregated['Sample_Weight'].sum() == agg_summary['original_count'], \"Weight sum mismatch\"\n",
    "\n",
    "print(f\"\\n[VALIDATED] Shape: {df_aggregated.shape}\")\n",
    "print(f\"[VALIDATED] Weight range: [{df_aggregated['Sample_Weight'].min()}, {df_aggregated['Sample_Weight'].max()}]\")\n",
    "print(f\"[VALIDATED] Weight mean: {df_aggregated['Sample_Weight'].mean():.2f}\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 7 VERIFICATION: Profile Aggregation\n",
      "============================================================\n",
      "[PASS] All data integrity assertions passed.\n",
      "\n",
      "[VALIDATED] Shape: (229781, 23)\n",
      "[VALIDATED] Weight range: [1, 59]\n",
      "[VALIDATED] Weight mean: 1.10\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "a7e8e9f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:38.096175Z",
     "start_time": "2026-01-31T19:05:38.043223900Z"
    }
   },
   "source": [
    "# Display aggregated dataframe structure\n",
    "print(f\"\\n[INFO] Aggregated dataset preview:\")\n",
    "df_aggregated.head(10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Aggregated dataset preview:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Diabetes_012  HighBP  HighChol  CholCheck     BMI  Smoker  Stroke  HeartDiseaseorAttack  PhysActivity  Fruits  Veggies  HvyAlcoholConsump  AnyHealthcare  NoDocbcCost  GenHlth  MentHlth  PhysHlth  \\\n",
       "0        0.0000  0.0000    0.0000     0.0000 14.0000  1.0000  0.0000                0.0000        1.0000  1.0000   1.0000             0.0000         1.0000       0.0000   3.0000    4.0000    4.0000   \n",
       "1        0.0000  0.0000    0.0000     0.0000 15.0000  0.0000  0.0000                0.0000        0.0000  1.0000   0.0000             0.0000         0.0000       0.0000   1.0000    0.0000    0.0000   \n",
       "2        0.0000  0.0000    0.0000     0.0000 15.0000  1.0000  0.0000                0.0000        0.0000  0.0000   0.0000             0.0000         1.0000       0.0000   5.0000   20.0000   28.0000   \n",
       "3        0.0000  0.0000    0.0000     0.0000 15.0000  1.0000  0.0000                1.0000        0.0000  1.0000   1.0000             0.0000         0.0000       1.0000   3.0000    0.0000   29.0000   \n",
       "4        0.0000  0.0000    0.0000     0.0000 16.0000  0.0000  0.0000                0.0000        0.0000  0.0000   0.0000             0.0000         1.0000       0.0000   2.0000    0.0000    0.0000   \n",
       "5        0.0000  0.0000    0.0000     0.0000 16.0000  0.0000  0.0000                0.0000        0.0000  1.0000   1.0000             0.0000         1.0000       0.0000   2.0000    7.0000    5.0000   \n",
       "6        0.0000  0.0000    0.0000     0.0000 16.0000  0.0000  0.0000                0.0000        1.0000  0.0000   0.0000             0.0000         1.0000       1.0000   2.0000    0.0000    0.0000   \n",
       "7        0.0000  0.0000    0.0000     0.0000 16.0000  0.0000  0.0000                0.0000        1.0000  0.0000   1.0000             0.0000         1.0000       0.0000   2.0000    0.0000    4.0000   \n",
       "8        0.0000  0.0000    0.0000     0.0000 16.0000  0.0000  0.0000                0.0000        1.0000  1.0000   1.0000             0.0000         0.0000       0.0000   2.0000    0.0000    0.0000   \n",
       "9        0.0000  0.0000    0.0000     0.0000 16.0000  0.0000  0.0000                1.0000        1.0000  0.0000   1.0000             0.0000         1.0000       1.0000   4.0000    0.0000    3.0000   \n",
       "\n",
       "   DiffWalk    Sex     Age  Education  Income  Sample_Weight  \n",
       "0    0.0000 1.0000 11.0000     6.0000  8.0000              1  \n",
       "1    0.0000 0.0000  1.0000     5.0000  7.0000              1  \n",
       "2    1.0000 0.0000 10.0000     6.0000  4.0000              1  \n",
       "3    0.0000 0.0000  7.0000     5.0000  2.0000              1  \n",
       "4    0.0000 0.0000 11.0000     5.0000  5.0000              1  \n",
       "5    0.0000 0.0000  1.0000     5.0000  5.0000              1  \n",
       "6    0.0000 0.0000  6.0000     5.0000  6.0000              1  \n",
       "7    0.0000 0.0000  3.0000     6.0000  8.0000              1  \n",
       "8    0.0000 0.0000  9.0000     5.0000  2.0000              1  \n",
       "9    0.0000 0.0000  6.0000     6.0000  2.0000              1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes_012</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>HvyAlcoholConsump</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "      <th>Sample_Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>14.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>28.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "c355e85c",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Logical Consistency Cleaning\n",
    "\n",
    "Records exhibiting logical inconsistencies between self-reported health indicators are identified and removed. Specifically, profiles reporting 30 days of poor physical health (`PhysHlth=30`) while simultaneously claiming excellent general health (`GenHlth=1`) represent contradictory self-assessments that may compromise model reliability."
   ]
  },
  {
   "cell_type": "code",
   "id": "f0ddb500",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:38.260765300Z",
     "start_time": "2026-01-31T19:05:38.150163700Z"
    }
   },
   "source": [
    "def remove_logical_inconsistencies(df: pd.DataFrame) -> Tuple[pd.DataFrame, int]:\n",
    "    \"\"\"\n",
    "    Remove records with contradictory health indicator combinations.\n",
    "    \n",
    "    Logical inconsistency definition:\n",
    "    - PhysHlth = 30 (poor physical health for all 30 days)\n",
    "    - GenHlth = 1 (self-reported Excellent general health)\n",
    "    \n",
    "    These mutually exclusive conditions indicate response errors or\n",
    "    misinterpretation of survey questions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with PhysHlth and GenHlth columns.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[pd.DataFrame, int]\n",
    "        - Cleaned dataframe with inconsistent records removed.\n",
    "        - Count of removed records.\n",
    "    \"\"\"\n",
    "    initial_count = len(df)\n",
    "    \n",
    "    # Define inconsistency mask: maximum physical distress + excellent health\n",
    "    inconsistency_mask = (df['PhysHlth'] == 30) & (df['GenHlth'] == 1)\n",
    "    \n",
    "    # Filter out inconsistent records\n",
    "    df_cleaned = df[~inconsistency_mask].copy()\n",
    "    \n",
    "    removed_count = initial_count - len(df_cleaned)\n",
    "    \n",
    "    return df_cleaned, removed_count"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "de5d13a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:38.528603700Z",
     "start_time": "2026-01-31T19:05:38.392026500Z"
    }
   },
   "source": [
    "# Execute logical consistency cleaning\n",
    "df_consistent, removed_count = remove_logical_inconsistencies(df_aggregated)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 8 VERIFICATION: Logical Consistency Cleaning\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Records before cleaning:  {len(df_aggregated):>12,}\")\n",
    "print(f\"Inconsistent records:     {removed_count:>12,}\")\n",
    "print(f\"Records after cleaning:   {len(df_consistent):>12,}\")\n",
    "\n",
    "# Calculate weight statistics\n",
    "weight_sum_before = df_aggregated['Sample_Weight'].sum()\n",
    "weight_sum_after = df_consistent['Sample_Weight'].sum()\n",
    "weight_removed = weight_sum_before - weight_sum_after\n",
    "\n",
    "print(f\"\\nWeight statistics:\")\n",
    "print(f\"  Weight sum before:      {weight_sum_before:>12,.0f}\")\n",
    "print(f\"  Weight sum after:       {weight_sum_after:>12,.0f}\")\n",
    "print(f\"  Weight removed:         {weight_removed:>12,.0f}\")\n",
    "\n",
    "# Verify no contradictory records remain\n",
    "inconsistent_check = ((df_consistent['PhysHlth'] == 30) & (df_consistent['GenHlth'] == 1)).sum()\n",
    "assert inconsistent_check == 0, f\"Inconsistency removal failed: {inconsistent_check} contradictory records remain\"\n",
    "\n",
    "print(f\"\\n[VALIDATED] No contradictory records remaining: {inconsistent_check}\")\n",
    "print(f\"[VALIDATED] Data integrity maintained\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 8 VERIFICATION: Logical Consistency Cleaning\n",
      "============================================================\n",
      "Records before cleaning:       229,781\n",
      "Inconsistent records:              416\n",
      "Records after cleaning:        229,365\n",
      "\n",
      "Weight statistics:\n",
      "  Weight sum before:           253,680\n",
      "  Weight sum after:            253,264\n",
      "  Weight removed:                  416\n",
      "\n",
      "[VALIDATED] No contradictory records remaining: 0\n",
      "[VALIDATED] Data integrity maintained\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "a99d0d75",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Target Re-alignment (Binarization)\n",
    "\n",
    "The original trinary target variable (`Diabetes_012`) is transformed into a binary classification target. Pre-diabetic cases (value 1) are consolidated with diabetic cases (value 2) to create a unified positive class, enabling binary classification modeling."
   ]
  },
  {
   "cell_type": "code",
   "id": "3b3fc1fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:38.734478900Z",
     "start_time": "2026-01-31T19:05:38.722126200Z"
    }
   },
   "source": [
    "def binarize_target(df: pd.DataFrame, source_col: str = 'Diabetes_012') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert trinary diabetes target to binary classification target.\n",
    "    \n",
    "    Mapping logic:\n",
    "    - 0 (No diabetes) -> 0 (Negative class)\n",
    "    - 1 (Pre-diabetes) -> 1 (Positive class)\n",
    "    - 2 (Diabetes) -> 1 (Positive class)\n",
    "    \n",
    "    Note: This function does NOT drop the original column. The original\n",
    "    column is retained for validation purposes and will be removed in\n",
    "    a subsequent Feature Cleanup stage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with trinary target column.\n",
    "    source_col : str, optional\n",
    "        Name of the original target column, by default 'Diabetes_012'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe with new binary target column 'Diabetes_binary'.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the source column contains NaN values.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Check for NaN values\n",
    "    if df[source_col].isna().any():\n",
    "        nan_count = df[source_col].isna().sum()\n",
    "        raise ValueError(\n",
    "            f\"Found {nan_count} NaN values in {source_col}. \"\n",
    "            \"Please handle missing values before binarization.\"\n",
    "        )\n",
    "    \n",
    "    # Binary mapping: 0 stays 0, any positive value becomes 1\n",
    "    df['Diabetes_binary'] = df[source_col].apply(lambda x: 0 if x == 0 else 1)\n",
    "    \n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "ba68c22d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:38.984845500Z",
     "start_time": "2026-01-31T19:05:38.852697600Z"
    }
   },
   "source": [
    "# Execute target binarization on cleaned dataset\n",
    "df_binarized = binarize_target(df_consistent)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 9 VERIFICATION: Target Re-alignment (Binarization)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nOriginal target distribution (Diabetes_012):\")\n",
    "original_dist = df_binarized['Diabetes_012'].value_counts().sort_index()\n",
    "for val, count in original_dist.items():\n",
    "    label = {0: 'No diabetes', 1: 'Pre-diabetes', 2: 'Diabetes'}[int(val)]\n",
    "    print(f\"  {int(val)} ({label}): {count:,}\")\n",
    "\n",
    "print(\"\\nBinarized target distribution (Diabetes_binary):\")\n",
    "binary_dist = df_binarized['Diabetes_binary'].value_counts().sort_index()\n",
    "for val, count in binary_dist.items():\n",
    "    label = {0: 'Negative', 1: 'Positive'}[val]\n",
    "    print(f\"  {val} ({label}): {count:,}\")\n",
    "\n",
    "# Verify mapping correctness\n",
    "expected_negative = original_dist.get(0, 0)\n",
    "expected_positive = original_dist.get(1, 0) + original_dist.get(2, 0)\n",
    "actual_negative = binary_dist.get(0, 0)\n",
    "actual_positive = binary_dist.get(1, 0)\n",
    "\n",
    "assert expected_negative == actual_negative, \"Negative class count mismatch\"\n",
    "assert expected_positive == actual_positive, \"Positive class count mismatch\"\n",
    "assert set(df_binarized['Diabetes_binary'].unique()) == {0, 1}, \"Invalid values in binary target\"\n",
    "\n",
    "print(f\"\\n[VALIDATED] Mapping correctness: Negative={actual_negative:,}, Positive={actual_positive:,}\")\n",
    "print(f\"[VALIDATED] Binary target contains only values: {sorted(df_binarized['Diabetes_binary'].unique())}\")\n",
    "print(f\"[VALIDATED] Class balance ratio: {actual_positive/actual_negative:.4f}\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 9 VERIFICATION: Target Re-alignment (Binarization)\n",
      "============================================================\n",
      "\n",
      "Original target distribution (Diabetes_012):\n",
      "  0 (No diabetes): 189,697\n",
      "  1 (Pre-diabetes): 4,620\n",
      "  2 (Diabetes): 35,048\n",
      "\n",
      "Binarized target distribution (Diabetes_binary):\n",
      "  0 (Negative): 189,697\n",
      "  1 (Positive): 39,668\n",
      "\n",
      "[VALIDATED] Mapping correctness: Negative=189,697, Positive=39,668\n",
      "[VALIDATED] Binary target contains only values: [np.int64(0), np.int64(1)]\n",
      "[VALIDATED] Class balance ratio: 0.2091\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "d006a593",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Feature Cleanup\n",
    "\n",
    "The original trinary target column is removed to prevent target leakage and ensure the downstream modeling pipeline uses only the binary target variable."
   ]
  },
  {
   "cell_type": "code",
   "id": "1ecff04b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:39.059625100Z",
     "start_time": "2026-01-31T19:05:39.030849800Z"
    }
   },
   "source": [
    "# Drop original trinary target to prevent leakage\n",
    "df_cleaned = df_binarized.drop(columns=['Diabetes_012'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 10 VERIFICATION: Feature Cleanup\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "assert 'Diabetes_012' not in df_cleaned.columns, \"Target leakage: Diabetes_012 column still exists\"\n",
    "assert 'Diabetes_binary' in df_cleaned.columns, \"Binary target column missing\"\n",
    "\n",
    "print(f\"[VALIDATED] Diabetes_012 removed (prevent target leakage)\")\n",
    "print(f\"[VALIDATED] Diabetes_binary preserved (binary target)\")\n",
    "print(f\"[VALIDATED] Current shape: {df_cleaned.shape}\")\n",
    "print(f\"[VALIDATED] Current columns: {df_cleaned.shape[1]}\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 10 VERIFICATION: Feature Cleanup\n",
      "============================================================\n",
      "[VALIDATED] Diabetes_012 removed (prevent target leakage)\n",
      "[VALIDATED] Diabetes_binary preserved (binary target)\n",
      "[VALIDATED] Current shape: (229365, 23)\n",
      "[VALIDATED] Current columns: 23\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "19164132",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Ultimate Deduplication (Critical)\n",
    "\n",
    "After feature cleanup (removing `Diabetes_012`), rows that originally had different `Diabetes_012` values but the same features and `Diabetes_binary` value now become duplicates. This step performs the **ultimate aggregation** by grouping all identical feature profiles and summing their weights."
   ]
  },
  {
   "cell_type": "code",
   "id": "ddd62685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:39.492507900Z",
     "start_time": "2026-01-31T19:05:39.060626600Z"
    }
   },
   "source": [
    "# Perform ultimate deduplication: merge all identical profiles\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 11: ULTIMATE DEDUPLICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "records_before = len(df_cleaned)\n",
    "feature_cols = [col for col in df_cleaned.columns if col != 'Sample_Weight']\n",
    "\n",
    "# Check for potential duplicates\n",
    "dup_mask = df_cleaned[feature_cols].duplicated(keep=False)\n",
    "print(f\"Records before deduplication: {records_before:,}\")\n",
    "print(f\"Potentially duplicate profiles: {dup_mask.sum()}\")\n",
    "\n",
    "# Aggregate: group by all features, sum weights\n",
    "df_deduplicated = (\n",
    "    df_cleaned.groupby(feature_cols, as_index=False)\n",
    "    .agg({'Sample_Weight': 'sum'})\n",
    ")\n",
    "\n",
    "records_after = len(df_deduplicated)\n",
    "merged_count = records_before - records_after\n",
    "\n",
    "print(f\"Records after deduplication:  {records_after:,}\")\n",
    "print(f\"Duplicate records merged:     {merged_count}\")\n",
    "\n",
    "# Verify no duplicates remain\n",
    "final_dup_check = df_deduplicated[feature_cols].duplicated().sum()\n",
    "assert final_dup_check == 0, f\"Deduplication failed: {final_dup_check} duplicates remain\"\n",
    "\n",
    "# Verify weight integrity\n",
    "weight_sum_before = df_cleaned['Sample_Weight'].sum()\n",
    "weight_sum_after = df_deduplicated['Sample_Weight'].sum()\n",
    "assert np.isclose(weight_sum_before, weight_sum_after, rtol=1e-9), \"Weight sum changed during deduplication\"\n",
    "\n",
    "print(f\"\\n[VALIDATED] No duplicate profiles remain: {final_dup_check}\")\n",
    "print(f\"[VALIDATED] Weight sum preserved: {weight_sum_after:,.2f}\")\n",
    "print(f\"[VALIDATED] Dataset ready for normalization\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 11: ULTIMATE DEDUPLICATION\n",
      "============================================================\n",
      "Records before deduplication: 229,365\n",
      "Potentially duplicate profiles: 138\n",
      "Records after deduplication:  229,296\n",
      "Duplicate records merged:     69\n",
      "\n",
      "[VALIDATED] No duplicate profiles remain: 0\n",
      "[VALIDATED] Weight sum preserved: 253,264.00\n",
      "[VALIDATED] Dataset ready for normalization\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "459fac6d",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Weight Normalization\n",
    "\n",
    "After all structural changes are complete, sample weights are normalized by dividing each weight by the mean. This ensures the weight distribution has a mean of 1.0, facilitating stable numerical computations in downstream modeling while preserving relative prevalence ratios."
   ]
  },
  {
   "cell_type": "code",
   "id": "d63204c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:39.504447100Z",
     "start_time": "2026-01-31T19:05:39.495364100Z"
    }
   },
   "source": [
    "def normalize_weights(df: pd.DataFrame, weight_col: str = 'Sample_Weight') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize sample weights by dividing each weight by the mean.\n",
    "    \n",
    "    This ensures the weight distribution has a mean of 1.0, facilitating\n",
    "    stable numerical computations in downstream modeling while preserving\n",
    "    the relative prevalence ratios between clinical profiles.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with raw sample weights.\n",
    "    weight_col : str, optional\n",
    "        Name of the weight column, by default 'Sample_Weight'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe with normalized weights.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    mean_weight = df[weight_col].mean()\n",
    "    df[weight_col] = df[weight_col] / mean_weight\n",
    "    \n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "d5c83470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:39.572144300Z",
     "start_time": "2026-01-31T19:05:39.505447900Z"
    }
   },
   "source": [
    "# Execute weight normalization on deduplicated dataset\n",
    "pre_norm_mean = df_deduplicated['Sample_Weight'].mean()\n",
    "df_normalized = normalize_weights(df_deduplicated)\n",
    "post_norm_mean = df_normalized['Sample_Weight'].mean()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 12 VERIFICATION: Weight Normalization\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Pre-normalization mean:   {pre_norm_mean:.6f}\")\n",
    "print(f\"Post-normalization mean:  {post_norm_mean:.6f}\")\n",
    "\n",
    "# Validate mean equals 1.0 (within floating-point tolerance)\n",
    "assert np.isclose(post_norm_mean, 1.0, atol=1e-10), (\n",
    "    f\"Weight normalization failed: mean = {post_norm_mean}, expected 1.0\"\n",
    ")\n",
    "\n",
    "weight_std = df_normalized['Sample_Weight'].std()\n",
    "weight_min = df_normalized['Sample_Weight'].min()\n",
    "weight_max = df_normalized['Sample_Weight'].max()\n",
    "\n",
    "print(f\"\\n[VALIDATED] Weight mean: {post_norm_mean:.6f} (target: 1.0)\")\n",
    "print(f\"[VALIDATED] Weight std: {weight_std:.4f}\")\n",
    "print(f\"[VALIDATED] Weight range: [{weight_min:.4f}, {weight_max:.4f}]\")\n",
    "print(f\"\\nWeight distribution:\")\n",
    "print(df_normalized['Sample_Weight'].describe())\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 12 VERIFICATION: Weight Normalization\n",
      "============================================================\n",
      "Pre-normalization mean:   1.104529\n",
      "Post-normalization mean:  1.000000\n",
      "\n",
      "[VALIDATED] Weight mean: 1.000000 (target: 1.0)\n",
      "[VALIDATED] Weight std: 0.7697\n",
      "[VALIDATED] Weight range: [0.9054, 53.4165]\n",
      "\n",
      "Weight distribution:\n",
      "count   229296.0000\n",
      "mean         1.0000\n",
      "std          0.7697\n",
      "min          0.9054\n",
      "25%          0.9054\n",
      "50%          0.9054\n",
      "75%          0.9054\n",
      "max         53.4165\n",
      "Name: Sample_Weight, dtype: float64\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "7a60f7e8",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Post-Normalization Verification\n",
    "\n",
    "As a final safety check, verify that no duplicate rows exist after normalization. Theoretically, normalization should not create duplicates since it only scales values, but this verification ensures data integrity."
   ]
  },
  {
   "cell_type": "code",
   "id": "ed7ba97f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:39.764606400Z",
     "start_time": "2026-01-31T19:05:39.576145700Z"
    }
   },
   "source": [
    "# Final duplicate check after normalization\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 13 VERIFICATION: Post-Normalization Duplicate Check\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_final = df_normalized.copy()\n",
    "\n",
    "# Check for complete duplicates (all columns including Sample_Weight)\n",
    "full_dup = df_final.duplicated().sum()\n",
    "print(f\"Complete duplicate rows: {full_dup}\")\n",
    "\n",
    "# Check for feature duplicates (excluding Sample_Weight)\n",
    "feature_cols = [col for col in df_final.columns if col != 'Sample_Weight']\n",
    "feature_dup = df_final[feature_cols].duplicated().sum()\n",
    "print(f\"Feature duplicate rows (excluding Sample_Weight): {feature_dup}\")\n",
    "\n",
    "assert full_dup == 0, f\"ERROR: {full_dup} complete duplicates found after normalization!\"\n",
    "assert feature_dup == 0, f\"ERROR: {feature_dup} feature duplicates found after normalization!\"\n",
    "\n",
    "print(f\"\\n[VALIDATED] No duplicate rows detected\")\n",
    "print(f\"[VALIDATED] Final dataset shape: {df_final.shape}\")\n",
    "print(f\"[VALIDATED] Final record count: {len(df_final):,}\")\n",
    "print(f\"[VALIDATED] Data pipeline complete - ready for export\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 13 VERIFICATION: Post-Normalization Duplicate Check\n",
      "============================================================\n",
      "Complete duplicate rows: 0\n",
      "Feature duplicate rows (excluding Sample_Weight): 0\n",
      "\n",
      "[VALIDATED] No duplicate rows detected\n",
      "[VALIDATED] Final dataset shape: (229296, 23)\n",
      "[VALIDATED] Final record count: 229,296\n",
      "[VALIDATED] Data pipeline complete - ready for export\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "d65b4582",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Export Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "7ae4692d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:42.663376900Z",
     "start_time": "2026-01-31T19:05:39.766628200Z"
    }
   },
   "source": [
    "# Save final cleaned and processed dataset\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EXPORT\")\n",
    "print(\"=\"*60)\n",
    "save_processed_data(df_final, OUTPUT_FILE)\n",
    "print(f\"[SUCCESS] Dataset ready for feature engineering\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL EXPORT\n",
      "============================================================\n",
      "[SUCCESS] Saved to: D:\\ProgramSoftware\\PyCharm\\WorkPlace\\DiaMetric-CDC\\data\\processed\\CDC_Diabetes_Cleaned.csv\n",
      "[INFO] File size: 23.54 MB\n",
      "[SUCCESS] Dataset ready for feature engineering\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "be1e75ee",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. Preprocessing Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "id": "3d019936",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T19:05:42.814446400Z",
     "start_time": "2026-01-31T19:05:42.777694300Z"
    }
   },
   "source": [
    "# Comprehensive preprocessing summary\n",
    "print(\"\\n\" + \"#\"*60)\n",
    "print(\"DATA PREPROCESSING PIPELINE SUMMARY\")\n",
    "print(\"#\"*60)\n",
    "\n",
    "# Verify all required variables exist (use globals() for notebook scope)\n",
    "required_vars = {\n",
    "    'agg_summary': 'Section 6-7 (Profile Aggregation)',\n",
    "    'removed_count': 'Section 8 (Logical Consistency)',\n",
    "    'records_before': 'Section 11 (Ultimate Deduplication)',\n",
    "    'merged_count': 'Section 11 (Ultimate Deduplication)',\n",
    "    'records_after': 'Section 11 (Ultimate Deduplication)',\n",
    "    'pre_norm_mean': 'Section 12 (Weight Normalization)',\n",
    "    'post_norm_mean': 'Section 12 (Weight Normalization)',\n",
    "    'full_dup': 'Section 13 (Post-Norm Verification)',\n",
    "    'feature_dup': 'Section 13 (Post-Norm Verification)',\n",
    "    'df_final': 'Section 13 (Post-Norm Verification)'\n",
    "}\n",
    "\n",
    "missing_vars = {var: stage for var, stage in required_vars.items() if var not in globals()}\n",
    "if missing_vars:\n",
    "    print(\"\\n WARNING: Cannot generate complete summary\")\n",
    "    print(\"The following variables are missing:\\n\")\n",
    "    for var, stage in missing_vars.items():\n",
    "        print(f\"  - {var} (from {stage})\")\n",
    "    print(\"\\n Please run all cells in order from the beginning.\")\n",
    "    print(\"#\"*60)\n",
    "else:\n",
    "    # Recalculate class distribution from final dataset for accuracy\n",
    "    actual_negative = (df_final['Diabetes_binary'] == 0).sum()\n",
    "    actual_positive = (df_final['Diabetes_binary'] == 1).sum()\n",
    "    \n",
    "    print(\"\\n[INPUT]\")\n",
    "    print(f\"  Source file: {INPUT_FILE.name}\")\n",
    "    print(f\"  Raw records: {agg_summary['original_count']:,}\")\n",
    "\n",
    "    print(\"\\n[PROCESSING STAGES]\")\n",
    "    print(f\"  Section 6-7: Profile Aggregation & Weight Calculation\")\n",
    "    print(f\"    - Unique profiles: {agg_summary['unique_profiles']:,}\")\n",
    "    print(f\"    - Duplicates merged: {agg_summary['duplicates_removed']:,}\")\n",
    "    print(f\"    - Data reduction: {agg_summary['reduction_percentage']:.2f}%\")\n",
    "    print(f\"    - Added: Sample_Weight column\")\n",
    "    print(f\"     Verified: Weight sum = {agg_summary['weight_sum']:,}\")\n",
    "\n",
    "    print(f\"\\n  Section 8: Logical Consistency Cleaning\")\n",
    "    print(f\"    - Inconsistent records removed: {removed_count:,}\")\n",
    "    print(f\"    - Criteria: PhysHlth=30 & GenHlth=1 (contradictory)\")\n",
    "    # Display weight impact if variables exist\n",
    "    if 'weight_removed' in globals():\n",
    "        print(f\"    - Weight removed: {weight_removed:,.0f}\")\n",
    "    print(f\"     Verified: No contradictory records remain\")\n",
    "\n",
    "    print(f\"\\n  Section 9: Target Re-alignment (Binarization)\")\n",
    "    print(f\"    - Original: Diabetes_012 (0=No, 1=Pre, 2=Yes)\")\n",
    "    print(f\"    - Binarized: Diabetes_binary (0=Negative, 1=Positive)\")\n",
    "    print(f\"     Verified: Mapping correctness validated\")\n",
    "\n",
    "    print(f\"\\n  Section 10: Feature Cleanup\")\n",
    "    print(f\"    - Dropped: Diabetes_012 (prevent target leakage)\")\n",
    "    print(f\"    - Retained: Diabetes_binary (binary target)\")\n",
    "    print(f\"     Verified: No data leakage\")\n",
    "\n",
    "    print(f\"\\n  Section 11: Ultimate Deduplication (Critical)\")\n",
    "    print(f\"    - Records before: {records_before:,}\")\n",
    "    print(f\"    - Duplicates merged: {merged_count}\")\n",
    "    print(f\"    - Records after: {records_after:,}\")\n",
    "    print(f\"     Verified: No duplicate profiles remain\")\n",
    "\n",
    "    print(f\"\\n  Section 12: Weight Normalization (Final Step)\")\n",
    "    print(f\"    - Pre-normalization mean: {pre_norm_mean:.4f}\")\n",
    "    print(f\"    - Post-normalization mean: {post_norm_mean:.6f}\")\n",
    "    print(f\"     Verified: Mean = 1.0 (numerical stability ensured)\")\n",
    "\n",
    "    print(f\"\\n  Section 13: Post-Normalization Verification\")\n",
    "    print(f\"    - Complete duplicates: {full_dup}\")\n",
    "    print(f\"    - Feature duplicates: {feature_dup}\")\n",
    "    print(f\"     Verified: Zero duplicates after normalization\")\n",
    "\n",
    "    print(\"\\n[OUTPUT]\")\n",
    "    print(f\"  File: {OUTPUT_FILE.name}\")\n",
    "    print(f\"  Final shape: {df_final.shape}\")\n",
    "    print(f\"  Composition: 21 features + Diabetes_binary (target) + Sample_Weight\")\n",
    "    print(f\"  Records: {len(df_final):,}\")\n",
    "    print(f\"  Class distribution:\")\n",
    "    print(f\"    - Negative class: {actual_negative:,}\")\n",
    "    print(f\"    - Positive class: {actual_positive:,}\")\n",
    "    print(f\"    - Balance ratio: {actual_positive/actual_negative:.4f}\")\n",
    "\n",
    "    print(\"\\n[STATUS]\")\n",
    "    print(f\"   All validation checks passed\")\n",
    "    print(f\"   Dataset ready for feature engineering (Phase 3)\")\n",
    "    print(\"\\n\" + \"#\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "DATA PREPROCESSING PIPELINE SUMMARY\n",
      "############################################################\n",
      "\n",
      "[INPUT]\n",
      "  Source file: CDC Diabetes Dataset.csv\n",
      "  Raw records: 253,680\n",
      "\n",
      "[PROCESSING STAGES]\n",
      "  Section 6-7: Profile Aggregation & Weight Calculation\n",
      "    - Unique profiles: 229,781\n",
      "    - Duplicates merged: 23,899\n",
      "    - Data reduction: 9.42%\n",
      "    - Added: Sample_Weight column\n",
      "     Verified: Weight sum = 253,680\n",
      "\n",
      "  Section 8: Logical Consistency Cleaning\n",
      "    - Inconsistent records removed: 416\n",
      "    - Criteria: PhysHlth=30 & GenHlth=1 (contradictory)\n",
      "    - Weight removed: 416\n",
      "     Verified: No contradictory records remain\n",
      "\n",
      "  Section 9: Target Re-alignment (Binarization)\n",
      "    - Original: Diabetes_012 (0=No, 1=Pre, 2=Yes)\n",
      "    - Binarized: Diabetes_binary (0=Negative, 1=Positive)\n",
      "     Verified: Mapping correctness validated\n",
      "\n",
      "  Section 10: Feature Cleanup\n",
      "    - Dropped: Diabetes_012 (prevent target leakage)\n",
      "    - Retained: Diabetes_binary (binary target)\n",
      "     Verified: No data leakage\n",
      "\n",
      "  Section 11: Ultimate Deduplication (Critical)\n",
      "    - Records before: 229,365\n",
      "    - Duplicates merged: 69\n",
      "    - Records after: 229,296\n",
      "     Verified: No duplicate profiles remain\n",
      "\n",
      "  Section 12: Weight Normalization (Final Step)\n",
      "    - Pre-normalization mean: 1.1045\n",
      "    - Post-normalization mean: 1.000000\n",
      "     Verified: Mean = 1.0 (numerical stability ensured)\n",
      "\n",
      "  Section 13: Post-Normalization Verification\n",
      "    - Complete duplicates: 0\n",
      "    - Feature duplicates: 0\n",
      "     Verified: Zero duplicates after normalization\n",
      "\n",
      "[OUTPUT]\n",
      "  File: CDC_Diabetes_Cleaned.csv\n",
      "  Final shape: (229296, 23)\n",
      "  Composition: 21 features + Diabetes_binary (target) + Sample_Weight\n",
      "  Records: 229,296\n",
      "  Class distribution:\n",
      "    - Negative class: 189,697\n",
      "    - Positive class: 39,599\n",
      "    - Balance ratio: 0.2087\n",
      "\n",
      "[STATUS]\n",
      "   All validation checks passed\n",
      "   Dataset ready for feature engineering (Phase 3)\n",
      "\n",
      "############################################################\n"
     ]
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiaMetric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
