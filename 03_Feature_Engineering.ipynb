{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ebe7c28",
   "metadata": {},
   "source": [
    "# Phase 3: Feature Engineering\n",
    "\n",
    "## Systematic Feature Construction for Diabetes Risk Modeling\n",
    "\n",
    "**Pipeline Stages:**\n",
    "1. Environment Configuration\n",
    "2. Path Configuration and Constants\n",
    "3. Data Loading and Validation\n",
    "4. Outlier Refinement\n",
    "5. Clinical Discretization\n",
    "6. Interaction Feature Synthesis\n",
    "7. Feature Aggregation\n",
    "8. Advanced Features\n",
    "9. Feature Quality Control and Selection\n",
    "10. Stratified Data Partitioning\n",
    "11. Robust Numerical Scaling\n",
    "12. Multi-Task Data Export\n",
    "13. Pipeline Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb8564",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "39abf51d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:31:23.874700800Z",
     "start_time": "2026-02-02T11:31:23.689816Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 35)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Pandas: {pd.__version__}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.11 | packaged by Anaconda, Inc. | (main, Dec 10 2025, 21:21:58) [MSC v.1929 64 bit (AMD64)]\n",
      "Pandas: 3.0.0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "eb06dff4",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Path Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "id": "ed3b30da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:31:23.905980800Z",
     "start_time": "2026-02-02T11:31:23.878706400Z"
    }
   },
   "source": [
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\" / \"feature_engineering\"\n",
    "\n",
    "INPUT_FILE = DATA_PROCESSED_DIR / \"CDC_Diabetes_Cleaned.csv\"\n",
    "\n",
    "# === BASELINE OUTPUT FILES ===\n",
    "# These files contain engineered features WITHOUT cluster-derived features.\n",
    "# Purpose: \n",
    "#   1. Baseline for ablation studies (measuring cluster feature contribution)\n",
    "#   2. Input for clustering pipeline (Phase 4) to load labels and weights\n",
    "TRAIN_OUTPUT = DATA_PROCESSED_DIR / \"CDC_Train_Classification_BASELINE.csv\"\n",
    "TEST_OUTPUT = DATA_PROCESSED_DIR / \"CDC_Test_Classification_BASELINE.csv\"\n",
    "\n",
    "# Raw data prepared for K-Prototypes\n",
    "CLUSTERING_RAW_OUTPUT = DATA_PROCESSED_DIR / \"CDC_Clustering_RAW.csv\"\n",
    "# Fully scaled data prepared for PCA/GMM\n",
    "CLUSTERING_SCALED_OUTPUT = DATA_PROCESSED_DIR / \"CDC_Clustering_SCALED.csv\"\n",
    "METADATA_OUTPUT = OUTPUT_DIR / \"feature_metadata.json\"\n",
    "SCALER_OUTPUT = OUTPUT_DIR / \"robust_scaler.pkl\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Thresholds\n",
    "BMI_EXTREME_THRESHOLD = 50.0\n",
    "BMI_WINSORIZE_PERCENTILE = 99\n",
    "VARIANCE_THRESHOLD = 0.01\n",
    "VIF_THRESHOLD = 10.0\n",
    "CORRELATION_THRESHOLD = 0.85\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"[CONFIG] Input: {INPUT_FILE}\")\n",
    "print(f\"[CONFIG] Output: {OUTPUT_DIR}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIG] Input: D:\\ProgramSoftware\\PyCharm\\WorkPlace\\DiaMetric-CDC\\data\\processed\\CDC_Diabetes_Cleaned.csv\n",
      "[CONFIG] Output: D:\\ProgramSoftware\\PyCharm\\WorkPlace\\DiaMetric-CDC\\outputs\\feature_engineering\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "41589fa9",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Loading and Validation"
   ]
  },
  {
   "cell_type": "code",
   "id": "b123e849",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:31:24.302795500Z",
     "start_time": "2026-02-02T11:31:23.910981700Z"
    }
   },
   "source": [
    "def load_and_validate_data(filepath: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load dataset and validate Sample_Weight normalization (mean ≈ 1.0).\"\"\"\n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(f\"Input file not found: {filepath}\")\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    required_cols = ['Diabetes_binary', 'Sample_Weight', 'BMI', 'Age', 'GenHlth']\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    assert len(missing) == 0, f\"Missing columns: {missing}\"\n",
    "    \n",
    "    weight_mean = df['Sample_Weight'].mean()\n",
    "    assert np.isclose(weight_mean, 1.0, atol=0.01), f\"Weight mean: {weight_mean:.4f}\"\n",
    "    \n",
    "    print(f\"[LOADED] {len(df):,} records, {df.shape[1]} columns\")\n",
    "    print(f\"[VALIDATED] Sample_Weight mean: {weight_mean:.6f}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_and_validate_data(INPUT_FILE)\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOADED] 229,296 records, 23 columns\n",
      "[VALIDATED] Sample_Weight mean: 1.000000\n",
      "\n",
      "Columns: ['HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income', 'Diabetes_binary', 'Sample_Weight']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   HighBP  HighChol  CholCheck     BMI  Smoker  Stroke  HeartDiseaseorAttack  \\\n",
       "0  0.0000    0.0000     0.0000 14.0000  1.0000  0.0000                0.0000   \n",
       "1  0.0000    0.0000     0.0000 15.0000  0.0000  0.0000                0.0000   \n",
       "2  0.0000    0.0000     0.0000 15.0000  1.0000  0.0000                0.0000   \n",
       "3  0.0000    0.0000     0.0000 15.0000  1.0000  0.0000                1.0000   \n",
       "4  0.0000    0.0000     0.0000 16.0000  0.0000  0.0000                0.0000   \n",
       "\n",
       "   PhysActivity  Fruits  Veggies  HvyAlcoholConsump  AnyHealthcare  \\\n",
       "0        1.0000  1.0000   1.0000             0.0000         1.0000   \n",
       "1        0.0000  1.0000   0.0000             0.0000         0.0000   \n",
       "2        0.0000  0.0000   0.0000             0.0000         1.0000   \n",
       "3        0.0000  1.0000   1.0000             0.0000         0.0000   \n",
       "4        0.0000  0.0000   0.0000             0.0000         1.0000   \n",
       "\n",
       "   NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk    Sex     Age  \\\n",
       "0       0.0000   3.0000    4.0000    4.0000    0.0000 1.0000 11.0000   \n",
       "1       0.0000   1.0000    0.0000    0.0000    0.0000 0.0000  1.0000   \n",
       "2       0.0000   5.0000   20.0000   28.0000    1.0000 0.0000 10.0000   \n",
       "3       1.0000   3.0000    0.0000   29.0000    0.0000 0.0000  7.0000   \n",
       "4       0.0000   2.0000    0.0000    0.0000    0.0000 0.0000 11.0000   \n",
       "\n",
       "   Education  Income  Diabetes_binary  Sample_Weight  \n",
       "0     6.0000  8.0000                0         0.9054  \n",
       "1     5.0000  7.0000                0         0.9054  \n",
       "2     6.0000  4.0000                0         0.9054  \n",
       "3     5.0000  2.0000                0         0.9054  \n",
       "4     5.0000  5.0000                0         0.9054  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>HvyAlcoholConsump</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "      <th>Diabetes_binary</th>\n",
       "      <th>Sample_Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>14.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>28.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "18999906",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Outlier Refinement"
   ]
  },
  {
   "cell_type": "code",
   "id": "0de13a92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:31:24.525791900Z",
     "start_time": "2026-02-02T11:31:24.351402200Z"
    }
   },
   "source": [
    "def mark_extreme_values(df: pd.DataFrame, col: str, threshold: float, marker: str) -> pd.DataFrame:\n",
    "    \"\"\"Create binary flag for values > threshold.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[marker] = (df[col] > threshold).astype(int)\n",
    "    print(f\"[MARKER] {marker}: {df[marker].sum():,} flagged ({df[marker].mean()*100:.2f}%)\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def winsorize_column(df: pd.DataFrame, col: str, pct: float = 99) -> pd.DataFrame:\n",
    "    \"\"\"Clip upper tail at specified percentile.\"\"\"\n",
    "    df = df.copy()\n",
    "    upper = np.percentile(df[col], pct)\n",
    "    orig_max = df[col].max()\n",
    "    df[col] = df[col].clip(upper=upper)\n",
    "    print(f\"[WINSORIZE] {col}: max {orig_max:.2f} -> {upper:.2f} (P{pct})\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df = mark_extreme_values(df, 'BMI', BMI_EXTREME_THRESHOLD, 'Is_Extreme_BMI')\n",
    "df = winsorize_column(df, 'BMI', BMI_WINSORIZE_PERCENTILE)\n",
    "print(f\"\\nBMI after refinement:\")\n",
    "print(df['BMI'].describe())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MARKER] Is_Extreme_BMI: 2,172 flagged (0.95%)\n",
      "[WINSORIZE] BMI: max 98.00 -> 50.00 (P99)\n",
      "\n",
      "BMI after refinement:\n",
      "count   229296.0000\n",
      "mean        28.5725\n",
      "std          6.1899\n",
      "min         12.0000\n",
      "25%         24.0000\n",
      "50%         27.0000\n",
      "75%         32.0000\n",
      "max         50.0000\n",
      "Name: BMI, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "b5a3decc",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Clinical Discretization"
   ]
  },
  {
   "cell_type": "code",
   "id": "25a6bfee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:31:25.121655600Z",
     "start_time": "2026-02-02T11:31:24.790824400Z"
    }
   },
   "source": [
    "def discretize_bmi_who(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Discretize BMI into WHO categories: 1=Underweight, 2=Normal, 3=Overweight, 4=Obese.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['BMI_WHO'] = pd.cut(df['BMI'], bins=[0, 18.5, 25, 30, np.inf], \n",
    "                          labels=[1, 2, 3, 4], include_lowest=True).astype(int)\n",
    "    print(f\"[BMI_WHO]\\n{df['BMI_WHO'].value_counts().sort_index()}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def discretize_age_lifecycle(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Discretize age into lifecycle stages: 1=18-29, 2=30-44, 3=45-59, 4=60-74, 5=75+.\"\"\"\n",
    "    df = df.copy()\n",
    "    mapping = {1:1, 2:1, 3:2, 4:2, 5:3, 6:3, 7:3, 8:4, 9:4, 10:4, 11:5, 12:5, 13:5}\n",
    "    df['Age_Group'] = df['Age'].map(mapping)\n",
    "    print(f\"[Age_Group]\\n{df['Age_Group'].value_counts().sort_index()}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def discretize_health_days(df: pd.DataFrame, src: str, tgt: str) -> pd.DataFrame:\n",
    "    \"\"\"Discretize health burden days: 0=None, 1=Moderate(1-13d), 2=Severe(14-30d).\"\"\"\n",
    "    df = df.copy()\n",
    "    df[tgt] = pd.cut(df[src], bins=[-1, 0, 13, 30], \n",
    "                     labels=[0, 1, 2], include_lowest=True).astype(int)\n",
    "    print(f\"[{tgt}]\\n{df[tgt].value_counts().sort_index()}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"=\"*50)\n",
    "df = discretize_bmi_who(df)\n",
    "print()\n",
    "df = discretize_age_lifecycle(df)\n",
    "print()\n",
    "df = discretize_health_days(df, 'MentHlth', 'MentHlth_Cat')\n",
    "print()\n",
    "df = discretize_health_days(df, 'PhysHlth', 'PhysHlth_Cat')\n",
    "print(\"=\"*50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "[BMI_WHO]\n",
      "BMI_WHO\n",
      "1     3043\n",
      "2    73563\n",
      "3    81366\n",
      "4    71324\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[Age_Group]\n",
      "Age_Group\n",
      "1    12562\n",
      "2    22218\n",
      "3    54377\n",
      "4    86004\n",
      "5    54135\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[MentHlth_Cat]\n",
      "MentHlth_Cat\n",
      "0    152263\n",
      "1     52586\n",
      "2     24447\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[PhysHlth_Cat]\n",
      "PhysHlth_Cat\n",
      "0    136811\n",
      "1     59261\n",
      "2     33224\n",
      "Name: count, dtype: int64\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "1f3cd6ba",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Interaction Feature Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "id": "747f3c3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:31:25.381058Z",
     "start_time": "2026-02-02T11:31:25.326487800Z"
    }
   },
   "source": [
    "def create_interaction_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate Age×BMI interaction, CVD co-morbidity, and metabolic syndrome risk features.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['Age_BMI_Interaction'] = df['Age'] * df['BMI']\n",
    "    df['CVD_Risk'] = (df['HighBP'] * df['HighChol']).astype(int)\n",
    "    df['MetSyn_Risk'] = df['HighBP'] + df['HighChol'] + (df['BMI_WHO'] >= 4).astype(int)\n",
    "    \n",
    "    print(f\"[Age_BMI_Interaction] range: [{df['Age_BMI_Interaction'].min():.1f}, {df['Age_BMI_Interaction'].max():.1f}]\")\n",
    "    print(f\"[CVD_Risk] positive: {df['CVD_Risk'].sum():,} ({df['CVD_Risk'].mean()*100:.1f}%)\")\n",
    "    print(f\"[MetSyn_Risk] mean: {df['MetSyn_Risk'].mean():.2f}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"=\"*50)\n",
    "df = create_interaction_features(df)\n",
    "print(\"=\"*50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "[Age_BMI_Interaction] range: [14.0, 650.0]\n",
      "[CVD_Risk] positive: 62,174 (27.1%)\n",
      "[MetSyn_Risk] mean: 1.21\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "29501d7d",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Feature Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "id": "28e1661a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:31:25.450520700Z",
     "start_time": "2026-02-02T11:31:25.383060200Z"
    }
   },
   "source": [
    "def create_aggregate_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate composite indices: chronic conditions, SDOH, lifestyle, risk behavior.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['Chronic_Count'] = df['HighBP'] + df['HighChol'] + df['HeartDiseaseorAttack'] + df['Stroke']\n",
    "    df['SDOH_Index'] = (df['Education'] + df['Income']) / 2\n",
    "    df['Lifestyle_Score'] = df['PhysActivity'] + df['Fruits'] + df['Veggies']\n",
    "    df['Risk_Behavior'] = df['Smoker'] + df['HvyAlcoholConsump']\n",
    "    \n",
    "    print(f\"[Chronic_Count] range: [0, {df['Chronic_Count'].max()}], mean: {df['Chronic_Count'].mean():.2f}\")\n",
    "    print(f\"[SDOH_Index] range: [{df['SDOH_Index'].min():.1f}, {df['SDOH_Index'].max():.1f}]\")\n",
    "    print(f\"[Lifestyle_Score] range: [0, {df['Lifestyle_Score'].max()}]\")\n",
    "    print(f\"[Risk_Behavior] range: [0, {df['Risk_Behavior'].max()}]\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"=\"*50)\n",
    "df = create_aggregate_features(df)\n",
    "print(\"=\"*50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "[Chronic_Count] range: [0, 4.0], mean: 1.04\n",
      "[SDOH_Index] range: [1.0, 7.0]\n",
      "[Lifestyle_Score] range: [0, 3.0]\n",
      "[Risk_Behavior] range: [0, 2.0]\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "fd0152e1",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "id": "70d12b84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:31:25.521536800Z",
     "start_time": "2026-02-02T11:31:25.452520100Z"
    }
   },
   "source": [
    "def create_advanced_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate BMI² for non-linear effects and mental-physical health imbalance.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['BMI_Squared'] = df['BMI'] ** 2\n",
    "    df['Health_Imbalance'] = df['MentHlth_Cat'] - df['PhysHlth_Cat']\n",
    "    \n",
    "    print(f\"[BMI_Squared] range: [{df['BMI_Squared'].min():.1f}, {df['BMI_Squared'].max():.1f}]\")\n",
    "    print(f\"[Health_Imbalance] range: [{df['Health_Imbalance'].min()}, {df['Health_Imbalance'].max()}]\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"=\"*50)\n",
    "df = create_advanced_features(df)\n",
    "print(f\"\\nCurrent feature count: {df.shape[1]}\")\n",
    "print(\"=\"*50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "[BMI_Squared] range: [144.0, 2500.0]\n",
      "[Health_Imbalance] range: [-2, 2]\n",
      "\n",
      "Current feature count: 37\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "5d0c7fa1",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Feature Quality Control and Selection"
   ]
  },
  {
   "cell_type": "code",
   "id": "dcc623e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:31:25.541607900Z",
     "start_time": "2026-02-02T11:31:25.523545300Z"
    }
   },
   "source": [
    "def calculate_vif(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Calculate VIF with intercept for multicollinearity detection.\"\"\"\n",
    "    X = df[cols].values\n",
    "    X_with_const = np.column_stack([np.ones(X.shape[0]), X])  # Add intercept\n",
    "    vif_data = []\n",
    "    for i, col in enumerate(cols):\n",
    "        try:\n",
    "            vif = variance_inflation_factor(X_with_const, i + 1)  # Skip intercept column\n",
    "        except:\n",
    "            vif = np.inf\n",
    "        vif_data.append({'Feature': col, 'VIF': vif})\n",
    "    return pd.DataFrame(vif_data).sort_values('VIF', ascending=False)\n",
    "\n",
    "\n",
    "def filter_high_vif(df: pd.DataFrame, cols: List[str], threshold: float = 10.0) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Iteratively remove features with VIF > threshold.\"\"\"\n",
    "    remaining, removed = cols.copy(), []\n",
    "    while True:\n",
    "        if len(remaining) < 2:\n",
    "            break\n",
    "        vif_df = calculate_vif(df, remaining)\n",
    "        max_vif = vif_df['VIF'].max()\n",
    "        if max_vif <= threshold or np.isinf(max_vif):\n",
    "            break\n",
    "        worst = vif_df.iloc[0]['Feature']\n",
    "        remaining.remove(worst)\n",
    "        removed.append(worst)\n",
    "        print(f\"  [VIF] Removed {worst} (VIF={max_vif:.2f})\")\n",
    "    return remaining, removed\n",
    "\n",
    "\n",
    "def filter_near_zero_var(df: pd.DataFrame, cols: List[str], threshold: float = 0.01) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Remove features with variance < threshold.\"\"\"\n",
    "    retained, removed = [], []\n",
    "    for col in cols:\n",
    "        v = df[col].var()\n",
    "        if v < threshold:\n",
    "            removed.append(col)\n",
    "            print(f\"  [VAR] Removed {col} (var={v:.6f})\")\n",
    "        else:\n",
    "            retained.append(col)\n",
    "    return retained, removed\n",
    "\n",
    "\n",
    "def filter_high_corr(df: pd.DataFrame, cols: List[str], threshold: float = 0.85) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Remove one feature from highly correlated pairs (Spearman).\"\"\"\n",
    "    corr = df[cols].corr(method='spearman').abs()\n",
    "    to_remove = set()\n",
    "    for i in range(len(cols)):\n",
    "        if cols[i] in to_remove:\n",
    "            continue  # Skip already marked features\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            if cols[j] in to_remove:\n",
    "                continue\n",
    "            if corr.iloc[i, j] > threshold:\n",
    "                ci, cj = cols[i], cols[j]\n",
    "                drop = ci if df[ci].var() < df[cj].var() else cj\n",
    "                to_remove.add(drop)\n",
    "                print(f\"  [CORR] {ci} vs {cj}: r={corr.iloc[i,j]:.3f}, removed {drop}\")\n",
    "    return [c for c in cols if c not in to_remove], list(to_remove)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "3f601e0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:31:25.566201900Z",
     "start_time": "2026-02-02T11:31:25.543607800Z"
    }
   },
   "source": [
    "TARGET_COL = 'Diabetes_binary'\n",
    "WEIGHT_COL = 'Sample_Weight'\n",
    "LOW_UTILITY = ['AnyHealthcare', 'CholCheck']\n",
    "\n",
    "all_features = [c for c in df.columns if c not in [TARGET_COL, WEIGHT_COL]]\n",
    "print(f\"[INFO] Features before selection: {len(all_features)}\")\n",
    "\n",
    "# Tracking\n",
    "removed_log = {'low_utility': [], 'near_zero_var': [], 'high_vif': [], 'high_corr': []}"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Features before selection: 35\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "ec2f4364",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:31:25.592415600Z",
     "start_time": "2026-02-02T11:31:25.570201700Z"
    }
   },
   "source": [
    "print(\"=\"*50)\n",
    "print(\"[STEP 1] Low Utility Removal\")\n",
    "for f in LOW_UTILITY:\n",
    "    if f in all_features:\n",
    "        all_features.remove(f)\n",
    "        removed_log['low_utility'].append(f)\n",
    "        print(f\"  Removed {f}\")\n",
    "print(f\"  Remaining: {len(all_features)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "[STEP 1] Low Utility Removal\n",
      "  Removed AnyHealthcare\n",
      "  Removed CholCheck\n",
      "  Remaining: 33\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "50d9d46b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:31:25.674757700Z",
     "start_time": "2026-02-02T11:31:25.593415700Z"
    }
   },
   "source": [
    "print(\"\\n[STEP 2] Near-Zero Variance Filtering\")\n",
    "all_features, removed_var = filter_near_zero_var(df, all_features, VARIANCE_THRESHOLD)\n",
    "removed_log['near_zero_var'] = removed_var\n",
    "if not removed_var:\n",
    "    print(\"  No features removed\")\n",
    "print(f\"  Remaining: {len(all_features)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2] Near-Zero Variance Filtering\n",
      "  [VAR] Removed Is_Extreme_BMI (var=0.009383)\n",
      "  Remaining: 32\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "d27e64a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:31:41.120978500Z",
     "start_time": "2026-02-02T11:31:25.676758200Z"
    }
   },
   "source": [
    "print(\"\\n[STEP 3] VIF Multicollinearity Filtering\")\n",
    "numeric_cols = df[all_features].select_dtypes(include=[np.number]).columns.tolist()\n",
    "all_features, removed_vif = filter_high_vif(df, numeric_cols, VIF_THRESHOLD)\n",
    "removed_log['high_vif'] = removed_vif\n",
    "if not removed_vif:\n",
    "    print(\"  No features removed\")\n",
    "print(f\"  Remaining: {len(all_features)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 3] VIF Multicollinearity Filtering\n",
      "  No features removed\n",
      "  Remaining: 32\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "b341576a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:31:42.739611600Z",
     "start_time": "2026-02-02T11:31:41.141199Z"
    }
   },
   "source": [
    "print(\"\\n[STEP 4] High Correlation Filtering\")\n",
    "FINAL_FEATURES, removed_corr = filter_high_corr(df, all_features, CORRELATION_THRESHOLD)\n",
    "removed_log['high_corr'] = removed_corr\n",
    "if not removed_corr:\n",
    "    print(\"  No features removed\")\n",
    "print(f\"  Remaining: {len(FINAL_FEATURES)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 4] High Correlation Filtering\n",
      "  [CORR] BMI vs BMI_WHO: r=0.946, removed BMI_WHO\n",
      "  [CORR] BMI vs BMI_Squared: r=1.000, removed BMI\n",
      "  [CORR] Smoker vs Risk_Behavior: r=0.946, removed Smoker\n",
      "  [CORR] MentHlth vs MentHlth_Cat: r=0.991, removed MentHlth_Cat\n",
      "  [CORR] PhysHlth vs PhysHlth_Cat: r=0.988, removed PhysHlth_Cat\n",
      "  [CORR] Age vs Age_Group: r=0.964, removed Age_Group\n",
      "  [CORR] Age vs Age_BMI_Interaction: r=0.851, removed Age\n",
      "  [CORR] Income vs SDOH_Index: r=0.930, removed SDOH_Index\n",
      "  Remaining: 24\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "b2782115",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:31:42.782345600Z",
     "start_time": "2026-02-02T11:31:42.756252200Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FEATURE SELECTION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Initial: {len(df.columns) - 2} | Final: {len(FINAL_FEATURES)}\")\n",
    "print(f\"\\n[REMOVED LOG]\")\n",
    "for k, v in removed_log.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "all_removed = sum(removed_log.values(), [])\n",
    "print(f\"\\nTotal removed: {len(all_removed)}\")\n",
    "print(f\"\\n[FINAL FEATURES]\\n{FINAL_FEATURES}\")\n",
    "print(\"=\"*50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FEATURE SELECTION SUMMARY\n",
      "==================================================\n",
      "Initial: 35 | Final: 24\n",
      "\n",
      "[REMOVED LOG]\n",
      "  low_utility: ['AnyHealthcare', 'CholCheck']\n",
      "  near_zero_var: ['Is_Extreme_BMI']\n",
      "  high_vif: []\n",
      "  high_corr: ['Age_Group', 'SDOH_Index', 'Age', 'Smoker', 'BMI_WHO', 'PhysHlth_Cat', 'BMI', 'MentHlth_Cat']\n",
      "\n",
      "Total removed: 11\n",
      "\n",
      "[FINAL FEATURES]\n",
      "['HighBP', 'HighChol', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Education', 'Income', 'Age_BMI_Interaction', 'CVD_Risk', 'MetSyn_Risk', 'Chronic_Count', 'Lifestyle_Score', 'Risk_Behavior', 'BMI_Squared', 'Health_Imbalance']\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "653943af",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Stratified Data Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "id": "d62f85f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:31:42.933407100Z",
     "start_time": "2026-02-02T11:31:42.785347600Z"
    }
   },
   "source": [
    "def stratified_split(df: pd.DataFrame, features: List[str], target: str, weight: str,\n",
    "                     test_size: float = 0.2, seed: int = 42):\n",
    "    \"\"\"Execute stratified train-test split preserving class distribution and sample weights.\"\"\"\n",
    "    X, y, w = df[features], df[target], df[weight]\n",
    "    return train_test_split(X, y, w, test_size=test_size, stratify=y, random_state=seed)\n",
    "\n",
    "\n",
    "print(\"=\"*50)\n",
    "X_train, X_test, y_train, y_test, w_train, w_test = stratified_split(\n",
    "    df, FINAL_FEATURES, TARGET_COL, WEIGHT_COL, TEST_SIZE, RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train):,} ({(1-TEST_SIZE)*100:.0f}%)\")\n",
    "print(f\"Test:  {len(X_test):,} ({TEST_SIZE*100:.0f}%)\")\n",
    "\n",
    "train_rate, test_rate = y_train.mean(), y_test.mean()\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Train positive: {train_rate:.4f}\")\n",
    "print(f\"  Test positive:  {test_rate:.4f}\")\n",
    "\n",
    "assert np.isclose(train_rate, test_rate, atol=0.01), \"Stratification failed\"\n",
    "print(\"\\n[VALIDATED] Stratification verified\")\n",
    "print(\"=\"*50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Train: 183,436 (80%)\n",
      "Test:  45,860 (20%)\n",
      "\n",
      "Class distribution:\n",
      "  Train positive: 0.1727\n",
      "  Test positive:  0.1727\n",
      "\n",
      "[VALIDATED] Stratification verified\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "08b66112",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Robust Numerical Scaling"
   ]
  },
  {
   "cell_type": "code",
   "id": "11ee0c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:31:43.054590Z",
     "start_time": "2026-02-02T11:31:42.935405600Z"
    }
   },
   "source": [
    "def identify_continuous(df: pd.DataFrame, threshold: int = 10) -> List[str]:\n",
    "    \"\"\"Identify features with >threshold unique values as continuous.\"\"\"\n",
    "    return [c for c in df.columns if df[c].nunique() > threshold]\n",
    "\n",
    "\n",
    "def apply_robust_scaling(X_train: pd.DataFrame, X_test: pd.DataFrame, \n",
    "                         cols: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame, RobustScaler]:\n",
    "    \"\"\"Apply RobustScaler (median/IQR normalization) fitted on training set only.\"\"\"\n",
    "    Xtr, Xte = X_train.copy(), X_test.copy()\n",
    "    cols_scale = [c for c in cols if c in Xtr.columns]\n",
    "    if not cols_scale:\n",
    "        print(\"[WARNING] No continuous columns\")\n",
    "        return Xtr, Xte, None\n",
    "    scaler = RobustScaler()\n",
    "    Xtr[cols_scale] = scaler.fit_transform(Xtr[cols_scale])\n",
    "    Xte[cols_scale] = scaler.transform(Xte[cols_scale])\n",
    "    print(f\"[SCALING] Applied to {len(cols_scale)} features: {cols_scale}\")\n",
    "    return Xtr, Xte, scaler\n",
    "\n",
    "\n",
    "print(\"=\"*50)\n",
    "continuous_features = identify_continuous(X_train, threshold=10)\n",
    "print(f\"Continuous features: {len(continuous_features)}\")\n",
    "\n",
    "X_train_scaled, X_test_scaled, scaler = apply_robust_scaling(X_train, X_test, continuous_features)\n",
    "\n",
    "if scaler:\n",
    "    print(f\"\\nPost-scaling validation (train):\")\n",
    "    for c in continuous_features[:3]:\n",
    "        if c in X_train_scaled.columns:\n",
    "            print(f\"  {c}: median={X_train_scaled[c].median():.4f}\")\n",
    "print(\"\\n[VALIDATED] Scaling complete\")\n",
    "print(\"=\"*50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Continuous features: 4\n",
      "[SCALING] Applied to 4 features: ['MentHlth', 'PhysHlth', 'Age_BMI_Interaction', 'BMI_Squared']\n",
      "\n",
      "Post-scaling validation (train):\n",
      "  MentHlth: median=0.0000\n",
      "  PhysHlth: median=0.0000\n",
      "  Age_BMI_Interaction: median=0.0000\n",
      "\n",
      "[VALIDATED] Scaling complete\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "a46a2c91",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Multi-Task Data Export"
   ]
  },
  {
   "cell_type": "code",
   "id": "14c97987",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:31:43.069783900Z",
     "start_time": "2026-02-02T11:31:43.056589100Z"
    }
   },
   "source": [
    "def export_classification(X_tr, X_te, y_tr, y_te, w_tr, w_te, tr_path, te_path):\n",
    "    \"\"\"Export train and test datasets with features, target, and sample weights.\"\"\"\n",
    "    train_df = X_tr.copy()\n",
    "    train_df['Diabetes_binary'] = y_tr.values\n",
    "    train_df['Sample_Weight'] = w_tr.values\n",
    "    \n",
    "    test_df = X_te.copy()\n",
    "    test_df['Diabetes_binary'] = y_te.values\n",
    "    test_df['Sample_Weight'] = w_te.values\n",
    "    \n",
    "    train_df.to_csv(tr_path, index=False)\n",
    "    test_df.to_csv(te_path, index=False)\n",
    "    print(f\"[EXPORT] Train: {tr_path.name} ({len(train_df):,})\")\n",
    "    print(f\"[EXPORT] Test: {te_path.name} ({len(test_df):,})\")\n",
    "\n",
    "\n",
    "def export_clustering(df, features, path, scaler=None, cont_cols=None):\n",
    "    \"\"\"Export feature matrix for clustering tasks without target column.\"\"\"\n",
    "    cluster_df = df[features].copy()\n",
    "    if scaler and cont_cols:\n",
    "        cols = [c for c in cont_cols if c in cluster_df.columns]\n",
    "        if cols:\n",
    "            cluster_df[cols] = scaler.transform(cluster_df[cols])\n",
    "    cluster_df.to_csv(path, index=False)\n",
    "    print(f\"[EXPORT] Clustering: {path.name} ({len(cluster_df):,}, {len(features)} features)\")\n",
    "\n",
    "\n",
    "def export_metadata(features, removed, cont_cols, path):\n",
    "    \"\"\"Export feature metadata including binning definitions and pipeline configuration.\"\"\"\n",
    "    # Binning definitions for engineered categorical features\n",
    "    binning_definitions = {\n",
    "        'BMI_WHO': {\n",
    "            'description': 'WHO BMI classification',\n",
    "            'bins': {'1': 'Underweight (BMI < 18.5)', '2': 'Normal (18.5 <= BMI < 25)', \n",
    "                     '3': 'Overweight (25 <= BMI < 30)', '4': 'Obese (BMI >= 30)'}\n",
    "        },\n",
    "        'Age_Group': {\n",
    "            'description': 'Life stage categories derived from Age (1-13)',\n",
    "            'bins': {'1': '18-29 (Young Adult)', '2': '30-44 (Early Middle Age)', \n",
    "                     '3': '45-59 (Late Middle Age)', '4': '60-74 (Young-Old)', '5': '75+ (Old-Old)'}\n",
    "        },\n",
    "        'MentHlth_Cat': {\n",
    "            'description': 'Mental health burden (days in past 30)',\n",
    "            'bins': {'0': 'None (0 days)', '1': 'Moderate (1-13 days)', '2': 'Severe (14-30 days)'}\n",
    "        },\n",
    "        'PhysHlth_Cat': {\n",
    "            'description': 'Physical health burden (days in past 30)',\n",
    "            'bins': {'0': 'None (0 days)', '1': 'Moderate (1-13 days)', '2': 'Severe (14-30 days)'}\n",
    "        },\n",
    "        'Is_Extreme_BMI': {\n",
    "            'description': 'Extreme obesity indicator',\n",
    "            'bins': {'0': 'BMI <= 50', '1': 'BMI > 50 (extreme)'}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Composite feature definitions\n",
    "    composite_definitions = {\n",
    "        'Age_BMI_Interaction': 'Age × BMI product term for non-linear synergy',\n",
    "        'CVD_Risk': 'HighBP × HighChol (cardiovascular co-morbidity indicator)',\n",
    "        'MetSyn_Risk': 'HighBP + HighChol + (BMI_WHO >= 4), range [0-3]',\n",
    "        'Chronic_Count': 'Sum of HighBP, HighChol, HeartDiseaseorAttack, Stroke',\n",
    "        'SDOH_Index': '(Education + Income) / 2, social determinants composite',\n",
    "        'Lifestyle_Score': 'PhysActivity + Fruits + Veggies, range [0-3]',\n",
    "        'Risk_Behavior': 'Smoker + HvyAlcoholConsump, range [0-2]',\n",
    "        'BMI_Squared': 'BMI² for non-linear effects',\n",
    "        'Health_Imbalance': 'MentHlth_Cat - PhysHlth_Cat, range [-2, 2]'\n",
    "    }\n",
    "    \n",
    "    meta = {\n",
    "        'final_features': features,\n",
    "        'feature_count': len(features),\n",
    "        'removed_features': removed,\n",
    "        'continuous_features': cont_cols,\n",
    "        'scaling_method': 'RobustScaler',\n",
    "        'train_test_split': {'test_size': TEST_SIZE, 'random_state': RANDOM_STATE, 'stratified': True},\n",
    "        'binning_definitions': binning_definitions,\n",
    "        'composite_definitions': composite_definitions,\n",
    "        'thresholds': {\n",
    "            'BMI_EXTREME': BMI_EXTREME_THRESHOLD,\n",
    "            'BMI_WINSORIZE_PCT': BMI_WINSORIZE_PERCENTILE,\n",
    "            'VARIANCE': VARIANCE_THRESHOLD,\n",
    "            'VIF': VIF_THRESHOLD,\n",
    "            'CORRELATION': CORRELATION_THRESHOLD\n",
    "        }\n",
    "    }\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    print(f\"[EXPORT] Metadata: {path.name}\")"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "25a17b36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:31:53.805284400Z",
     "start_time": "2026-02-02T11:31:43.071783Z"
    }
   },
   "source": [
    "print(\"=\"*50)\n",
    "print(\"MULTI-TASK DATA EXPORT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "export_classification(X_train_scaled, X_test_scaled, y_train, y_test, \n",
    "                      w_train, w_test, TRAIN_OUTPUT, TEST_OUTPUT)\n",
    "\n",
    "# === DATA LEAKAGE PREVENTION: Split Column Methodology ===\n",
    "# Test set must remain unseen during all model fitting steps.\n",
    "# A 'split' column is appended to clustering data to maintain explicit\n",
    "# train/test provenance throughout the pipeline.\n",
    "# Prevents index-based errors when CSV files are exported without indices.\n",
    "df_with_split = df.copy()\n",
    "df_with_split['split'] = 'train'\n",
    "df_with_split.loc[X_test.index, 'split'] = 'test'\n",
    "\n",
    "print(f\"\\n[SPLIT] Train samples: {(df_with_split['split'] == 'train').sum():,}\")\n",
    "print(f\"[SPLIT] Test samples: {(df_with_split['split'] == 'test').sum():,}\")\n",
    "\n",
    "# === DUAL-SCALE EXPORT STRATEGY ===\n",
    "# Mixed-type clustering requires raw unscaled data to preserve\n",
    "# categorical semantics and numerical scales.\n",
    "export_clustering(\n",
    "    df_with_split,\n",
    "    FINAL_FEATURES + ['split'],\n",
    "    CLUSTERING_RAW_OUTPUT,\n",
    "    scaler=None\n",
    ")\n",
    "\n",
    "# === STANDARDIZED DATA FOR PCA ===\n",
    "# PCA requires standardized features to prevent scale-dominance bias.\n",
    "# Export StandardScaler-normalized data for dimensionality reduction\n",
    "full_scaler = StandardScaler()\n",
    "X_fully_scaled = full_scaler.fit_transform(df[FINAL_FEATURES])\n",
    "df_fully_scaled = pd.DataFrame(X_fully_scaled, columns=FINAL_FEATURES, index=df.index)\n",
    "df_fully_scaled['split'] = df_with_split['split']\n",
    "export_clustering(\n",
    "    df_fully_scaled,\n",
    "    FINAL_FEATURES + ['split'],\n",
    "    CLUSTERING_SCALED_OUTPUT,\n",
    "    scaler=None\n",
    ")\n",
    "\n",
    "export_metadata(FINAL_FEATURES, removed_log, continuous_features, METADATA_OUTPUT)\n",
    "export_metadata(FINAL_FEATURES, removed_log, continuous_features, METADATA_OUTPUT)\n",
    "if scaler:\n",
    "    joblib.dump(scaler, SCALER_OUTPUT)\n",
    "    print(f\"[EXPORT] Scaler: {SCALER_OUTPUT.name}\")\n",
    "\n",
    "print(\"=\"*50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "MULTI-TASK DATA EXPORT\n",
      "==================================================\n",
      "[EXPORT] Train: CDC_Train_Classification_BASELINE.csv (183,436)\n",
      "[EXPORT] Test: CDC_Test_Classification_BASELINE.csv (45,860)\n",
      "\n",
      "[SPLIT] Train samples: 183,436\n",
      "[SPLIT] Test samples: 45,860\n",
      "[EXPORT] Clustering: CDC_Clustering_RAW.csv (229,296, 25 features)\n",
      "[EXPORT] Clustering: CDC_Clustering_SCALED.csv (229,296, 25 features)\n",
      "[EXPORT] Metadata: feature_metadata.json\n",
      "[EXPORT] Metadata: feature_metadata.json\n",
      "[EXPORT] Scaler: robust_scaler.pkl\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "e93c3136",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "id": "834c3d6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:31:53.890201100Z",
     "start_time": "2026-02-02T11:31:53.847534300Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"#\"*50)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"#\"*50)\n",
    "\n",
    "print(\"\\n[STAGES]\")\n",
    "print(\"  1. Environment Configuration: Library imports and random seed initialization\")\n",
    "print(\"  2. Path Configuration and Constants: File paths and threshold definitions\")\n",
    "print(\"  3. Data Loading and Validation: CSV loading with Sample_Weight normalization check\")\n",
    "print(\"  4. Outlier Refinement: Is_Extreme_BMI marker + P99 winsorization\")\n",
    "print(\"  5. Clinical Discretization: BMI_WHO, Age_Group, MentHlth_Cat, PhysHlth_Cat\")\n",
    "print(\"  6. Interaction Feature Synthesis: Age_BMI_Interaction, CVD_Risk, MetSyn_Risk\")\n",
    "print(\"  7. Feature Aggregation: Chronic_Count, SDOH_Index, Lifestyle_Score, Risk_Behavior\")\n",
    "print(\"  8. Advanced Features: BMI_Squared, Health_Imbalance\")\n",
    "print(\"  9. Feature Quality Control and Selection: Low-utility + Variance + VIF + Correlation filtering\")\n",
    "print(\"  10. Stratified Data Partitioning: 80/20 split with class balance preservation\")\n",
    "print(\"  11. Robust Numerical Scaling: RobustScaler fitted on training set only\")\n",
    "print(\"  12. Multi-Task Data Export: Classification (train/test) + Clustering (raw/scaled) datasets\")\n",
    "print(\"  13. Pipeline Summary: Complete workflow overview and file manifest\")\n",
    "\n",
    "print(f\"\\n[STATS] Initial: {len(df.columns)-2} | Final: {len(FINAL_FEATURES)} | Removed: {len(all_removed)}\")\n",
    "\n",
    "print(f\"\\n[FILES]\")\n",
    "print(f\"  {TRAIN_OUTPUT.name}\")\n",
    "print(f\"  {TEST_OUTPUT.name}\")\n",
    "print(f\"  {CLUSTERING_RAW_OUTPUT.name} (For K-Prototypes)\")\n",
    "print(f\"  {CLUSTERING_SCALED_OUTPUT.name} (For PCA/GMM)\")\n",
    "print(f\"  {METADATA_OUTPUT.name}\")\n",
    "print(f\"  {SCALER_OUTPUT.name}\")\n",
    "\n",
    "print(\"#\"*50)\n",
    "print(\"\\n[READY FOR] Phase 4: Clustering | Phase 5: Classification\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################\n",
      "FEATURE ENGINEERING COMPLETE\n",
      "##################################################\n",
      "\n",
      "[STAGES]\n",
      "  1. Environment Configuration: Library imports and random seed initialization\n",
      "  2. Path Configuration and Constants: File paths and threshold definitions\n",
      "  3. Data Loading and Validation: CSV loading with Sample_Weight normalization check\n",
      "  4. Outlier Refinement: Is_Extreme_BMI marker + P99 winsorization\n",
      "  5. Clinical Discretization: BMI_WHO, Age_Group, MentHlth_Cat, PhysHlth_Cat\n",
      "  6. Interaction Feature Synthesis: Age_BMI_Interaction, CVD_Risk, MetSyn_Risk\n",
      "  7. Feature Aggregation: Chronic_Count, SDOH_Index, Lifestyle_Score, Risk_Behavior\n",
      "  8. Advanced Features: BMI_Squared, Health_Imbalance\n",
      "  9. Feature Quality Control and Selection: Low-utility + Variance + VIF + Correlation filtering\n",
      "  10. Stratified Data Partitioning: 80/20 split with class balance preservation\n",
      "  11. Robust Numerical Scaling: RobustScaler fitted on training set only\n",
      "  12. Multi-Task Data Export: Classification (train/test) + Clustering (raw/scaled) datasets\n",
      "  13. Pipeline Summary: Complete workflow overview and file manifest\n",
      "\n",
      "[STATS] Initial: 35 | Final: 24 | Removed: 11\n",
      "\n",
      "[FILES]\n",
      "  CDC_Train_Classification_BASELINE.csv\n",
      "  CDC_Test_Classification_BASELINE.csv\n",
      "  CDC_Clustering_RAW.csv (For K-Prototypes)\n",
      "  CDC_Clustering_SCALED.csv (For PCA/GMM)\n",
      "  feature_metadata.json\n",
      "  robust_scaler.pkl\n",
      "##################################################\n",
      "\n",
      "[READY FOR] Phase 4: Clustering | Phase 5: Classification\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiaMetric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
