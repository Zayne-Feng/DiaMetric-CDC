{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ebe7c28",
   "metadata": {},
   "source": [
    "# Phase 3: Feature Engineering\n",
    "\n",
    "## Overview\n",
    "This notebook implements systematic feature construction to transform 21 raw BRFSS survey variables into clinically interpretable predictors for diabetes risk modeling. The pipeline employs domain-driven engineering strategies including clinical discretization, interaction synthesis, feature aggregation, and quality control filtering. Outputs support dual modeling objectives: unsupervised population segmentation (Phase 4) and supervised classification (Phase 5).\n",
    "\n",
    "**Core Objectives:**\n",
    "- **Outlier Management**: Flag extreme BMI cases and apply P99 winsorization to preserve outlier information while reducing skewness\n",
    "- **Clinical Discretization**: Convert continuous health metrics into WHO-aligned categories (BMI_WHO, Age_Group, mental/physical health)\n",
    "- **Interaction Synthesis**: Create multiplicative features capturing synergistic effects (Age×BMI, cardiovascular risk, metabolic syndrome)\n",
    "- **Feature Aggregation**: Composite indices for chronic disease burden, lifestyle quality, and behavioral risk\n",
    "- **Quality Control**: Remove low-variance, collinear, and high-VIF features to ensure statistical independence\n",
    "- **Multi-Task Preparation**: Generate four dataset variants optimized for clustering (raw/scaled) and classification (train/test)\n",
    "\n",
    "**Pipeline Stages:**\n",
    "1. Environment Configuration\n",
    "2. Path Configuration and Constants\n",
    "3. Data Loading and Validation\n",
    "4. Outlier Refinement\n",
    "5. Clinical Discretization\n",
    "6. Interaction Feature Synthesis\n",
    "7. Feature Aggregation\n",
    "8. Advanced Features\n",
    "9. Feature Quality Control and Selection\n",
    "10. Stratified Data Partitioning\n",
    "11. Robust Numerical Scaling\n",
    "12. Multi-Task Data Export\n",
    "13. Pipeline Summary\n",
    "\n",
    "## Output Datasets\n",
    "**Purpose**: Provide analysis-ready datasets for downstream unsupervised clustering (Phase 4) and supervised classification (Phase 5). Each dataset variant is optimized for specific algorithmic requirements while maintaining consistent sample weights and target labels.\n",
    "\n",
    "### Classification Datasets (Baseline Features)(for Phase 5)\n",
    "**Location**: `data/processed/feature_engineering/`\n",
    "\n",
    "1. **CDC_Train_Classification_BASELINE.csv**\n",
    "   - **Purpose**: Training set for classification models with engineered features (no cluster-derived features)\n",
    "   - **Schema**: 27 predictive features + Diabetes_binary (target) + Sample_Weight\n",
    "   - **Features**: Includes interaction terms, aggregated indices, and scaled continuous variables\n",
    "   - **Usage**: Model training, cross-validation, hyperparameter tuning\n",
    "\n",
    "2. **CDC_Test_Classification_BASELINE.csv**\n",
    "   - **Purpose**: Held-out test set for unbiased model evaluation\n",
    "   - **Schema**: Identical to training set (consistent feature engineering pipeline)\n",
    "   - **Split Strategy**: Stratified 80/20 split preserving class balance\n",
    "   - **Usage**: Final model evaluation, performance metrics reporting\n",
    "\n",
    "### Clustering Datasets (for Phase 4)\n",
    "**Location**: `data/processed/feature_engineering/`\n",
    "\n",
    "3. **CDC_Clustering_RAW.csv**\n",
    "   - **Purpose**: Unscaled features for distance-based algorithms (K-Prototypes, K-Modes)\n",
    "   - **Schema**: 21 original features + Diabetes_binary + Sample_Weight + split (train/test indicator)\n",
    "   - **Characteristics**: Preserves original scale for categorical/ordinal features\n",
    "   - **Usage**: K-Prototypes clustering, mixed-type distance computation\n",
    "\n",
    "4. **CDC_Clustering_SCALED.csv**\n",
    "   - **Purpose**: Standardized features for variance-sensitive algorithms (PCA, GMM, DBSCAN)\n",
    "   - **Schema**: Same structure as raw dataset but with RobustScaler normalization\n",
    "   - **Scaler**: RobustScaler (median-based, outlier-resistant)\n",
    "   - **Usage**: Dimensionality reduction, density-based clustering\n",
    "\n",
    "### Metadata and Artifacts\n",
    "**Location**: `outputs/feature_engineering/`\n",
    "\n",
    "5. **feature_metadata.json**\n",
    "   - Feature names, data types, engineering methods, and removal rationale\n",
    "   \n",
    "6. **robust_scaler.pkl**\n",
    "   - Fitted RobustScaler object for consistent test set transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb8564",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "39abf51d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:18:24.061495200Z",
     "start_time": "2026-02-04T17:18:23.750749400Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 35)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Pandas: {pd.__version__}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.11 | packaged by Anaconda, Inc. | (main, Dec 10 2025, 21:21:58) [MSC v.1929 64 bit (AMD64)]\n",
      "Pandas: 3.0.0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "eb06dff4",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Path Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "id": "ed3b30da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:18:24.099974400Z",
     "start_time": "2026-02-04T17:18:24.074496800Z"
    }
   },
   "source": [
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "PREPROCESSING_DATA_DIR = DATA_PROCESSED_DIR / \"data_preprocessing\"\n",
    "FEATURE_OUTPUT_DIR = DATA_PROCESSED_DIR / \"feature_engineering\"\n",
    "METADATA_OUTPUT_DIR = PROJECT_ROOT / \"outputs\" / \"feature_engineering\"\n",
    "\n",
    "INPUT_FILE = PREPROCESSING_DATA_DIR / \"CDC_Diabetes_Cleaned.csv\"\n",
    "\n",
    "# === BASELINE OUTPUT FILES ===\n",
    "# These files contain engineered features WITHOUT cluster-derived features.\n",
    "# Purpose: \n",
    "#   1. Baseline for ablation studies (measuring cluster feature contribution)\n",
    "#   2. Input for clustering pipeline (Phase 4) to load labels and weights\n",
    "TRAIN_OUTPUT = FEATURE_OUTPUT_DIR / \"CDC_Train_Classification_BASELINE.csv\"\n",
    "TEST_OUTPUT = FEATURE_OUTPUT_DIR / \"CDC_Test_Classification_BASELINE.csv\"\n",
    "\n",
    "# Raw data prepared for K-Prototypes\n",
    "CLUSTERING_RAW_OUTPUT = FEATURE_OUTPUT_DIR / \"CDC_Clustering_RAW.csv\"\n",
    "# Fully scaled data prepared for PCA/GMM\n",
    "CLUSTERING_SCALED_OUTPUT = FEATURE_OUTPUT_DIR / \"CDC_Clustering_SCALED.csv\"\n",
    "METADATA_OUTPUT = METADATA_OUTPUT_DIR / \"feature_metadata.json\"\n",
    "SCALER_OUTPUT = METADATA_OUTPUT_DIR / \"robust_scaler.pkl\"\n",
    "FEATURE_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "METADATA_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Thresholds\n",
    "BMI_EXTREME_THRESHOLD = 50.0\n",
    "BMI_WINSORIZE_PERCENTILE = 99\n",
    "VARIANCE_THRESHOLD = 0.01\n",
    "VIF_THRESHOLD = 10.0\n",
    "CORRELATION_THRESHOLD = 0.85\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "print(f\"[CONFIG] Preprocessing Data Directory: {PREPROCESSING_DATA_DIR}\")\n",
    "print(f\"[CONFIG] Input: {INPUT_FILE}\")\n",
    "print(f\"[CONFIG] Feature Output: {FEATURE_OUTPUT_DIR}\")\n",
    "\n",
    "print(f\"[CONFIG] Metadata Output: {METADATA_OUTPUT_DIR}\")\n",
    "print(f\"[CONFIG] Metadata Output: {METADATA_OUTPUT_DIR}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIG] Preprocessing Data Directory: D:\\ProgramSoftware\\PyCharm\\WorkPlace\\DiaMetric-CDC\\data\\processed\\data_preprocessing\n",
      "[CONFIG] Input: D:\\ProgramSoftware\\PyCharm\\WorkPlace\\DiaMetric-CDC\\data\\processed\\data_preprocessing\\CDC_Diabetes_Cleaned.csv\n",
      "[CONFIG] Feature Output: D:\\ProgramSoftware\\PyCharm\\WorkPlace\\DiaMetric-CDC\\data\\processed\\feature_engineering\n",
      "[CONFIG] Metadata Output: D:\\ProgramSoftware\\PyCharm\\WorkPlace\\DiaMetric-CDC\\outputs\\feature_engineering\n",
      "[CONFIG] Metadata Output: D:\\ProgramSoftware\\PyCharm\\WorkPlace\\DiaMetric-CDC\\outputs\\feature_engineering\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "41589fa9",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Loading and Validation"
   ]
  },
  {
   "cell_type": "code",
   "id": "b123e849",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:18:24.537828400Z",
     "start_time": "2026-02-04T17:18:24.100973800Z"
    }
   },
   "source": [
    "def load_and_validate_data(filepath: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load dataset and validate Sample_Weight normalization (mean ≈ 1.0).\"\"\"\n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(f\"Input file not found: {filepath}\")\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    required_cols = ['Diabetes_binary', 'Sample_Weight', 'BMI', 'Age', 'GenHlth']\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    assert len(missing) == 0, f\"Missing columns: {missing}\"\n",
    "    \n",
    "    weight_mean = df['Sample_Weight'].mean()\n",
    "    assert np.isclose(weight_mean, 1.0, atol=0.01), f\"Weight mean: {weight_mean:.4f}\"\n",
    "    \n",
    "    print(f\"[LOADED] {len(df):,} records, {df.shape[1]} columns\")\n",
    "    print(f\"[VALIDATED] Sample_Weight mean: {weight_mean:.6f}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_and_validate_data(INPUT_FILE)\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOADED] 229,296 records, 23 columns\n",
      "[VALIDATED] Sample_Weight mean: 1.000000\n",
      "\n",
      "Columns: ['HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income', 'Diabetes_binary', 'Sample_Weight']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   HighBP  HighChol  CholCheck     BMI  Smoker  Stroke  HeartDiseaseorAttack  \\\n",
       "0  0.0000    0.0000     0.0000 14.0000  1.0000  0.0000                0.0000   \n",
       "1  0.0000    0.0000     0.0000 15.0000  0.0000  0.0000                0.0000   \n",
       "2  0.0000    0.0000     0.0000 15.0000  1.0000  0.0000                0.0000   \n",
       "3  0.0000    0.0000     0.0000 15.0000  1.0000  0.0000                1.0000   \n",
       "4  0.0000    0.0000     0.0000 16.0000  0.0000  0.0000                0.0000   \n",
       "\n",
       "   PhysActivity  Fruits  Veggies  HvyAlcoholConsump  AnyHealthcare  \\\n",
       "0        1.0000  1.0000   1.0000             0.0000         1.0000   \n",
       "1        0.0000  1.0000   0.0000             0.0000         0.0000   \n",
       "2        0.0000  0.0000   0.0000             0.0000         1.0000   \n",
       "3        0.0000  1.0000   1.0000             0.0000         0.0000   \n",
       "4        0.0000  0.0000   0.0000             0.0000         1.0000   \n",
       "\n",
       "   NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk    Sex     Age  \\\n",
       "0       0.0000   3.0000    4.0000    4.0000    0.0000 1.0000 11.0000   \n",
       "1       0.0000   1.0000    0.0000    0.0000    0.0000 0.0000  1.0000   \n",
       "2       0.0000   5.0000   20.0000   28.0000    1.0000 0.0000 10.0000   \n",
       "3       1.0000   3.0000    0.0000   29.0000    0.0000 0.0000  7.0000   \n",
       "4       0.0000   2.0000    0.0000    0.0000    0.0000 0.0000 11.0000   \n",
       "\n",
       "   Education  Income  Diabetes_binary  Sample_Weight  \n",
       "0     6.0000  8.0000                0         0.9054  \n",
       "1     5.0000  7.0000                0         0.9054  \n",
       "2     6.0000  4.0000                0         0.9054  \n",
       "3     5.0000  2.0000                0         0.9054  \n",
       "4     5.0000  5.0000                0         0.9054  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>HvyAlcoholConsump</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "      <th>Diabetes_binary</th>\n",
       "      <th>Sample_Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>14.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>28.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "18999906",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Outlier Refinement"
   ]
  },
  {
   "cell_type": "code",
   "id": "0de13a92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:18:24.841986900Z",
     "start_time": "2026-02-04T17:18:24.593110900Z"
    }
   },
   "source": [
    "def mark_extreme_values(df: pd.DataFrame, col: str, threshold: float, marker: str) -> pd.DataFrame:\n",
    "    \"\"\"Create binary flag for values > threshold.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[marker] = (df[col] > threshold).astype(int)\n",
    "    print(f\"[MARKER] {marker}: {df[marker].sum():,} flagged ({df[marker].mean()*100:.2f}%)\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def winsorize_column(df: pd.DataFrame, col: str, pct: float = 99) -> pd.DataFrame:\n",
    "    \"\"\"Clip upper tail at specified percentile.\"\"\"\n",
    "    df = df.copy()\n",
    "    upper = np.percentile(df[col], pct)\n",
    "    orig_max = df[col].max()\n",
    "    df[col] = df[col].clip(upper=upper)\n",
    "    print(f\"[WINSORIZE] {col}: max {orig_max:.2f} -> {upper:.2f} (P{pct})\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df = mark_extreme_values(df, 'BMI', BMI_EXTREME_THRESHOLD, 'Is_Extreme_BMI')\n",
    "df = winsorize_column(df, 'BMI', BMI_WINSORIZE_PERCENTILE)\n",
    "print(f\"\\nBMI after refinement:\")\n",
    "print(df['BMI'].describe())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MARKER] Is_Extreme_BMI: 2,172 flagged (0.95%)\n",
      "[WINSORIZE] BMI: max 98.00 -> 50.00 (P99)\n",
      "\n",
      "BMI after refinement:\n",
      "count   229296.0000\n",
      "mean        28.5725\n",
      "std          6.1899\n",
      "min         12.0000\n",
      "25%         24.0000\n",
      "50%         27.0000\n",
      "75%         32.0000\n",
      "max         50.0000\n",
      "Name: BMI, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "b5a3decc",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Clinical Discretization"
   ]
  },
  {
   "cell_type": "code",
   "id": "25a6bfee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:18:25.237242200Z",
     "start_time": "2026-02-04T17:18:24.996713500Z"
    }
   },
   "source": [
    "def discretize_bmi_who(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Discretize BMI into WHO categories: 1=Underweight, 2=Normal, 3=Overweight, 4=Obese.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['BMI_WHO'] = pd.cut(df['BMI'], bins=[0, 18.5, 25, 30, np.inf], \n",
    "                          labels=[1, 2, 3, 4], include_lowest=True).astype(int)\n",
    "    print(f\"[BMI_WHO]\\n{df['BMI_WHO'].value_counts().sort_index()}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def discretize_age_lifecycle(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Discretize age into lifecycle stages: 1=18-29, 2=30-44, 3=45-59, 4=60-74, 5=75+.\"\"\"\n",
    "    df = df.copy()\n",
    "    mapping = {1:1, 2:1, 3:2, 4:2, 5:3, 6:3, 7:3, 8:4, 9:4, 10:4, 11:5, 12:5, 13:5}\n",
    "    df['Age_Group'] = df['Age'].map(mapping)\n",
    "    print(f\"[Age_Group]\\n{df['Age_Group'].value_counts().sort_index()}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def discretize_health_days(df: pd.DataFrame, src: str, tgt: str) -> pd.DataFrame:\n",
    "    \"\"\"Discretize health burden days: 0=None, 1=Moderate(1-14d), 2=Severe(15-30d).\"\"\"\n",
    "    df = df.copy()\n",
    "    df[tgt] = pd.cut(df[src], bins=[-1, 0, 14, 30], \n",
    "                     labels=[0, 1, 2], include_lowest=True).astype(int)\n",
    "    print(f\"[{tgt}]\\n{df[tgt].value_counts().sort_index()}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"=\"*50)\n",
    "df = discretize_bmi_who(df)\n",
    "print()\n",
    "df = discretize_age_lifecycle(df)\n",
    "print()\n",
    "df = discretize_health_days(df, 'MentHlth', 'MentHlth_Cat')\n",
    "print()\n",
    "df = discretize_health_days(df, 'PhysHlth', 'PhysHlth_Cat')\n",
    "print(\"=\"*50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "[BMI_WHO]\n",
      "BMI_WHO\n",
      "1     3043\n",
      "2    73563\n",
      "3    81366\n",
      "4    71324\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[Age_Group]\n",
      "Age_Group\n",
      "1    12562\n",
      "2    22218\n",
      "3    54377\n",
      "4    86004\n",
      "5    54135\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[MentHlth_Cat]\n",
      "MentHlth_Cat\n",
      "0    152263\n",
      "1     53752\n",
      "2     23281\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[PhysHlth_Cat]\n",
      "PhysHlth_Cat\n",
      "0    136811\n",
      "1     61845\n",
      "2     30640\n",
      "Name: count, dtype: int64\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "1f3cd6ba",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Interaction Feature Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "id": "747f3c3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:18:25.349157100Z",
     "start_time": "2026-02-04T17:18:25.300800600Z"
    }
   },
   "source": [
    "def create_interaction_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate Age×BMI interaction, CVD co-morbidity, and metabolic syndrome risk features.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['Age_BMI_Interaction'] = df['Age'] * df['BMI']\n",
    "    df['CVD_Risk'] = (df['HighBP'] * df['HighChol']).astype(int)\n",
    "    df['MetSyn_Risk'] = df['HighBP'] + df['HighChol'] + (df['BMI_WHO'] >= 4).astype(int)\n",
    "    \n",
    "    print(f\"[Age_BMI_Interaction] range: [{df['Age_BMI_Interaction'].min():.1f}, {df['Age_BMI_Interaction'].max():.1f}]\")\n",
    "    print(f\"[CVD_Risk] positive: {df['CVD_Risk'].sum():,} ({df['CVD_Risk'].mean()*100:.1f}%)\")\n",
    "    print(f\"[MetSyn_Risk] mean: {df['MetSyn_Risk'].mean():.2f}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"=\"*50)\n",
    "df = create_interaction_features(df)\n",
    "print(\"=\"*50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "[Age_BMI_Interaction] range: [14.0, 650.0]\n",
      "[CVD_Risk] positive: 62,174 (27.1%)\n",
      "[MetSyn_Risk] mean: 1.21\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "29501d7d",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Feature Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "id": "28e1661a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:18:25.503413800Z",
     "start_time": "2026-02-04T17:18:25.433339Z"
    }
   },
   "source": [
    "def create_aggregate_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate composite indices: chronic conditions, SDOH, lifestyle, risk behavior.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['Chronic_Count'] = df['HighBP'] + df['HighChol'] + df['HeartDiseaseorAttack'] + df['Stroke']\n",
    "    df['SDOH_Index'] = (df['Education'] + df['Income']) / 2\n",
    "    df['Lifestyle_Score'] = df['PhysActivity'] + df['Fruits'] + df['Veggies']\n",
    "    df['Risk_Behavior'] = df['Smoker'] + df['HvyAlcoholConsump']\n",
    "    \n",
    "    print(f\"[Chronic_Count] range: [0, {df['Chronic_Count'].max()}], mean: {df['Chronic_Count'].mean():.2f}\")\n",
    "    print(f\"[SDOH_Index] range: [{df['SDOH_Index'].min():.1f}, {df['SDOH_Index'].max():.1f}]\")\n",
    "    print(f\"[Lifestyle_Score] range: [0, {df['Lifestyle_Score'].max()}]\")\n",
    "    print(f\"[Risk_Behavior] range: [0, {df['Risk_Behavior'].max()}]\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"=\"*50)\n",
    "df = create_aggregate_features(df)\n",
    "print(\"=\"*50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "[Chronic_Count] range: [0, 4.0], mean: 1.04\n",
      "[SDOH_Index] range: [1.0, 7.0]\n",
      "[Lifestyle_Score] range: [0, 3.0]\n",
      "[Risk_Behavior] range: [0, 2.0]\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "fd0152e1",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "id": "70d12b84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:18:25.607132100Z",
     "start_time": "2026-02-04T17:18:25.544563900Z"
    }
   },
   "source": [
    "def create_advanced_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate BMI² for non-linear effects and mental-physical health imbalance.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['BMI_Squared'] = df['BMI'] ** 2\n",
    "    df['Health_Imbalance'] = df['MentHlth_Cat'] - df['PhysHlth_Cat']\n",
    "    \n",
    "    print(f\"[BMI_Squared] range: [{df['BMI_Squared'].min():.1f}, {df['BMI_Squared'].max():.1f}]\")\n",
    "    print(f\"[Health_Imbalance] range: [{df['Health_Imbalance'].min()}, {df['Health_Imbalance'].max()}]\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"=\"*50)\n",
    "df = create_advanced_features(df)\n",
    "print(f\"\\nCurrent feature count: {df.shape[1]}\")\n",
    "print(\"=\"*50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "[BMI_Squared] range: [144.0, 2500.0]\n",
      "[Health_Imbalance] range: [-2, 2]\n",
      "\n",
      "Current feature count: 37\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "5d0c7fa1",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Feature Quality Control and Selection"
   ]
  },
  {
   "cell_type": "code",
   "id": "dcc623e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:18:25.646970100Z",
     "start_time": "2026-02-04T17:18:25.623116600Z"
    }
   },
   "source": [
    "def calculate_vif(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Calculate VIF with intercept for multicollinearity detection.\"\"\"\n",
    "    X = df[cols].values\n",
    "    X_with_const = np.column_stack([np.ones(X.shape[0]), X])  # Add intercept\n",
    "    vif_data = []\n",
    "    for i, col in enumerate(cols):\n",
    "        try:\n",
    "            vif = variance_inflation_factor(X_with_const, i + 1)  # Skip intercept column\n",
    "        except:\n",
    "            vif = np.inf\n",
    "        vif_data.append({'Feature': col, 'VIF': vif})\n",
    "    return pd.DataFrame(vif_data).sort_values('VIF', ascending=False)\n",
    "\n",
    "\n",
    "def filter_high_vif(df: pd.DataFrame, cols: List[str], threshold: float = 10.0) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Iteratively remove features with VIF > threshold.\"\"\"\n",
    "    remaining, removed = cols.copy(), []\n",
    "    while True:\n",
    "        if len(remaining) < 2:\n",
    "            break\n",
    "        vif_df = calculate_vif(df, remaining)\n",
    "        max_vif = vif_df['VIF'].max()\n",
    "        if max_vif <= threshold or np.isinf(max_vif):\n",
    "            break\n",
    "        worst = vif_df.iloc[0]['Feature']\n",
    "        remaining.remove(worst)\n",
    "        removed.append(worst)\n",
    "        print(f\"  [VIF] Removed {worst} (VIF={max_vif:.2f})\")\n",
    "    return remaining, removed\n",
    "\n",
    "\n",
    "def filter_near_zero_var(df: pd.DataFrame, cols: List[str], threshold: float = 0.01) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Remove features with variance < threshold.\"\"\"\n",
    "    retained, removed = [], []\n",
    "    for col in cols:\n",
    "        v = df[col].var()\n",
    "        if v < threshold:\n",
    "            removed.append(col)\n",
    "            print(f\"  [VAR] Removed {col} (var={v:.6f})\")\n",
    "        else:\n",
    "            retained.append(col)\n",
    "    return retained, removed\n",
    "\n",
    "\n",
    "def filter_high_corr(df: pd.DataFrame, cols: List[str], threshold: float = 0.85) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Remove one feature from highly correlated pairs (Spearman).\"\"\"\n",
    "    corr = df[cols].corr(method='spearman').abs()\n",
    "    to_remove = set()\n",
    "    for i in range(len(cols)):\n",
    "        if cols[i] in to_remove:\n",
    "            continue  # Skip already marked features\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            if cols[j] in to_remove:\n",
    "                continue\n",
    "            if corr.iloc[i, j] > threshold:\n",
    "                ci, cj = cols[i], cols[j]\n",
    "                drop = ci if df[ci].var() < df[cj].var() else cj\n",
    "                to_remove.add(drop)\n",
    "                print(f\"  [CORR] {ci} vs {cj}: r={corr.iloc[i,j]:.3f}, removed {drop}\")\n",
    "    return [c for c in cols if c not in to_remove], list(to_remove)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "3f601e0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:18:25.673641200Z",
     "start_time": "2026-02-04T17:18:25.649969500Z"
    }
   },
   "source": [
    "TARGET_COL = 'Diabetes_binary'\n",
    "WEIGHT_COL = 'Sample_Weight'\n",
    "LOW_UTILITY = ['AnyHealthcare', 'CholCheck']\n",
    "\n",
    "# Redundant features to remove\n",
    "REDUNDANT_RAW = ['MentHlth', 'PhysHlth', 'BMI']\n",
    "\n",
    "all_features = [c for c in df.columns if c not in [TARGET_COL, WEIGHT_COL]]\n",
    "print(f\"[INFO] Features before selection: {len(all_features)}\")\n",
    "\n",
    "# Tracking\n",
    "removed_log = {'low_utility': [], 'near_zero_var': [], 'high_vif': [], 'high_corr': []}"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Features before selection: 35\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "ec2f4364",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:18:25.706158400Z",
     "start_time": "2026-02-04T17:18:25.677643400Z"
    }
   },
   "source": [
    "print(\"=\"*50)\n",
    "print(\"[STEP 1] Low Utility Removal\")\n",
    "for f in LOW_UTILITY + REDUNDANT_RAW:\n",
    "    if f in all_features:\n",
    "        all_features.remove(f)\n",
    "        removed_log['low_utility'].append(f)\n",
    "        print(f\"  Removed (Low Utility/Redundant): {f}\")\n",
    "\n",
    "print(f\"  Remaining: {len(all_features)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "[STEP 1] Low Utility Removal\n",
      "  Removed (Low Utility/Redundant): AnyHealthcare\n",
      "  Removed (Low Utility/Redundant): CholCheck\n",
      "  Removed (Low Utility/Redundant): MentHlth\n",
      "  Removed (Low Utility/Redundant): PhysHlth\n",
      "  Removed (Low Utility/Redundant): BMI\n",
      "  Remaining: 30\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "50d9d46b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:18:25.814629600Z",
     "start_time": "2026-02-04T17:18:25.707160300Z"
    }
   },
   "source": [
    "print(\"\\n[STEP 2] Near-Zero Variance Filtering\")\n",
    "all_features, removed_var = filter_near_zero_var(df, all_features, VARIANCE_THRESHOLD)\n",
    "removed_log['near_zero_var'] = removed_var\n",
    "if not removed_var:\n",
    "    print(\"  No features removed\")\n",
    "print(f\"  Remaining: {len(all_features)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2] Near-Zero Variance Filtering\n",
      "  [VAR] Removed Is_Extreme_BMI (var=0.009383)\n",
      "  Remaining: 29\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "d27e64a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:18:39.892578400Z",
     "start_time": "2026-02-04T17:18:25.816631Z"
    }
   },
   "source": [
    "print(\"\\n[STEP 3] VIF Multicollinearity Filtering\")\n",
    "numeric_cols = df[all_features].select_dtypes(include=[np.number]).columns.tolist()\n",
    "all_features, removed_vif = filter_high_vif(df, numeric_cols, VIF_THRESHOLD)\n",
    "removed_log['high_vif'] = removed_vif\n",
    "if not removed_vif:\n",
    "    print(\"  No features removed\")\n",
    "print(f\"  Remaining: {len(all_features)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 3] VIF Multicollinearity Filtering\n",
      "  No features removed\n",
      "  Remaining: 29\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "b341576a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:18:41.266747400Z",
     "start_time": "2026-02-04T17:18:39.933628800Z"
    }
   },
   "source": [
    "print(\"\\n[STEP 4] High Correlation Filtering\")\n",
    "FINAL_FEATURES, removed_corr = filter_high_corr(df, all_features, CORRELATION_THRESHOLD)\n",
    "removed_log['high_corr'] = removed_corr\n",
    "if not removed_corr:\n",
    "    print(\"  No features removed\")\n",
    "print(f\"  Remaining: {len(FINAL_FEATURES)}\")\n",
    "# 1. Map the current DataFrame index to an explicit columnme index to an explicit column\n",
    "df['Original_Index'] = df.index\n",
    "# 2. Add this index column to the final feature list to participate in the Split and Export process\n",
    "FINAL_FEATURES.append('Original_Index')\n",
    "print(f\"[INDEXING] 'Original_Index' added to features. Final count: {len(FINAL_FEATURES)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 4] High Correlation Filtering\n",
      "  [CORR] Smoker vs Risk_Behavior: r=0.946, removed Smoker\n",
      "  [CORR] Age vs Age_Group: r=0.964, removed Age_Group\n",
      "  [CORR] Age vs Age_BMI_Interaction: r=0.851, removed Age\n",
      "  [CORR] Income vs SDOH_Index: r=0.930, removed SDOH_Index\n",
      "  [CORR] BMI_WHO vs BMI_Squared: r=0.946, removed BMI_WHO\n",
      "  Remaining: 24\n",
      "[INDEXING] 'Original_Index' added to features. Final count: 25\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "653943af",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Stratified Data Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "id": "d62f85f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:18:41.437342900Z",
     "start_time": "2026-02-04T17:18:41.283444200Z"
    }
   },
   "source": [
    "def stratified_split(df: pd.DataFrame, features: List[str], target: str, weight: str,\n",
    "                     test_size: float = 0.2, seed: int = 42):\n",
    "    \"\"\"Execute stratified train-test split preserving class distribution and sample weights.\"\"\"\n",
    "    X, y, w = df[features], df[target], df[weight]\n",
    "    return train_test_split(X, y, w, test_size=test_size, stratify=y, random_state=seed)\n",
    "\n",
    "\n",
    "print(\"=\"*50)\n",
    "X_train, X_test, y_train, y_test, w_train, w_test = stratified_split(\n",
    "    df, FINAL_FEATURES, TARGET_COL, WEIGHT_COL, TEST_SIZE, RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train):,} ({(1-TEST_SIZE)*100:.0f}%)\")\n",
    "print(f\"Test:  {len(X_test):,} ({TEST_SIZE*100:.0f}%)\")\n",
    "\n",
    "train_rate, test_rate = y_train.mean(), y_test.mean()\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Train positive: {train_rate:.4f}\")\n",
    "print(f\"  Test positive:  {test_rate:.4f}\")\n",
    "\n",
    "assert np.isclose(train_rate, test_rate, atol=0.01), \"Stratification failed\"\n",
    "print(\"\\n[VALIDATED] Stratification verified\")\n",
    "print(\"=\"*50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Train: 183,436 (80%)\n",
      "Test:  45,860 (20%)\n",
      "\n",
      "Class distribution:\n",
      "  Train positive: 0.1727\n",
      "  Test positive:  0.1727\n",
      "\n",
      "[VALIDATED] Stratification verified\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "08b66112",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Robust Numerical Scaling"
   ]
  },
  {
   "cell_type": "code",
   "id": "11ee0c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:18:41.574572900Z",
     "start_time": "2026-02-04T17:18:41.441619100Z"
    }
   },
   "source": [
    "def identify_continuous(df: pd.DataFrame, threshold: int = 10) -> List[str]:\n",
    "    \"\"\"Identify features with >threshold unique values as continuous.\"\"\"\n",
    "    return [c for c in df.columns if df[c].nunique() > threshold]\n",
    "\n",
    "\n",
    "def apply_robust_scaling(X_train: pd.DataFrame, X_test: pd.DataFrame, \n",
    "                         cols: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame, RobustScaler]:\n",
    "    \"\"\"Apply RobustScaler (median/IQR normalization) fitted on training set only.\"\"\"\n",
    "    Xtr, Xte = X_train.copy(), X_test.copy()\n",
    "    cols_scale = [c for c in cols if c in Xtr.columns]\n",
    "    if not cols_scale:\n",
    "        print(\"[WARNING] No continuous columns\")\n",
    "        return Xtr, Xte, None\n",
    "    scaler = RobustScaler()\n",
    "    Xtr[cols_scale] = scaler.fit_transform(Xtr[cols_scale])\n",
    "    Xte[cols_scale] = scaler.transform(Xte[cols_scale])\n",
    "    print(f\"[SCALING] Applied to {len(cols_scale)} features: {cols_scale}\")\n",
    "    return Xtr, Xte, scaler\n",
    "\n",
    "\n",
    "print(\"=\"*50)\n",
    "continuous_features = [c for c in identify_continuous(X_train, threshold=5) \n",
    "                       if c != 'Original_Index']\n",
    "\n",
    "print(f\"Continuous features (excluding Index): {len(continuous_features)}\")\n",
    "X_train_scaled, X_test_scaled, scaler = apply_robust_scaling(X_train, X_test, continuous_features)\n",
    "\n",
    "if scaler:\n",
    "    print(f\"\\nPost-scaling validation (train):\")\n",
    "    for c in continuous_features[:3]:\n",
    "        if c in X_train_scaled.columns:\n",
    "            print(f\"  {c}: median={X_train_scaled[c].median():.4f}\")\n",
    "print(\"\\n[VALIDATED] Scaling complete\")\n",
    "print(\"=\"*50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Continuous features (excluding Index): 4\n",
      "[SCALING] Applied to 4 features: ['Education', 'Income', 'Age_BMI_Interaction', 'BMI_Squared']\n",
      "\n",
      "Post-scaling validation (train):\n",
      "  Education: median=0.0000\n",
      "  Income: median=0.0000\n",
      "  Age_BMI_Interaction: median=0.0000\n",
      "\n",
      "[VALIDATED] Scaling complete\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "a46a2c91",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Multi-Task Data Export"
   ]
  },
  {
   "cell_type": "code",
   "id": "14c97987",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:18:41.590924700Z",
     "start_time": "2026-02-04T17:18:41.575581100Z"
    }
   },
   "source": [
    "def export_classification(X_tr, X_te, y_tr, y_te, w_tr, w_te, tr_path, te_path):\n",
    "    \"\"\"Export train and test datasets with explicit index alignment check.\"\"\"\n",
    "    # Ensure indices match perfectly before re-attaching target and weights\n",
    "    assert (X_tr.index == y_tr.index).all(), \"Critical: Train Index Mismatch!\"\n",
    "    assert (X_tr.index == w_tr.index).all(), \"Critical: Train Weight Mismatch!\"\n",
    "    \n",
    "    train_df = X_tr.copy()\n",
    "    train_df['Diabetes_binary'] = y_tr # Dropping .values preserves index alignment safety\n",
    "    train_df['Sample_Weight'] = w_tr\n",
    "    \n",
    "    test_df = X_te.copy()\n",
    "    test_df['Diabetes_binary'] = y_te\n",
    "    test_df['Sample_Weight'] = w_te\n",
    "    \n",
    "    train_df.to_csv(tr_path, index=False)\n",
    "    test_df.to_csv(te_path, index=False)\n",
    "    print(f\"[EXPORT] Train: {tr_path.name} | [STABLE] Index-aligned\")\n",
    "    print(f\"[EXPORT] Test: {te_path.name} | [STABLE] Index-aligned\")\n",
    "\n",
    "\n",
    "def export_clustering(df, features, path, scaler=None, cont_cols=None):\n",
    "    \"\"\"\n",
    "    Export feature matrix for clustering. \n",
    "    'features' here should be the list of model inputs + Original_Index.\n",
    "    \"\"\"\n",
    "    cluster_df = df[features].copy()\n",
    "    cluster_df.to_csv(path, index=False)\n",
    "    if scaler and cont_cols:\n",
    "        cols = [c for c in cont_cols if c in cluster_df.columns]\n",
    "        if cols:\n",
    "            cluster_df[cols] = scaler.transform(cluster_df[cols])\n",
    "    cluster_df.to_csv(path, index=False)\n",
    "    print(f\"[EXPORT] Clustering: {path.name} ({len(cluster_df):,}, {len(features)} total columns)\")\n",
    "\n",
    "def export_metadata(features, removed, cont_cols, path):\n",
    "    \"\"\"Export feature metadata including binning definitions and pipeline configuration.\"\"\"\n",
    "    # Binning definitions for engineered categorical features\n",
    "    binning_definitions = {\n",
    "        'BMI_WHO': {\n",
    "            'description': 'WHO BMI classification',\n",
    "            'bins': {'1': 'Underweight (BMI < 18.5)', '2': 'Normal (18.5 <= BMI < 25)', \n",
    "                     '3': 'Overweight (25 <= BMI < 30)', '4': 'Obese (BMI >= 30)'}\n",
    "        },\n",
    "        'Age_Group': {\n",
    "            'description': 'Life stage categories derived from Age (1-13)',\n",
    "            'bins': {'1': '18-29 (Young Adult)', '2': '30-44 (Early Middle Age)', \n",
    "                     '3': '45-59 (Late Middle Age)', '4': '60-74 (Young-Old)', '5': '75+ (Old-Old)'}\n",
    "        },\n",
    "        'MentHlth_Cat': {\n",
    "            'description': 'Mental health burden (days in past 30)',\n",
    "            'bins': {'0': 'None (0 days)', '1': 'Moderate (1-14 days)', '2': 'Severe (15-30 days)'}\n",
    "        },\n",
    "        'PhysHlth_Cat': {\n",
    "            'description': 'Physical health burden (days in past 30)',\n",
    "            'bins': {'0': 'None (0 days)', '1': 'Moderate (1-14 days)', '2': 'Severe (15-30 days)'}\n",
    "        },\n",
    "        'Is_Extreme_BMI': {\n",
    "            'description': 'Extreme obesity indicator',\n",
    "            'bins': {'0': 'BMI <= 50', '1': 'BMI > 50 (extreme)'}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Composite feature definitions\n",
    "    composite_definitions = {\n",
    "        'Age_BMI_Interaction': 'Age × BMI product term for non-linear synergy',\n",
    "        'CVD_Risk': 'HighBP × HighChol (cardiovascular co-morbidity indicator)',\n",
    "        'MetSyn_Risk': 'HighBP + HighChol + (BMI_WHO >= 4), range [0-3]',\n",
    "        'Chronic_Count': 'Sum of HighBP, HighChol, HeartDiseaseorAttack, Stroke',\n",
    "        'SDOH_Index': '(Education + Income) / 2, social determinants composite',\n",
    "        'Lifestyle_Score': 'PhysActivity + Fruits + Veggies, range [0-3]',\n",
    "        'Risk_Behavior': 'Smoker + HvyAlcoholConsump, range [0-2]',\n",
    "        'BMI_Squared': 'BMI² for non-linear effects',\n",
    "        'Health_Imbalance': 'MentHlth_Cat - PhysHlth_Cat, range [-2, 2]'\n",
    "    }\n",
    "    \n",
    "    meta = {\n",
    "        'final_features': features,\n",
    "        'feature_count': len(features),\n",
    "        'removed_features': removed,\n",
    "        'continuous_features': cont_cols,\n",
    "        'scaling_method': 'RobustScaler',\n",
    "        'train_test_split': {'test_size': TEST_SIZE, 'random_state': RANDOM_STATE, 'stratified': True},\n",
    "        'binning_definitions': binning_definitions,\n",
    "        'composite_definitions': composite_definitions,\n",
    "        'thresholds': {\n",
    "            'BMI_EXTREME': BMI_EXTREME_THRESHOLD,\n",
    "            'BMI_WINSORIZE_PCT': BMI_WINSORIZE_PERCENTILE,\n",
    "            'VARIANCE': VARIANCE_THRESHOLD,\n",
    "            'VIF': VIF_THRESHOLD,\n",
    "            'CORRELATION': CORRELATION_THRESHOLD\n",
    "        }\n",
    "    }\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    print(f\"[EXPORT] Metadata: {path.name}\")"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "25a17b36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:18:54.779797300Z",
     "start_time": "2026-02-04T17:18:41.592924100Z"
    }
   },
   "source": [
    "print(\"=\"*50)\n",
    "print(\"MULTI-TASK DATA EXPORT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "export_classification(X_train_scaled, X_test_scaled, y_train, y_test, \n",
    "                      w_train, w_test, TRAIN_OUTPUT, TEST_OUTPUT)\n",
    "\n",
    "# === DATA LEAKAGE PREVENTION: Split Column Methodology ===\n",
    "# Test set must remain unseen during all model fitting steps.\n",
    "# A 'split' column is appended to clustering data to maintain explicit\n",
    "# train/test provenance throughout the pipeline.\n",
    "# Prevents index-based errors when CSV files are exported without indices.\n",
    "df_with_split = df.copy()\n",
    "df_with_split['split'] = 'train'\n",
    "df_with_split.loc[X_test.index, 'split'] = 'test'\n",
    "\n",
    "print(f\"\\n[SPLIT] Train samples: {(df_with_split['split'] == 'train').sum():,}\")\n",
    "print(f\"[SPLIT] Test samples: {(df_with_split['split'] == 'test').sum():,}\")\n",
    "\n",
    "# === DUAL-SCALE EXPORT STRATEGY ===\n",
    "# Mixed-type clustering requires raw unscaled data to preserve\n",
    "# categorical semantics and numerical scales.\n",
    "export_clustering(\n",
    "    df_with_split,\n",
    "    FINAL_FEATURES + ['split'], \n",
    "    CLUSTERING_RAW_OUTPUT,\n",
    "    scaler=None\n",
    ")\n",
    "\n",
    "\n",
    "# Fully scaled data for distance-based clustering and PCA\n",
    "full_scaler = RobustScaler()\n",
    "features_to_scale = [c for c in FINAL_FEATURES if c in continuous_features]\n",
    "train_indices = df_with_split[df_with_split['split'] == 'train'].index\n",
    "full_scaler.fit(df_with_split.loc[train_indices, features_to_scale])\n",
    "df_fully_scaled = df_with_split[FINAL_FEATURES + ['split']].copy()\n",
    "df_fully_scaled[features_to_scale] = full_scaler.transform(df_fully_scaled[features_to_scale])\n",
    "export_clustering(\n",
    "    df_fully_scaled,\n",
    "    FINAL_FEATURES + ['split'],\n",
    "    CLUSTERING_SCALED_OUTPUT,\n",
    "    scaler=None\n",
    ")\n",
    "export_metadata(FINAL_FEATURES, removed_log, continuous_features, METADATA_OUTPUT)\n",
    "if full_scaler:\n",
    "    joblib.dump(full_scaler, SCALER_OUTPUT)\n",
    "    print(f\"[EXPORT] Scaler: {SCALER_OUTPUT.name}\")\n",
    "\n",
    "print(\"=\"*50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "MULTI-TASK DATA EXPORT\n",
      "==================================================\n",
      "[EXPORT] Train: CDC_Train_Classification_BASELINE.csv | [STABLE] Index-aligned\n",
      "[EXPORT] Test: CDC_Test_Classification_BASELINE.csv | [STABLE] Index-aligned\n",
      "\n",
      "[SPLIT] Train samples: 183,436\n",
      "[SPLIT] Test samples: 45,860\n",
      "[EXPORT] Clustering: CDC_Clustering_RAW.csv (229,296, 26 total columns)\n",
      "[EXPORT] Clustering: CDC_Clustering_SCALED.csv (229,296, 26 total columns)\n",
      "[EXPORT] Metadata: feature_metadata.json\n",
      "[EXPORT] Scaler: robust_scaler.pkl\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "e93c3136",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "id": "834c3d6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:18:54.848479100Z",
     "start_time": "2026-02-04T17:18:54.821233800Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"#\"*50)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"#\"*50)\n",
    "\n",
    "print(\"\\n[STAGES]\")\n",
    "print(\"  1. Environment Configuration: Library imports and random seed initialization\")\n",
    "print(\"  2. Path Configuration and Constants: File paths and threshold definitions\")\n",
    "print(\"  3. Data Loading and Validation: CSV loading with Sample_Weight normalization check\")\n",
    "print(\"  4. Outlier Refinement: Is_Extreme_BMI marker + P99 winsorization\")\n",
    "print(\"  5. Clinical Discretization: BMI_WHO, Age_Group, MentHlth_Cat, PhysHlth_Cat\")\n",
    "print(\"  6. Interaction Feature Synthesis: Age_BMI_Interaction, CVD_Risk, MetSyn_Risk\")\n",
    "print(\"  7. Feature Aggregation: Chronic_Count, SDOH_Index, Lifestyle_Score, Risk_Behavior\")\n",
    "print(\"  8. Advanced Features: BMI_Squared, Health_Imbalance\")\n",
    "print(\"  9. Feature Quality Control and Selection: Low-utility + Variance + VIF + Correlation filtering\")\n",
    "print(\"  10. Stratified Data Partitioning: 80/20 split with class balance preservation\")\n",
    "print(\"  11. Robust Numerical Scaling: RobustScaler fitted on training set only\")\n",
    "print(\"  12. Multi-Task Data Export: Classification (train/test) + Clustering (raw/scaled) datasets\")\n",
    "print(\"  13. Pipeline Summary: Complete workflow overview and file manifest\")\n",
    "\n",
    "all_removed = [item for sublist in removed_log.values() for item in sublist]\n",
    "predictive_feature_count = len([f for f in FINAL_FEATURES if f != 'Original_Index'])\n",
    "print(f\"\\n[STATS] Initial: {len(df.columns)-2} | Final Predictors: {predictive_feature_count} | Identifiers: 1\")\n",
    "\n",
    "print(f\"\\n[FILES]\")\n",
    "print(f\"  Classification Datasets (data/processed/feature_engineering/):\")\n",
    "print(f\"    - {TRAIN_OUTPUT.name}\")\n",
    "print(f\"    - {TEST_OUTPUT.name}\")\n",
    "print(f\"  Clustering Datasets (data/processed/feature_engineering/):\")\n",
    "print(f\"    - {CLUSTERING_RAW_OUTPUT.name} (For K-Prototypes)\")\n",
    "print(f\"    - {CLUSTERING_SCALED_OUTPUT.name} (For PCA/GMM)\")\n",
    "print(f\"  Metadata (outputs/feature_engineering/):\")\n",
    "print(f\"    - {METADATA_OUTPUT.name}\")\n",
    "print(f\"    - {SCALER_OUTPUT.name}\")\n",
    "\n",
    "print(\"#\"*50)\n",
    "print(\"\\n[READY FOR] Phase 4: Clustering | Phase 5: Classification\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################\n",
      "FEATURE ENGINEERING COMPLETE\n",
      "##################################################\n",
      "\n",
      "[STAGES]\n",
      "  1. Environment Configuration: Library imports and random seed initialization\n",
      "  2. Path Configuration and Constants: File paths and threshold definitions\n",
      "  3. Data Loading and Validation: CSV loading with Sample_Weight normalization check\n",
      "  4. Outlier Refinement: Is_Extreme_BMI marker + P99 winsorization\n",
      "  5. Clinical Discretization: BMI_WHO, Age_Group, MentHlth_Cat, PhysHlth_Cat\n",
      "  6. Interaction Feature Synthesis: Age_BMI_Interaction, CVD_Risk, MetSyn_Risk\n",
      "  7. Feature Aggregation: Chronic_Count, SDOH_Index, Lifestyle_Score, Risk_Behavior\n",
      "  8. Advanced Features: BMI_Squared, Health_Imbalance\n",
      "  9. Feature Quality Control and Selection: Low-utility + Variance + VIF + Correlation filtering\n",
      "  10. Stratified Data Partitioning: 80/20 split with class balance preservation\n",
      "  11. Robust Numerical Scaling: RobustScaler fitted on training set only\n",
      "  12. Multi-Task Data Export: Classification (train/test) + Clustering (raw/scaled) datasets\n",
      "  13. Pipeline Summary: Complete workflow overview and file manifest\n",
      "\n",
      "[STATS] Initial: 36 | Final Predictors: 24 | Identifiers: 1\n",
      "\n",
      "[FILES]\n",
      "  Classification Datasets (data/processed/feature_engineering/):\n",
      "    - CDC_Train_Classification_BASELINE.csv\n",
      "    - CDC_Test_Classification_BASELINE.csv\n",
      "  Clustering Datasets (data/processed/feature_engineering/):\n",
      "    - CDC_Clustering_RAW.csv (For K-Prototypes)\n",
      "    - CDC_Clustering_SCALED.csv (For PCA/GMM)\n",
      "  Metadata (outputs/feature_engineering/):\n",
      "    - feature_metadata.json\n",
      "    - robust_scaler.pkl\n",
      "##################################################\n",
      "\n",
      "[READY FOR] Phase 4: Clustering | Phase 5: Classification\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiaMetric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
